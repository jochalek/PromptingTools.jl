<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RAGTools · PromptingTools.jl</title><meta name="title" content="RAGTools · PromptingTools.jl"/><meta property="og:title" content="RAGTools · PromptingTools.jl"/><meta property="twitter:title" content="RAGTools · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/readme_examples/">Various examples</a></li><li><a class="tocitem" href="../examples/working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../examples/working_with_ollama/">Local models with Ollama.ai</a></li><li><a class="tocitem" href="../examples/working_with_google_ai_studio/">Google AIStudio</a></li><li><a class="tocitem" href="../examples/working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../examples/building_RAG/">Building RAG Application</a></li></ul></li><li><a class="tocitem" href="../frequently_asked_questions/">F.A.Q.</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference/">PromptingTools.jl</a></li><li><a class="tocitem" href="../reference_experimental/">Experimental Modules</a></li><li class="is-active"><a class="tocitem" href>RAGTools</a></li><li><a class="tocitem" href="../reference_agenttools/">AgentTools</a></li><li><a class="tocitem" href="../reference_apitools/">APITools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>RAGTools</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RAGTools</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/reference_ragtools.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference-for-RAGTools"><a class="docs-heading-anchor" href="#Reference-for-RAGTools">Reference for RAGTools</a><a id="Reference-for-RAGTools-1"></a><a class="docs-heading-anchor-permalink" href="#Reference-for-RAGTools" title="Permalink"></a></h1><ul><li><a href="#PromptingTools.Experimental.RAGTools.ChunkIndex"><code>PromptingTools.Experimental.RAGTools.ChunkIndex</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.JudgeAllScores"><code>PromptingTools.Experimental.RAGTools.JudgeAllScores</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.JudgeRating"><code>PromptingTools.Experimental.RAGTools.JudgeRating</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.MultiIndex"><code>PromptingTools.Experimental.RAGTools.MultiIndex</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.RAGContext"><code>PromptingTools.Experimental.RAGTools.RAGContext</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools._normalize"><code>PromptingTools.Experimental.RAGTools._normalize</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.airag"><code>PromptingTools.Experimental.RAGTools.airag</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}"><code>PromptingTools.Experimental.RAGTools.build_context</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_index"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_tags"><code>PromptingTools.Experimental.RAGTools.build_tags</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.cohere_api-Tuple{}"><code>PromptingTools.Experimental.RAGTools.cohere_api</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}"><code>PromptingTools.Experimental.RAGTools.find_closest</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.get_chunks-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_chunks</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.get_embeddings-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_embeddings</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.get_metadata-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_metadata</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}"><code>PromptingTools.Experimental.RAGTools.metadata_extract</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.rerank-Tuple{PromptingTools.Experimental.RAGTools.CohereRerank, PromptingTools.Experimental.RAGTools.AbstractChunkIndex, Any, Any}"><code>PromptingTools.Experimental.RAGTools.rerank</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_hit</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_rank</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools" href="#PromptingTools.Experimental.RAGTools"><code>PromptingTools.Experimental.RAGTools</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">RAGTools</code></pre><p>Provides Retrieval-Augmented Generation (RAG) functionality.</p><p>Requires: LinearAlgebra, SparseArrays, PromptingTools for proper functionality.</p><p>This module is experimental and may change at any time. It is intended to be moved to a separate package in the future.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/RAGTools.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.ChunkIndex" href="#PromptingTools.Experimental.RAGTools.ChunkIndex"><code>PromptingTools.Experimental.RAGTools.ChunkIndex</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChunkIndex</code></pre><p>Main struct for storing document chunks and their embeddings. It also stores tags and sources for each chunk.</p><p><strong>Fields</strong></p><ul><li><code>id::Symbol</code>: unique identifier of each index (to ensure we&#39;re using the right index with <code>CandidateChunks</code>)</li><li><code>chunks::Vector{&lt;:AbstractString}</code>: underlying document chunks / snippets</li><li><code>embeddings::Union{Nothing, Matrix{&lt;:Real}}</code>: for semantic search</li><li><code>tags::Union{Nothing, AbstractMatrix{&lt;:Bool}}</code>: for exact search, filtering, etc. This is often a sparse matrix indicating which chunks have the given <code>tag</code> (see <code>tag_vocab</code> for the position lookup)</li><li><code>tags_vocab::Union{Nothing, Vector{&lt;:AbstractString}}</code>: vocabulary for the <code>tags</code> matrix (each column in <code>tags</code> is one item in <code>tags_vocab</code> and rows are the chunks)</li><li><code>sources::Vector{&lt;:AbstractString}</code>: sources of the chunks</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/types.jl#L11-L23">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.JudgeAllScores" href="#PromptingTools.Experimental.RAGTools.JudgeAllScores"><code>PromptingTools.Experimental.RAGTools.JudgeAllScores</code></a> — <span class="docstring-category">Type</span></header><section><div><p><code>final_rating</code> is the average of all scoring criteria. Explain the <code>final_rating</code> in <code>rationale</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.JudgeRating" href="#PromptingTools.Experimental.RAGTools.JudgeRating"><code>PromptingTools.Experimental.RAGTools.JudgeRating</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Provide the <code>final_rating</code> between 1-5. Provide the rationale for it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.MultiIndex" href="#PromptingTools.Experimental.RAGTools.MultiIndex"><code>PromptingTools.Experimental.RAGTools.MultiIndex</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Composite index that stores multiple ChunkIndex objects and their embeddings</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/types.jl#L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.RAGContext" href="#PromptingTools.Experimental.RAGTools.RAGContext"><code>PromptingTools.Experimental.RAGTools.RAGContext</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAGContext</code></pre><p>A struct for debugging RAG answers. It contains the question, answer, context, and the candidate chunks at each step of the RAG pipeline.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/types.jl#L190-L194">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools._normalize" href="#PromptingTools.Experimental.RAGTools._normalize"><code>PromptingTools.Experimental.RAGTools._normalize</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Shortcut to LinearAlgebra.normalize. Provided in the package extension <code>RAGToolsExperimentalExt</code> (Requires SparseArrays and LinearAlgebra)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.airag" href="#PromptingTools.Experimental.RAGTools.airag"><code>PromptingTools.Experimental.RAGTools.airag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">airag(index::AbstractChunkIndex, rag_template::Symbol = :RAGAnswerFromContext;
    question::AbstractString,
    top_k::Int = 100, top_n::Int = 5, minimum_similarity::AbstractFloat = -1.0,
    tag_filter::Union{Symbol, Vector{String}, Regex, Nothing} = :auto,
    rerank_strategy::RerankingStrategy = Passthrough(),
    model_embedding::String = PT.MODEL_EMBEDDING, model_chat::String = PT.MODEL_CHAT,
    model_metadata::String = PT.MODEL_CHAT,
    metadata_template::Symbol = :RAGExtractMetadataShort,
    chunks_window_margin::Tuple{Int, Int} = (1, 1),
    return_context::Bool = false, verbose::Bool = true,
    rerank_kwargs::NamedTuple = NamedTuple(),
    api_kwargs::NamedTuple = NamedTuple(),
    aiembed_kwargs::NamedTuple = NamedTuple(),
    aigenerate_kwargs::NamedTuple = NamedTuple(),
    aiextract_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generates a response for a given question using a Retrieval-Augmented Generation (RAG) approach. </p><p>The function selects relevant chunks from an <code>ChunkIndex</code>, optionally filters them based on metadata tags, reranks them, and then uses these chunks to construct a context for generating a response.</p><p><strong>Arguments</strong></p><ul><li><code>index::AbstractChunkIndex</code>: The chunk index to search for relevant text.</li><li><code>rag_template::Symbol</code>: Template for the RAG model, defaults to <code>:RAGAnswerFromContext</code>.</li><li><code>question::AbstractString</code>: The question to be answered.</li><li><code>top_k::Int</code>: Number of top candidates to retrieve based on embedding similarity.</li><li><code>top_n::Int</code>: Number of candidates to return after reranking.</li><li><code>minimum_similarity::AbstractFloat</code>: Minimum similarity threshold (between -1 and 1) for filtering chunks based on embedding similarity. Defaults to -1.0.</li><li><code>tag_filter::Union{Symbol, Vector{String}, Regex}</code>: Mechanism for filtering chunks based on tags (either automatically detected, specific tags, or a regex pattern). Disabled by setting to <code>nothing</code>.</li><li><code>rerank_strategy::RerankingStrategy</code>: Strategy for reranking the retrieved chunks. Defaults to <code>Passthrough()</code>. Use <code>CohereRerank</code> for better results (requires <code>COHERE_API_KEY</code> to be set)</li><li><code>model_embedding::String</code>: Model used for embedding the question, default is <code>PT.MODEL_EMBEDDING</code>.</li><li><code>model_chat::String</code>: Model used for generating the final response, default is <code>PT.MODEL_CHAT</code>.</li><li><code>model_metadata::String</code>: Model used for extracting metadata, default is <code>PT.MODEL_CHAT</code>.</li><li><code>metadata_template::Symbol</code>: Template for the metadata extraction process from the question, defaults to: <code>:RAGExtractMetadataShort</code></li><li><code>chunks_window_margin::Tuple{Int,Int}</code>: The window size around each chunk to consider for context building. See <code>?build_context</code> for more information.</li><li><code>return_context::Bool</code>: If <code>true</code>, returns the context used for RAG along with the response.</li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging.</li><li><code>api_kwargs</code>: API parameters that will be forwarded to ALL of the API calls (<code>aiembed</code>, <code>aigenerate</code>, and <code>aiextract</code>).</li><li><code>aiembed_kwargs</code>: API parameters that will be forwarded to the <code>aiembed</code> call. If you need to provide <code>api_kwargs</code> only to this function, simply add them as a keyword argument, eg, <code>aiembed_kwargs = (; api_kwargs = (; x=1))</code>.</li><li><code>aigenerate_kwargs</code>: API parameters that will be forwarded to the <code>aigenerate</code> call. If you need to provide <code>api_kwargs</code> only to this function, simply add them as a keyword argument, eg, <code>aigenerate_kwargs = (; api_kwargs = (; temperature=0.3))</code>.</li><li><code>aiextract_kwargs</code>: API parameters that will be forwarded to the <code>aiextract</code> call for the metadata extraction.</li></ul><p><strong>Returns</strong></p><ul><li>If <code>return_context</code> is <code>false</code>, returns the generated message (<code>msg</code>).</li><li>If <code>return_context</code> is <code>true</code>, returns a tuple of the generated message (<code>msg</code>) and the RAG context (<code>rag_context</code>).</li></ul><p><strong>Notes</strong></p><ul><li>The function first finds the closest chunks to the question embedding, then optionally filters these based on tags. After that, it reranks the candidates and builds a context for the RAG model.</li><li>The <code>tag_filter</code> can be used to refine the search. If set to <code>:auto</code>, it attempts to automatically determine relevant tags (if <code>index</code> has them available).</li><li>The <code>chunks_window_margin</code> allows including surrounding chunks for richer context, considering they are from the same source.</li><li>The function currently supports only single <code>ChunkIndex</code>. </li></ul><p><strong>Examples</strong></p><p>Using <code>airag</code> to get a response for a question:</p><pre><code class="language-julia hljs">index = build_index(...)  # create an index
question = &quot;How to make a barplot in Makie.jl?&quot;
msg = airag(index, :RAGAnswerFromContext; question)

# or simply
msg = airag(index; question)</code></pre><p>See also <code>build_index</code>, <code>build_context</code>, <code>CandidateChunks</code>, <code>find_closest</code>, <code>find_tags</code>, <code>rerank</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/generation.jl#L39-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}" href="#PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}"><code>PromptingTools.Experimental.RAGTools.build_context</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_context(index::AbstractChunkIndex, reranked_candidates::CandidateChunks; chunks_window_margin::Tuple{Int, Int}) -&gt; Vector{String}</code></pre><p>Build context strings for each position in <code>reranked_candidates</code> considering a window margin around each position.</p><p><strong>Arguments</strong></p><ul><li><code>reranked_candidates::CandidateChunks</code>: Candidate chunks which contain positions to extract context from.</li><li><code>index::ChunkIndex</code>: The index containing chunks and sources.</li><li><code>chunks_window_margin::Tuple{Int, Int}</code>: A tuple indicating the margin (before, after) around each position to include in the context.  Defaults to <code>(1,1)</code>, which means 1 preceding and 1 suceeding chunk will be included. With <code>(0,0)</code>, only the matching chunks will be included.</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{String}</code>: A vector of context strings, each corresponding to a position in <code>reranked_candidates</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">index = ChunkIndex(...)  # Assuming a proper index is defined
candidates = CandidateChunks(index.id, [2, 4], [0.1, 0.2])
context = build_context(index, candidates; chunks_window_margin=(0, 1)) # include only one following chunk for each matching chunk</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/generation.jl#L4-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_index" href="#PromptingTools.Experimental.RAGTools.build_index"><code>PromptingTools.Experimental.RAGTools.build_index</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Build an index for RAG (Retriever-Augmented Generation) applications. REQUIRES SparseArrays and LinearAlgebra packages to be loaded!!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_index</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_index(files_or_docs::Vector{&lt;:AbstractString}; reader::Symbol = :files,
    separators = [&quot;</code></pre><p>&quot;, &quot;. &quot;, &quot; &quot;], max<em>length::Int = 256,         sources::Vector{&lt;:AbstractString} = files</em>or<em>docs,         extract</em>metadata::Bool = false, verbose::Integer = 1,         index<em>id = gensym(&quot;ChunkIndex&quot;),         metadata</em>template::Symbol = :RAGExtractMetadataShort,         model<em>embedding::String = PT.MODEL</em>EMBEDDING,         model<em>metadata::String = PT.MODEL</em>CHAT,         embedding<em>kwargs::NamedTuple = NamedTuple(),         metadata</em>kwargs::NamedTuple = NamedTuple(),         api<em>kwargs::NamedTuple = NamedTuple(),         cost</em>tracker = Threads.Atomic{Float64}(0.0))</p><p>Build an index for RAG (Retriever-Augmented Generation) applications from the provided file paths.  The function processes each file, splits its content into chunks, embeds these chunks,  optionally extracts metadata, and then compiles this information into a retrievable index.</p><p><strong>Arguments</strong></p><ul><li><code>files_or_docs</code>: A vector of valid file paths OR string documents to be indexed (chunked and embedded).</li><li><code>reader</code>: A symbol indicating the type of input, can be either <code>:files</code> or <code>:docs</code>. Default is <code>:files</code>.</li><li><code>separators</code>: A list of strings used as separators for splitting the text in each file into chunks. Default is `[</li></ul><p>&quot;, &quot;. &quot;, &quot; &quot;]`.</p><ul><li><code>max_length</code>: The maximum length of each chunk (if possible with provided separators). Default is 256.</li><li><code>sources</code>: A vector of strings indicating the source of each chunk. Default is equal to <code>files_or_docs</code> (for <code>reader=:files</code>)</li><li><code>extract_metadata</code>: A boolean flag indicating whether to extract metadata from each chunk (to build filter <code>tags</code> in the index). Default is <code>false</code>. Metadata extraction incurs additional cost and requires <code>model_metadata</code> and <code>metadata_template</code> to be provided.</li><li><code>verbose</code>: An Integer specifying the verbosity of the logs. Default is <code>1</code> (high-level logging). <code>0</code> is disabled.</li><li><code>metadata_template</code>: A symbol indicating the template to be used for metadata extraction. Default is <code>:RAGExtractMetadataShort</code>.</li><li><code>model_embedding</code>: The model to use for embedding.</li><li><code>model_metadata</code>: The model to use for metadata extraction.</li><li><code>api_kwargs</code>: Parameters to be provided to the API endpoint. Shared across all API calls.</li><li><code>embedding_kwargs</code>: Parameters to be provided to the <code>get_embedding</code> function. Useful to change the batch sizes (<code>target_batch_size_length</code>) or reduce asyncmap tasks (<code>ntasks</code>).</li><li><code>metadata_kwargs</code>: Parameters to be provided to the <code>get_metadata</code> function.</li></ul><p><strong>Returns</strong></p><ul><li><code>ChunkIndex</code>: An object containing the compiled index of chunks, embeddings, tags, vocabulary, and sources.</li></ul><p>See also: <code>MultiIndex</code>, <code>CandidateChunks</code>, <code>find_closest</code>, <code>find_tags</code>, <code>rerank</code>, <code>airag</code></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Assuming `test_files` is a vector of file paths
index = build_index(test_files; max_length=10, extract_metadata=true)

# Another example with metadata extraction and verbose output (`reader=:files` is implicit)
index = build_index([&quot;file1.txt&quot;, &quot;file2.txt&quot;]; 
                    separators=[&quot;. &quot;], 
                    extract_metadata=true, 
                    verbose=true)</code></pre><p><strong>Notes</strong></p><ul><li>If you get errors about exceeding embedding input sizes, first check the <code>max_length</code> in your chunks.  If that does NOT resolve the issue, try changing the <code>embedding_kwargs</code>.  In particular, reducing the <code>target_batch_size_length</code> parameter (eg, 10_000) and number of tasks <code>ntasks=1</code>.  Some providers cannot handle large batch sizes (eg, Databricks).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L204-L267">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_qa_evals(doc_chunks::Vector{&lt;:AbstractString}, sources::Vector{&lt;:AbstractString};
               model=PT.MODEL_CHAT, instructions=&quot;None.&quot;, qa_template::Symbol=:RAGCreateQAFromContext, 
               verbose::Bool=true, api_kwargs::NamedTuple = NamedTuple(), kwargs...) -&gt; Vector{QAEvalItem}</code></pre><p>Create a collection of question and answer evaluations (<code>QAEvalItem</code>) from document chunks and sources.  This function generates Q&amp;A pairs based on the provided document chunks, using a specified AI model and template.</p><p><strong>Arguments</strong></p><ul><li><code>doc_chunks::Vector{&lt;:AbstractString}</code>: A vector of document chunks, each representing a segment of text.</li><li><code>sources::Vector{&lt;:AbstractString}</code>: A vector of source identifiers corresponding to each chunk in <code>doc_chunks</code> (eg, filenames or paths).</li><li><code>model</code>: The AI model used for generating Q&amp;A pairs. Default is <code>PT.MODEL_CHAT</code>.</li><li><code>instructions::String</code>: Additional instructions or context to provide to the model generating QA sets. Defaults to &quot;None.&quot;.</li><li><code>qa_template::Symbol</code>: A template symbol that dictates the AITemplate that will be used. It must have placeholder <code>context</code>. Default is <code>:CreateQAFromContext</code>.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API endpoint.</li><li><code>verbose::Bool</code>: If <code>true</code>, additional information like costs will be logged. Defaults to <code>true</code>.</li></ul><p><strong>Returns</strong></p><p><code>Vector{QAEvalItem}</code>: A vector of <code>QAEvalItem</code> structs, each containing a source, context, question, and answer. Invalid or empty items are filtered out.</p><p><strong>Notes</strong></p><ul><li>The function internally uses <code>aiextract</code> to generate Q&amp;A pairs based on the provided <code>qa_template</code>. So you can use any kwargs that you want.</li><li>Each <code>QAEvalItem</code> includes the context (document chunk), the generated question and answer, and the source.</li><li>The function tracks and reports the cost of AI calls if <code>verbose</code> is enabled.</li><li>Items where the question, answer, or context is empty are considered invalid and are filtered out.</li></ul><p><strong>Examples</strong></p><p>Creating Q&amp;A evaluations from a set of document chunks:</p><pre><code class="language-julia hljs">doc_chunks = [&quot;Text from document 1&quot;, &quot;Text from document 2&quot;]
sources = [&quot;source1&quot;, &quot;source2&quot;]
qa_evals = build_qa_evals(doc_chunks, sources)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L65-L100">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_tags" href="#PromptingTools.Experimental.RAGTools.build_tags"><code>PromptingTools.Experimental.RAGTools.build_tags</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Builds a matrix of tags and a vocabulary list. REQUIRES SparseArrays and LinearAlgebra packages to be loaded!!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.cohere_api-Tuple{}" href="#PromptingTools.Experimental.RAGTools.cohere_api-Tuple{}"><code>PromptingTools.Experimental.RAGTools.cohere_api</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cohere_api(;
api_key::AbstractString,
endpoint::String,
url::AbstractString=&quot;https://api.cohere.ai/v1&quot;,
http_kwargs::NamedTuple=NamedTuple(),
kwargs...)</code></pre><p>Lightweight wrapper around the Cohere API. See https://cohere.com/docs for more details.</p><p><strong>Arguments</strong></p><ul><li><code>api_key</code>: Your Cohere API key. You can get one from https://dashboard.cohere.com/welcome/register (trial access is for free).</li><li><code>endpoint</code>: The Cohere endpoint to call. </li><li><code>url</code>: The base URL for the Cohere API. Default is <code>https://api.cohere.ai/v1</code>.</li><li><code>http_kwargs</code>: Any additional keyword arguments to pass to <code>HTTP.post</code>.</li><li><code>kwargs</code>: Any additional keyword arguments to pass to the Cohere API.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/api_services.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}" href="#PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}"><code>PromptingTools.Experimental.RAGTools.find_closest</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">find_closest(emb::AbstractMatrix{&lt;:Real},
    query_emb::AbstractVector{&lt;:Real};
    top_k::Int = 100, minimum_similarity::AbstractFloat = -1.0)</code></pre><p>Finds the indices of chunks (represented by embeddings in <code>emb</code>) that are closest (cosine similarity) to query embedding (<code>query_emb</code>). </p><p>If <code>minimum_similarity</code> is provided, only indices with similarity greater than or equal to it are returned.  Similarity can be between -1 and 1 (-1 = completely opposite, 1 = exactly the same).</p><p>Returns only <code>top_k</code> closest indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/retrieval.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.get_chunks-Tuple{Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.get_chunks-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_chunks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_chunks(files_or_docs::Vector{&lt;:AbstractString}; reader::Symbol = :files,
    sources::Vector{&lt;:AbstractString} = files_or_docs,
    verbose::Bool = true,
    separators = [&quot;\n\n&quot;, &quot;. &quot;, &quot;\n&quot;], max_length::Int = 256)</code></pre><p>Chunks the provided <code>files_or_docs</code> into chunks of maximum length <code>max_length</code> (if possible with provided <code>separators</code>).</p><p>Supports two modes of operation:</p><ul><li><code>reader=:files</code>: The function opens each file in <code>files_or_docs</code> and reads its content.</li><li><code>reader=:docs</code>: The function assumes that <code>files_or_docs</code> is a vector of strings to be chunked.</li></ul><p><strong>Arguments</strong></p><ul><li><code>files_or_docs</code>: A vector of valid file paths OR string documents to be chunked.</li><li><code>reader</code>: A symbol indicating the type of input, can be either <code>:files</code> or <code>:docs</code>. Default is <code>:files</code>.</li><li><code>separators</code>: A list of strings used as separators for splitting the text in each file into chunks. Default is <code>[\n\n&quot;, &quot;. &quot;, &quot;\n&quot;]</code>.</li><li><code>max_length</code>: The maximum length of each chunk (if possible with provided separators). Default is 256.</li><li><code>sources</code>: A vector of strings indicating the source of each chunk. Default is equal to <code>files_or_docs</code> (for <code>reader=:files</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L40-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.get_embeddings-Tuple{Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.get_embeddings-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_embeddings</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_embeddings(docs::Vector{&lt;:AbstractString};
    verbose::Bool = true,
    cost_tracker = Threads.Atomic{Float64}(0.0),
    target_batch_size_length::Int = 80_000,
    ntasks::Int = 4 * Threads.nthreads(),
    kwargs...)</code></pre><p>Embeds a vector of <code>docs</code> using the provided model (kwarg <code>model</code>). </p><p>Tries to batch embedding calls for roughly 80K characters per call (to avoid exceeding the API limit) but reduce network latency.</p><p><strong>Notes</strong></p><ul><li><code>docs</code> are assumed to be already chunked to the reasonable sizes that fit within the embedding context limit.</li><li>If you get errors about exceeding input sizes, first check the <code>max_length</code> in your chunks.  If that does NOT resolve the issue, try reducing the <code>target_batch_size_length</code> parameter (eg, 10_000) and number of tasks <code>ntasks=1</code>.  Some providers cannot handle large batch sizes.</li></ul><p><strong>Arguments</strong></p><ul><li><code>docs</code>: A vector of strings to be embedded.</li><li><code>verbose</code>: A boolean flag for verbose output. Default is <code>true</code>.</li><li><code>model</code>: The model to use for embedding. Default is <code>PT.MODEL_EMBEDDING</code>.</li><li><code>cost_tracker</code>: A <code>Threads.Atomic{Float64}</code> object to track the total cost of the API calls. Useful to pass the total cost to the parent call.</li><li><code>target_batch_size_length</code>: The target length (in characters) of each batch of document chunks sent for embedding. Default is 80_000 characters. Speeds up embedding process.</li><li><code>ntasks</code>: The number of tasks to use for asyncmap. Default is 4 * Threads.nthreads().</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L101-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.get_metadata-Tuple{Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.get_metadata-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_metadata</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_metadata(docs::Vector{&lt;:AbstractString};
    verbose::Bool = true,
    cost_tracker = Threads.Atomic{Float64}(0.0),
    kwargs...)</code></pre><p>Extracts metadata from a vector of <code>docs</code> using the provided model (kwarg <code>model</code>).</p><p><strong>Arguments</strong></p><ul><li><code>docs</code>: A vector of strings to be embedded.</li><li><code>verbose</code>: A boolean flag for verbose output. Default is <code>true</code>.</li><li><code>model</code>: The model to use for metadata extraction. Default is <code>PT.MODEL_CHAT</code>.</li><li><code>metadata_template</code>: A template to be used for metadata extraction. Default is <code>:RAGExtractMetadataShort</code>.</li><li><code>cost_tracker</code>: A <code>Threads.Atomic{Float64}</code> object to track the total cost of the API calls. Useful to pass the total cost to the parent call.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L161-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}" href="#PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}"><code>PromptingTools.Experimental.RAGTools.metadata_extract</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">metadata_extract(item::MetadataItem)
metadata_extract(items::Vector{MetadataItem})</code></pre><p>Extracts the metadata item into a string of the form <code>category:::value</code> (lowercased and spaces replaced with underscores).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">msg = aiextract(:RAGExtractMetadataShort; return_type=MaybeMetadataItems, text=&quot;I like package DataFrames&quot;, instructions=&quot;None.&quot;)
metadata = metadata_extract(msg.content.items)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/preparation.jl#L11-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.rerank-Tuple{PromptingTools.Experimental.RAGTools.CohereRerank, PromptingTools.Experimental.RAGTools.AbstractChunkIndex, Any, Any}" href="#PromptingTools.Experimental.RAGTools.rerank-Tuple{PromptingTools.Experimental.RAGTools.CohereRerank, PromptingTools.Experimental.RAGTools.AbstractChunkIndex, Any, Any}"><code>PromptingTools.Experimental.RAGTools.rerank</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">rerank(strategy::CohereRerank, index::AbstractChunkIndex, question,
    candidate_chunks;
    verbose::Bool = false,
    api_key::AbstractString = PT.COHERE_API_KEY,
    top_n::Integer = length(candidate_chunks.distances),
    model::AbstractString = &quot;rerank-english-v2.0&quot;,
    return_documents::Bool = false,
    kwargs...)</code></pre><p>Re-ranks a list of candidate chunks using the Cohere Rerank API. See https://cohere.com/rerank for more details. </p><p><strong>Arguments</strong></p><ul><li><code>query</code>: The query to be used for the search.</li><li><code>documents</code>: A vector of documents to be reranked.    The total max chunks (<code>length of documents * max_chunks_per_doc</code>) must be less than 10000. We recommend less than 1000 documents for optimal performance.</li><li><code>top_n</code>: The number of most relevant documents to return. Default is <code>length(documents)</code>.</li><li><code>model</code>: The model to use for reranking. Default is <code>rerank-english-v2.0</code>.</li><li><code>return_documents</code>: A boolean flag indicating whether to return the reranked documents in the response. Default is <code>false</code>.</li><li><code>max_chunks_per_doc</code>: The maximum number of chunks to use per document. Default is <code>10</code>.</li><li><code>verbose</code>: A boolean flag indicating whether to print verbose logging. Default is <code>false</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/retrieval.jl#L96-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}" href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_qa_evals(index::AbstractChunkIndex, qa_items::AbstractVector{&lt;:QAEvalItem};
    api_kwargs::NamedTuple = NamedTuple(),
    airag_kwargs::NamedTuple = NamedTuple(),
    qa_evals_kwargs::NamedTuple = NamedTuple(),
    verbose::Bool = true, parameters_dict::Dict{Symbol, &lt;:Any} = Dict{Symbol, Any}())</code></pre><p>Evaluates a vector of <code>QAEvalItem</code>s and returns a vector <code>QAEvalResult</code>.  This function assesses the relevance and accuracy of the answers generated in a QA evaluation context.</p><p>See <code>?run_qa_evals</code> for more details.</p><p><strong>Arguments</strong></p><ul><li><code>qa_items::AbstractVector{&lt;:QAEvalItem}</code>: The vector of QA evaluation items containing the questions and their answers.</li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging. Defaults to <code>true</code>.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API calls. See <code>?aiextract</code> for details.</li><li><code>airag_kwargs::NamedTuple</code>: Parameters that will be forwarded to <code>airag</code> calls. See <code>?airag</code> for details.</li><li><code>qa_evals_kwargs::NamedTuple</code>: Parameters that will be forwarded to <code>run_qa_evals</code> calls. See <code>?run_qa_evals</code> for details.</li><li><code>parameters_dict::Dict{Symbol, Any}</code>: Track any parameters used for later evaluations. Keys must be Symbols.</li></ul><p><strong>Returns</strong></p><p><code>Vector{QAEvalResult}</code>: Vector of evaluation results that includes various scores and metadata related to the QA evaluation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">index = &quot;...&quot; # Assuming a proper index is defined
qa_items = [QAEvalItem(question=&quot;What is the capital of France?&quot;, answer=&quot;Paris&quot;, context=&quot;France is a country in Europe.&quot;),
            QAEvalItem(question=&quot;What is the capital of Germany?&quot;, answer=&quot;Berlin&quot;, context=&quot;Germany is a country in Europe.&quot;)]

# Let&#39;s run a test with `top_k=5`
results = run_qa_evals(index, qa_items; airag_kwargs=(;top_k=5), parameters_dict=Dict(:top_k =&gt; 5))

# Filter out the &quot;failed&quot; calls
results = filter(x-&gt;!isnothing(x.answer_score), results);

# See average judge score
mean(x-&gt;x.answer_score, results)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L221-L260">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}" href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_qa_evals(qa_item::QAEvalItem, ctx::RAGContext; verbose::Bool = true,
             parameters_dict::Dict{Symbol, &lt;:Any}, judge_template::Symbol = :RAGJudgeAnswerFromContext,
             model_judge::AbstractString, api_kwargs::NamedTuple = NamedTuple()) -&gt; QAEvalResult</code></pre><p>Evaluates a single <code>QAEvalItem</code> using a RAG context (<code>RAGContext</code>) and returns a <code>QAEvalResult</code> structure. This function assesses the relevance and accuracy of the answers generated in a QA evaluation context.</p><p><strong>Arguments</strong></p><ul><li><code>qa_item::QAEvalItem</code>: The QA evaluation item containing the question and its answer.</li><li><code>ctx::RAGContext</code>: The context used for generating the QA pair, including the original context and the answers. Comes from <code>airag(...; return_context=true)</code></li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging. Defaults to <code>true</code>.</li><li><code>parameters_dict::Dict{Symbol, Any}</code>: Track any parameters used for later evaluations. Keys must be Symbols.</li><li><code>judge_template::Symbol</code>: The template symbol for the AI model used to judge the answer. Defaults to <code>:RAGJudgeAnswerFromContext</code>.</li><li><code>model_judge::AbstractString</code>: The AI model used for judging the answer&#39;s quality.  Defaults to standard chat model, but it is advisable to use more powerful model GPT-4.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API endpoint.</li></ul><p><strong>Returns</strong></p><p><code>QAEvalResult</code>: An evaluation result that includes various scores and metadata related to the QA evaluation.</p><p><strong>Notes</strong></p><ul><li>The function computes a retrieval score and rank based on how well the context matches the QA context.</li><li>It then uses the <code>judge_template</code> and <code>model_judge</code> to score the answer&#39;s accuracy and relevance.</li><li>In case of errors during evaluation, the function logs a warning (if <code>verbose</code> is <code>true</code>) and the <code>answer_score</code> will be set to <code>nothing</code>.</li></ul><p><strong>Examples</strong></p><p>Evaluating a QA pair using a specific context and model:</p><pre><code class="language-julia hljs">qa_item = QAEvalItem(question=&quot;What is the capital of France?&quot;, answer=&quot;Paris&quot;, context=&quot;France is a country in Europe.&quot;)
ctx = RAGContext(source=&quot;Wikipedia&quot;, context=&quot;France is a country in Europe.&quot;, answer=&quot;Paris&quot;)
parameters_dict = Dict(&quot;param1&quot; =&gt; &quot;value1&quot;, &quot;param2&quot; =&gt; &quot;value2&quot;)

eval_result = run_qa_evals(qa_item, ctx, parameters_dict=parameters_dict, model_judge=&quot;MyAIJudgeModel&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L145-L181">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_hit</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Returns 1.0 if <code>context</code> overlaps or is contained within any of the <code>candidate_context</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_rank</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Returns Integer rank of the position where <code>context</code> overlaps or is contained within a <code>candidate_context</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/c81140743df165406fc0d43805b696aa1184f2a2/src/Experimental/RAGTools/evaluation.jl#L138">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference_experimental/">« Experimental Modules</a><a class="docs-footer-nextpage" href="../reference_agenttools/">AgentTools »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Friday 23 February 2024 10:41">Friday 23 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
