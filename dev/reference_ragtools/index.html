<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>RAGTools · PromptingTools.jl</title><meta name="title" content="RAGTools · PromptingTools.jl"/><meta property="og:title" content="RAGTools · PromptingTools.jl"/><meta property="twitter:title" content="RAGTools · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/reference_ragtools/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/readme_examples/">Various examples</a></li><li><a class="tocitem" href="../examples/working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../examples/working_with_ollama/">Local models with Ollama.ai</a></li><li><a class="tocitem" href="../examples/working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../examples/building_RAG/">Building RAG Application</a></li></ul></li><li><a class="tocitem" href="../frequently_asked_questions/">F.A.Q.</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference/">PromptingTools.jl</a></li><li><a class="tocitem" href="../reference_experimental/">Experimental Modules</a></li><li class="is-active"><a class="tocitem" href>RAGTools</a></li><li><a class="tocitem" href="../reference_agenttools/">AgentTools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>RAGTools</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>RAGTools</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/reference_ragtools.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference-for-RAGTools"><a class="docs-heading-anchor" href="#Reference-for-RAGTools">Reference for RAGTools</a><a id="Reference-for-RAGTools-1"></a><a class="docs-heading-anchor-permalink" href="#Reference-for-RAGTools" title="Permalink"></a></h1><ul><li><a href="#PromptingTools.Experimental.RAGTools.JudgeAllScores"><code>PromptingTools.Experimental.RAGTools.JudgeAllScores</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.JudgeRating"><code>PromptingTools.Experimental.RAGTools.JudgeRating</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.MultiIndex"><code>PromptingTools.Experimental.RAGTools.MultiIndex</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.RAGContext"><code>PromptingTools.Experimental.RAGTools.RAGContext</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.airag"><code>PromptingTools.Experimental.RAGTools.airag</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}"><code>PromptingTools.Experimental.RAGTools.build_context</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_index"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.build_tags"><code>PromptingTools.Experimental.RAGTools.build_tags</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}"><code>PromptingTools.Experimental.RAGTools.find_closest</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}"><code>PromptingTools.Experimental.RAGTools.metadata_extract</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_hit</code></a></li><li><a href="#PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_rank</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools" href="#PromptingTools.Experimental.RAGTools"><code>PromptingTools.Experimental.RAGTools</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">RAGTools</code></pre><p>Provides Retrieval-Augmented Generation (RAG) functionality.</p><p>Requires: LinearAlgebra, SparseArrays, PromptingTools for proper functionality.</p><p>This module is experimental and may change at any time. It is intended to be moved to a separate package in the future.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/RAGTools.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.JudgeAllScores" href="#PromptingTools.Experimental.RAGTools.JudgeAllScores"><code>PromptingTools.Experimental.RAGTools.JudgeAllScores</code></a> — <span class="docstring-category">Type</span></header><section><div><p><code>final_rating</code> is the average of all scoring criteria. Explain the <code>final_rating</code> in <code>rationale</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.JudgeRating" href="#PromptingTools.Experimental.RAGTools.JudgeRating"><code>PromptingTools.Experimental.RAGTools.JudgeRating</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Provide the <code>final_rating</code> between 1-5. Provide the rationale for it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.MultiIndex" href="#PromptingTools.Experimental.RAGTools.MultiIndex"><code>PromptingTools.Experimental.RAGTools.MultiIndex</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Composite index that stores multiple ChunkIndex objects and their embeddings</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/types.jl#L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.RAGContext" href="#PromptingTools.Experimental.RAGTools.RAGContext"><code>PromptingTools.Experimental.RAGTools.RAGContext</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAGContext</code></pre><p>A struct for debugging RAG answers. It contains the question, answer, context, and the candidate chunks at each step of the RAG pipeline.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/types.jl#L116-L120">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.airag" href="#PromptingTools.Experimental.RAGTools.airag"><code>PromptingTools.Experimental.RAGTools.airag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">airag(index::AbstractChunkIndex, rag_template::Symbol = :RAGAnswerFromContext;
    question::AbstractString,
    top_k::Int = 3, `minimum_similarity::AbstractFloat`= -1.0,
    tag_filter::Union{Symbol, Vector{String}, Regex, Nothing} = :auto,
    rerank_strategy::RerankingStrategy = Passthrough(),
    model_embedding::String = PT.MODEL_EMBEDDING, model_chat::String = PT.MODEL_CHAT,
    model_metadata::String = PT.MODEL_CHAT,
    metadata_template::Symbol = :RAGExtractMetadataShort,
    chunks_window_margin::Tuple{Int, Int} = (1, 1),
    return_context::Bool = false, verbose::Bool = true,
    api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generates a response for a given question using a Retrieval-Augmented Generation (RAG) approach. </p><p>The function selects relevant chunks from an <code>ChunkIndex</code>, optionally filters them based on metadata tags, reranks them, and then uses these chunks to construct a context for generating a response.</p><p><strong>Arguments</strong></p><ul><li><code>index::AbstractChunkIndex</code>: The chunk index to search for relevant text.</li><li><code>rag_template::Symbol</code>: Template for the RAG model, defaults to <code>:RAGAnswerFromContext</code>.</li><li><code>question::AbstractString</code>: The question to be answered.</li><li><code>top_k::Int</code>: Number of top candidates to retrieve based on embedding similarity.</li><li><code>minimum_similarity::AbstractFloat</code>: Minimum similarity threshold (between -1 and 1) for filtering chunks based on embedding similarity. Defaults to -1.0.</li><li><code>tag_filter::Union{Symbol, Vector{String}, Regex}</code>: Mechanism for filtering chunks based on tags (either automatically detected, specific tags, or a regex pattern). Disabled by setting to <code>nothing</code>.</li><li><code>rerank_strategy::RerankingStrategy</code>: Strategy for reranking the retrieved chunks.</li><li><code>model_embedding::String</code>: Model used for embedding the question, default is <code>PT.MODEL_EMBEDDING</code>.</li><li><code>model_chat::String</code>: Model used for generating the final response, default is <code>PT.MODEL_CHAT</code>.</li><li><code>model_metadata::String</code>: Model used for extracting metadata, default is <code>PT.MODEL_CHAT</code>.</li><li><code>metadata_template::Symbol</code>: Template for the metadata extraction process from the question, defaults to: <code>:RAGExtractMetadataShort</code></li><li><code>chunks_window_margin::Tuple{Int,Int}</code>: The window size around each chunk to consider for context building. See <code>?build_context</code> for more information.</li><li><code>return_context::Bool</code>: If <code>true</code>, returns the context used for RAG along with the response.</li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging.</li><li><code>api_kwargs</code>: API parameters that will be forwarded to the API calls</li></ul><p><strong>Returns</strong></p><ul><li>If <code>return_context</code> is <code>false</code>, returns the generated message (<code>msg</code>).</li><li>If <code>return_context</code> is <code>true</code>, returns a tuple of the generated message (<code>msg</code>) and the RAG context (<code>rag_context</code>).</li></ul><p><strong>Notes</strong></p><ul><li>The function first finds the closest chunks to the question embedding, then optionally filters these based on tags. After that, it reranks the candidates and builds a context for the RAG model.</li><li>The <code>tag_filter</code> can be used to refine the search. If set to <code>:auto</code>, it attempts to automatically determine relevant tags (if <code>index</code> has them available).</li><li>The <code>chunks_window_margin</code> allows including surrounding chunks for richer context, considering they are from the same source.</li><li>The function currently supports only single <code>ChunkIndex</code>. </li></ul><p><strong>Examples</strong></p><p>Using <code>airag</code> to get a response for a question:</p><pre><code class="language-julia hljs">index = build_index(...)  # create an index
question = &quot;How to make a barplot in Makie.jl?&quot;
msg = airag(index, :RAGAnswerFromContext; question)

# or simply
msg = airag(index; question)</code></pre><p>See also <code>build_index</code>, <code>build_context</code>, <code>CandidateChunks</code>, <code>find_closest</code>, <code>find_tags</code>, <code>rerank</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/generation.jl#L39-L97">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}" href="#PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}"><code>PromptingTools.Experimental.RAGTools.build_context</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_context(index::AbstractChunkIndex, reranked_candidates::CandidateChunks; chunks_window_margin::Tuple{Int, Int}) -&gt; Vector{String}</code></pre><p>Build context strings for each position in <code>reranked_candidates</code> considering a window margin around each position.</p><p><strong>Arguments</strong></p><ul><li><code>reranked_candidates::CandidateChunks</code>: Candidate chunks which contain positions to extract context from.</li><li><code>index::ChunkIndex</code>: The index containing chunks and sources.</li><li><code>chunks_window_margin::Tuple{Int, Int}</code>: A tuple indicating the margin (before, after) around each position to include in the context.  Defaults to <code>(1,1)</code>, which means 1 preceding and 1 suceeding chunk will be included. With <code>(0,0)</code>, only the matching chunks will be included.</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{String}</code>: A vector of context strings, each corresponding to a position in <code>reranked_candidates</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">index = ChunkIndex(...)  # Assuming a proper index is defined
candidates = CandidateChunks(index.id, [2, 4], [0.1, 0.2])
context = build_context(index, candidates; chunks_window_margin=(0, 1)) # include only one following chunk for each matching chunk</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/generation.jl#L4-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_index" href="#PromptingTools.Experimental.RAGTools.build_index"><code>PromptingTools.Experimental.RAGTools.build_index</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Build an index for RAG (Retriever-Augmented Generation) applications. REQUIRES SparseArrays and LinearAlgebra packages to be loaded!!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/preparation.jl#L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_index</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_index(files::Vector{&lt;:AbstractString};
    separators = [&quot;</code></pre><p>&quot;, &quot;. &quot;, &quot; &quot;], max<em>length::Int = 256,         extract</em>metadata::Bool = false, verbose::Bool = true,         metadata<em>template::Symbol = :RAGExtractMetadataShort,         model</em>embedding::String = PT.MODEL<em>EMBEDDING,         model</em>metadata::String = PT.MODEL<em>CHAT,         api</em>kwargs::NamedTuple = NamedTuple())</p><p>Build an index for RAG (Retriever-Augmented Generation) applications from the provided file paths.  The function processes each file, splits its content into chunks, embeds these chunks,  optionally extracts metadata, and then compiles this information into a retrievable index.</p><p><strong>Arguments</strong></p><ul><li><code>files</code>: A vector of valid file paths to be indexed.</li><li><code>separators</code>: A list of strings used as separators for splitting the text in each file into chunks. Default is `[&quot;</li></ul><p>&quot;, &quot;. &quot;, &quot; &quot;]`.</p><ul><li><code>max_length</code>: The maximum length of each chunk (if possible with provided separators). Default is 256.</li><li><code>extract_metadata</code>: A boolean flag indicating whether to extract metadata from each chunk (to build filter <code>tags</code> in the index). Default is <code>false</code>. Metadata extraction incurs additional cost and requires <code>model_metadata</code> and <code>metadata_template</code> to be provided.</li><li><code>verbose</code>: A boolean flag for verbose output. Default is <code>true</code>.</li><li><code>metadata_template</code>: A symbol indicating the template to be used for metadata extraction. Default is <code>:RAGExtractMetadataShort</code>.</li><li><code>model_embedding</code>: The model to use for embedding.</li><li><code>model_metadata</code>: The model to use for metadata extraction.</li><li><code>api_kwargs</code>: Parameters to be provided to the API endpoint.</li></ul><p><strong>Returns</strong></p><ul><li><code>ChunkIndex</code>: An object containing the compiled index of chunks, embeddings, tags, vocabulary, and sources.</li></ul><p>See also: <code>MultiIndex</code>, <code>CandidateChunks</code>, <code>find_closest</code>, <code>find_tags</code>, <code>rerank</code>, <code>airag</code></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Assuming `test_files` is a vector of file paths
index = build_index(test_files; max_length=10, extract_metadata=true)

# Another example with metadata extraction and verbose output
index = build_index([&quot;file1.txt&quot;, &quot;file2.txt&quot;]; 
                    separators=[&quot;. &quot;], 
                    extract_metadata=true, 
                    verbose=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/preparation.jl#L37-L84">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_qa_evals(doc_chunks::Vector{&lt;:AbstractString}, sources::Vector{&lt;:AbstractString};
               model=PT.MODEL_CHAT, instructions=&quot;None.&quot;, qa_template::Symbol=:RAGCreateQAFromContext, 
               verbose::Bool=true, api_kwargs::NamedTuple = NamedTuple(), kwargs...) -&gt; Vector{QAEvalItem}</code></pre><p>Create a collection of question and answer evaluations (<code>QAEvalItem</code>) from document chunks and sources.  This function generates Q&amp;A pairs based on the provided document chunks, using a specified AI model and template.</p><p><strong>Arguments</strong></p><ul><li><code>doc_chunks::Vector{&lt;:AbstractString}</code>: A vector of document chunks, each representing a segment of text.</li><li><code>sources::Vector{&lt;:AbstractString}</code>: A vector of source identifiers corresponding to each chunk in <code>doc_chunks</code> (eg, filenames or paths).</li><li><code>model</code>: The AI model used for generating Q&amp;A pairs. Default is <code>PT.MODEL_CHAT</code>.</li><li><code>instructions::String</code>: Additional instructions or context to provide to the model generating QA sets. Defaults to &quot;None.&quot;.</li><li><code>qa_template::Symbol</code>: A template symbol that dictates the AITemplate that will be used. It must have placeholder <code>context</code>. Default is <code>:CreateQAFromContext</code>.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API endpoint.</li><li><code>verbose::Bool</code>: If <code>true</code>, additional information like costs will be logged. Defaults to <code>true</code>.</li></ul><p><strong>Returns</strong></p><p><code>Vector{QAEvalItem}</code>: A vector of <code>QAEvalItem</code> structs, each containing a source, context, question, and answer. Invalid or empty items are filtered out.</p><p><strong>Notes</strong></p><ul><li>The function internally uses <code>aiextract</code> to generate Q&amp;A pairs based on the provided <code>qa_template</code>. So you can use any kwargs that you want.</li><li>Each <code>QAEvalItem</code> includes the context (document chunk), the generated question and answer, and the source.</li><li>The function tracks and reports the cost of AI calls if <code>verbose</code> is enabled.</li><li>Items where the question, answer, or context is empty are considered invalid and are filtered out.</li></ul><p><strong>Examples</strong></p><p>Creating Q&amp;A evaluations from a set of document chunks:</p><pre><code class="language-julia hljs">doc_chunks = [&quot;Text from document 1&quot;, &quot;Text from document 2&quot;]
sources = [&quot;source1&quot;, &quot;source2&quot;]
qa_evals = build_qa_evals(doc_chunks, sources)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L65-L100">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.build_tags" href="#PromptingTools.Experimental.RAGTools.build_tags"><code>PromptingTools.Experimental.RAGTools.build_tags</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Builds a matrix of tags and a vocabulary list. REQUIRES SparseArrays and LinearAlgebra packages to be loaded!!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/preparation.jl#L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}" href="#PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}"><code>PromptingTools.Experimental.RAGTools.find_closest</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">find_closest(emb::AbstractMatrix{&lt;:Real},
    query_emb::AbstractVector{&lt;:Real};
    top_k::Int = 100, minimum_similarity::AbstractFloat = -1.0)</code></pre><p>Finds the indices of chunks (represented by embeddings in <code>emb</code>) that are closest (cosine similarity) to query embedding (<code>query_emb</code>). </p><p>If <code>minimum_similarity</code> is provided, only indices with similarity greater than or equal to it are returned.  Similarity can be between -1 and 1 (-1 = completely opposite, 1 = exactly the same).</p><p>Returns only <code>top_k</code> closest indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/retrieval.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}" href="#PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}"><code>PromptingTools.Experimental.RAGTools.metadata_extract</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">metadata_extract(item::MetadataItem)
metadata_extract(items::Vector{MetadataItem})</code></pre><p>Extracts the metadata item into a string of the form <code>category:::value</code> (lowercased and spaces replaced with underscores).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">msg = aiextract(:RAGExtractMetadataShort; return_type=MaybeMetadataItems, text=&quot;I like package DataFrames&quot;, instructions=&quot;None.&quot;)
metadata = metadata_extract(msg.content.items)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/preparation.jl#L11-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}" href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_qa_evals(index::AbstractChunkIndex, qa_items::AbstractVector{&lt;:QAEvalItem};
    api_kwargs::NamedTuple = NamedTuple(),
    airag_kwargs::NamedTuple = NamedTuple(),
    qa_evals_kwargs::NamedTuple = NamedTuple(),
    verbose::Bool = true, parameters_dict::Dict{Symbol, &lt;:Any} = Dict{Symbol, Any}())</code></pre><p>Evaluates a vector of <code>QAEvalItem</code>s and returns a vector <code>QAEvalResult</code>.  This function assesses the relevance and accuracy of the answers generated in a QA evaluation context.</p><p>See <code>?run_qa_evals</code> for more details.</p><p><strong>Arguments</strong></p><ul><li><code>qa_items::AbstractVector{&lt;:QAEvalItem}</code>: The vector of QA evaluation items containing the questions and their answers.</li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging. Defaults to <code>true</code>.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API calls. See <code>?aiextract</code> for details.</li><li><code>airag_kwargs::NamedTuple</code>: Parameters that will be forwarded to <code>airag</code> calls. See <code>?airag</code> for details.</li><li><code>qa_evals_kwargs::NamedTuple</code>: Parameters that will be forwarded to <code>run_qa_evals</code> calls. See <code>?run_qa_evals</code> for details.</li><li><code>parameters_dict::Dict{Symbol, Any}</code>: Track any parameters used for later evaluations. Keys must be Symbols.</li></ul><p><strong>Returns</strong></p><p><code>Vector{QAEvalResult}</code>: Vector of evaluation results that includes various scores and metadata related to the QA evaluation.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">index = &quot;...&quot; # Assuming a proper index is defined
qa_items = [QAEvalItem(question=&quot;What is the capital of France?&quot;, answer=&quot;Paris&quot;, context=&quot;France is a country in Europe.&quot;),
            QAEvalItem(question=&quot;What is the capital of Germany?&quot;, answer=&quot;Berlin&quot;, context=&quot;Germany is a country in Europe.&quot;)]

# Let&#39;s run a test with `top_k=5`
results = run_qa_evals(index, qa_items; airag_kwargs=(;top_k=5), parameters_dict=Dict(:top_k =&gt; 5))

# Filter out the &quot;failed&quot; calls
results = filter(x-&gt;!isnothing(x.answer_score), results);

# See average judge score
mean(x-&gt;x.answer_score, results)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L221-L260">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}" href="#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">run_qa_evals(qa_item::QAEvalItem, ctx::RAGContext; verbose::Bool = true,
             parameters_dict::Dict{Symbol, &lt;:Any}, judge_template::Symbol = :RAGJudgeAnswerFromContext,
             model_judge::AbstractString, api_kwargs::NamedTuple = NamedTuple()) -&gt; QAEvalResult</code></pre><p>Evaluates a single <code>QAEvalItem</code> using a RAG context (<code>RAGContext</code>) and returns a <code>QAEvalResult</code> structure. This function assesses the relevance and accuracy of the answers generated in a QA evaluation context.</p><p><strong>Arguments</strong></p><ul><li><code>qa_item::QAEvalItem</code>: The QA evaluation item containing the question and its answer.</li><li><code>ctx::RAGContext</code>: The context used for generating the QA pair, including the original context and the answers. Comes from <code>airag(...; return_context=true)</code></li><li><code>verbose::Bool</code>: If <code>true</code>, enables verbose logging. Defaults to <code>true</code>.</li><li><code>parameters_dict::Dict{Symbol, Any}</code>: Track any parameters used for later evaluations. Keys must be Symbols.</li><li><code>judge_template::Symbol</code>: The template symbol for the AI model used to judge the answer. Defaults to <code>:RAGJudgeAnswerFromContext</code>.</li><li><code>model_judge::AbstractString</code>: The AI model used for judging the answer&#39;s quality.  Defaults to standard chat model, but it is advisable to use more powerful model GPT-4.</li><li><code>api_kwargs::NamedTuple</code>: Parameters that will be forwarded to the API endpoint.</li></ul><p><strong>Returns</strong></p><p><code>QAEvalResult</code>: An evaluation result that includes various scores and metadata related to the QA evaluation.</p><p><strong>Notes</strong></p><ul><li>The function computes a retrieval score and rank based on how well the context matches the QA context.</li><li>It then uses the <code>judge_template</code> and <code>model_judge</code> to score the answer&#39;s accuracy and relevance.</li><li>In case of errors during evaluation, the function logs a warning (if <code>verbose</code> is <code>true</code>) and the <code>answer_score</code> will be set to <code>nothing</code>.</li></ul><p><strong>Examples</strong></p><p>Evaluating a QA pair using a specific context and model:</p><pre><code class="language-julia hljs">qa_item = QAEvalItem(question=&quot;What is the capital of France?&quot;, answer=&quot;Paris&quot;, context=&quot;France is a country in Europe.&quot;)
ctx = RAGContext(source=&quot;Wikipedia&quot;, context=&quot;France is a country in Europe.&quot;, answer=&quot;Paris&quot;)
parameters_dict = Dict(&quot;param1&quot; =&gt; &quot;value1&quot;, &quot;param2&quot; =&gt; &quot;value2&quot;)

eval_result = run_qa_evals(qa_item, ctx, parameters_dict=parameters_dict, model_judge=&quot;MyAIJudgeModel&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L145-L181">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_hit</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Returns 1.0 if <code>context</code> overlaps or is contained within any of the <code>candidate_context</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_rank</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Returns Integer rank of the position where <code>context</code> overlaps or is contained within a <code>candidate_context</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/d81a2d3a73c29eeeac1cf952380c4309b790622b/src/Experimental/RAGTools/evaluation.jl#L138">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference_experimental/">« Experimental Modules</a><a class="docs-footer-nextpage" href="../reference_agenttools/">AgentTools »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Wednesday 17 January 2024 09:00">Wednesday 17 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
