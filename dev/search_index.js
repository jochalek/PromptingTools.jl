var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = PromptingTools","category":"page"},{"location":"#PromptingTools","page":"Home","title":"PromptingTools","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for PromptingTools.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [PromptingTools]","category":"page"},{"location":"#PromptingTools.AITemplate","page":"Home","title":"PromptingTools.AITemplate","text":"AITemplate\n\nAITemplate is a template for a conversation prompt.   This type is merely a container for the template name, which is resolved into a set of messages (=prompt) by render.\n\nNaming Convention\n\nTemplate names should be in CamelCase\nFollow the format <Persona>...<Variable>... where possible, eg, JudgeIsItTrue, ``\nStarting with the Persona (=System prompt), eg, Judge = persona is meant to judge some provided information\nVariable to be filled in with context, eg, It = placeholder it\nEnding with the variable name is helpful, eg, JuliaExpertTask for a persona to be an expert in Julia language and task is the placeholder name\nIdeally, the template name should be self-explanatory, eg, JudgeIsItTrue = persona is meant to judge some provided information where it is true or false\n\nExamples\n\nSave time by re-using pre-made templates, just fill in the placeholders with the keyword arguments:\n\nmsg = aigenerate(:JuliaExpertAsk; ask = \"How do I add packages?\")\n\nThe above is equivalent to a more verbose version that explicitly uses the dispatch on AITemplate:\n\nmsg = aigenerate(AITemplate(:JuliaExpertAsk); ask = \"How do I add packages?\")\n\nFind available templates with aitemplates:\n\ntmps = aitemplates(\"JuliaExpertAsk\")\n# Will surface one specific template\n# 1-element Vector{AITemplateMetadata}:\n# PromptingTools.AITemplateMetadata\n#   name: Symbol JuliaExpertAsk\n#   description: String \"For asking questions about Julia language. Placeholders: `ask`\"\n#   version: String \"1\"\n#   wordcount: Int64 237\n#   variables: Array{Symbol}((1,))\n#   system_preview: String \"You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun\"\n#   user_preview: String \"# Question\n\n{{ask}}\"\n#   source: String \"\"\n\nThe above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).\n\nSearch for all Julia-related templates:\n\ntmps = aitemplates(\"Julia\")\n# 2-element Vector{AITemplateMetadata}... -> more to come later!\n\nIf you are on VSCode, you can leverage nice tabular display with vscodedisplay:\n\nusing DataFrames\ntmps = aitemplates(\"Julia\") |> DataFrame |> vscodedisplay\n\nI have my selected template, how do I use it? Just use the \"name\" in aigenerate or aiclassify   like you see in the first example!\n\nYou can inspect any template by \"rendering\" it (this is what the LLM will see):\n\njulia> AITemplate(:JudgeIsItTrue) |> PromptingTools.render\n\nSee also: save_template, load_template, load_templates! for more advanced use cases (and the corresponding script in examples/ folder)\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.AITemplateMetadata","page":"Home","title":"PromptingTools.AITemplateMetadata","text":"Helper for easy searching and reviewing of templates. Defined on loading of each template.\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.AbstractPromptSchema","page":"Home","title":"PromptingTools.AbstractPromptSchema","text":"Defines different prompting styles based on the model training and fine-tuning.\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.ChatMLSchema","page":"Home","title":"PromptingTools.ChatMLSchema","text":"ChatMLSchema is used by many open-source chatbots, by OpenAI models (under the hood) and by several models and inferfaces (eg, Ollama, vLLM)\n\nYou can explore it on tiktokenizer\n\nIt uses the following conversation structure:\n\n<im_start>system\n...<im_end>\n<|im_start|>user\n...<|im_end|>\n<|im_start|>assistant\n...<|im_end|>\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.MaybeExtract","page":"Home","title":"PromptingTools.MaybeExtract","text":"Extract a result from the provided data, if any, otherwise set the error and message fields.\n\nArguments\n\nerror::Bool: true if a result is found, false otherwise.\nmessage::String: Only present if no result is found, should be short and concise.\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.OllamaManagedSchema","page":"Home","title":"PromptingTools.OllamaManagedSchema","text":"Ollama by default manages different models and their associated prompt schemas when you pass system_prompt and prompt fields to the API.\n\nWarning: It works only for 1 system message and 1 user message, so anything more than that has to be rejected.\n\nIf you need to pass more messagese / longer conversational history, you can use define the model-specific schema directly and pass your Ollama requests with raw=true,   which disables and templating and schema management by Ollama.\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.OpenAISchema","page":"Home","title":"PromptingTools.OpenAISchema","text":"OpenAISchema is the default schema for OpenAI models.\n\nIt uses the following conversation template:\n\n[Dict(role=\"system\",content=\"...\"),Dict(role=\"user\",content=\"...\"),Dict(role=\"assistant\",content=\"...\")]\n\nIt's recommended to separate sections in your prompt with markdown headers (e.g. `##Answer\n\n`).\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.TestEchoOllamaManagedSchema","page":"Home","title":"PromptingTools.TestEchoOllamaManagedSchema","text":"Echoes the user's input back to them. Used for testing the implementation\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.TestEchoOpenAISchema","page":"Home","title":"PromptingTools.TestEchoOpenAISchema","text":"Echoes the user's input back to them. Used for testing the implementation\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.UserMessageWithImages-Tuple{AbstractString}","page":"Home","title":"PromptingTools.UserMessageWithImages","text":"Construct UserMessageWithImages with 1 or more images. Images can be either URLs or local paths.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.X123","page":"Home","title":"PromptingTools.X123","text":"With docstring\n\n\n\n\n\n","category":"type"},{"location":"#PromptingTools.aiclassify-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{<:PromptingTools.AbstractMessage}}}","page":"Home","title":"PromptingTools.aiclassify","text":"aiclassify(prompt_schema::AbstractOpenAISchema, prompt::ALLOWED_PROMPT_TYPE;\napi_kwargs::NamedTuple = (logit_bias = Dict(837 => 100, 905 => 100, 9987 => 100),\n    max_tokens = 1, temperature = 0),\nkwargs...)\n\nClassifies the given prompt/statement as true/false/unknown.\n\nNote: this is a very simple classifier, it is not meant to be used in production. Credit goes to AAAzzam.\n\nIt uses Logit bias trick and limits the output to 1 token to force the model to output only true/false/unknown.\n\nOutput tokens used (via api_kwargs):\n\n837: ' true'\n905: ' false'\n9987: ' unknown'\n\nArguments\n\nprompt_schema::AbstractOpenAISchema: The schema for the prompt.\nprompt: The prompt/statement to classify if it's a String. If it's a Symbol, it is expanded as a template via render(schema,template).\n\nExample\n\naiclassify(\"Is two plus two four?\") # true\naiclassify(\"Is two plus three a vegetable on Mars?\") # false\n\naiclassify returns only true/false/unknown. It's easy to get the proper Bool output type out with tryparse, eg,\n\ntryparse(Bool, aiclassify(\"Is two plus two four?\")) isa Bool # true\n\nOutput of type Nothing marks that the model couldn't classify the statement as true/false.\n\nIdeally, we would like to re-use some helpful system prompt to get more accurate responses. For this reason we have templates, eg, :JudgeIsItTrue. By specifying the template, we can provide our statement as the expected variable (it in this case) See that the model now correctly classifies the statement as \"unknown\".\n\naiclassify(:JudgeIsItTrue; it = \"Is two plus three a vegetable on Mars?\") # unknown\n\nFor better results, use higher quality models like gpt4, eg, \n\naiclassify(:JudgeIsItTrue;\n    it = \"If I had two apples and I got three more, I have five apples now.\",\n    model = \"gpt4\") # true\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F<:Function","page":"Home","title":"PromptingTools.aiembed","text":"aiembed(prompt_schema::AbstractOllamaManagedSchema,\n        doc_or_docs::Union{AbstractString, Vector{<:AbstractString}},\n        postprocess::F = identity;\n        verbose::Bool = true,\n        api_key::String = API_KEY,\n        model::String = MODEL_EMBEDDING,\n        http_kwargs::NamedTuple = (retry_non_idempotent = true,\n                                   retries = 5,\n                                   readtimeout = 120),\n        api_kwargs::NamedTuple = NamedTuple(),\n        kwargs...) where {F <: Function}\n\nThe aiembed function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.\n\nArguments\n\nprompt_schema::AbstractOllamaManagedSchema: The schema for the prompt.\ndoc_or_docs::Union{AbstractString, Vector{<:AbstractString}}: The document or list of documents to generate embeddings for. The list of documents is processed sequentially,  so users should consider implementing an async version with with Threads.@spawn\npostprocess::F: The post-processing function to apply to each embedding. Defaults to the identity function, but could be LinearAlgebra.normalize.\nverbose::Bool: A flag indicating whether to print verbose information. Defaults to true.\napi_key::String: The API key to use for the OpenAI API. Defaults to API_KEY.\nmodel::String: The model to use for generating embeddings. Defaults to MODEL_EMBEDDING.\nhttp_kwargs::NamedTuple: Additional keyword arguments for the HTTP request. Defaults to empty NamedTuple.\napi_kwargs::NamedTuple: Additional keyword arguments for the Ollama API. Defaults to an empty NamedTuple.\nkwargs: Prompt variables to be used to fill the prompt/template\n\nReturns\n\nmsg: A DataMessage object containing the embeddings, status, token count, and elapsed time.\n\nNote: Ollama API currently does not return the token count, so it's set to (0,0)\n\nExample\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema()\n\nmsg = aiembed(schema, \"Hello World\"; model=\"openhermes2.5-mistral\")\nmsg.content # 4096-element JSON3.Array{Float64...\n\nWe can embed multiple strings at once and they will be hcat into a matrix   (ie, each column corresponds to one string)\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema()\n\nmsg = aiembed(schema, [\"Hello World\", \"How are you?\"]; model=\"openhermes2.5-mistral\")\nmsg.content # 4096×2 Matrix{Float64}:\n\nIf you plan to calculate the cosine distance between embeddings, you can normalize them first:\n\nconst PT = PromptingTools\nusing LinearAlgebra\nschema = PT.OllamaManagedSchema()\n\nmsg = aiembed(schema, [\"embed me\", \"and me too\"], LinearAlgebra.normalize; model=\"openhermes2.5-mistral\")\n\n# calculate cosine distance between the two normalized embeddings as a simple dot product\nmsg.content' * msg.content[:, 1] # [1.0, 0.34]\n\nSimilarly, you can use the postprocess argument to materialize the data from JSON3.Object by using postprocess = copy\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema()\n\nmsg = aiembed(schema, \"Hello World\", copy; model=\"openhermes2.5-mistral\")\nmsg.content # 4096-element Vector{Float64}\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{<:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, Vector{<:AbstractString}}, F}} where F<:Function","page":"Home","title":"PromptingTools.aiembed","text":"aiembed(prompt_schema::AbstractOpenAISchema,\n        doc_or_docs::Union{AbstractString, Vector{<:AbstractString}},\n        postprocess::F = identity;\n        verbose::Bool = true,\n        api_key::String = API_KEY,\n        model::String = MODEL_EMBEDDING,\n        http_kwargs::NamedTuple = (retry_non_idempotent = true,\n                                   retries = 5,\n                                   readtimeout = 120),\n        api_kwargs::NamedTuple = NamedTuple(),\n        kwargs...) where {F <: Function}\n\nThe aiembed function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.\n\nArguments\n\nprompt_schema::AbstractOpenAISchema: The schema for the prompt.\ndoc_or_docs::Union{AbstractString, Vector{<:AbstractString}}: The document or list of documents to generate embeddings for.\npostprocess::F: The post-processing function to apply to each embedding. Defaults to the identity function.\nverbose::Bool: A flag indicating whether to print verbose information. Defaults to true.\napi_key::String: The API key to use for the OpenAI API. Defaults to API_KEY.\nmodel::String: The model to use for generating embeddings. Defaults to MODEL_EMBEDDING.\nhttp_kwargs::NamedTuple: Additional keyword arguments for the HTTP request. Defaults to (retry_non_idempotent = true, retries = 5, readtimeout = 120).\napi_kwargs::NamedTuple: Additional keyword arguments for the OpenAI API. Defaults to an empty NamedTuple.\nkwargs...: Additional keyword arguments.\n\nReturns\n\nmsg: A DataMessage object containing the embeddings, status, token count, and elapsed time.\n\nExample\n\nmsg = aiembed(\"Hello World\")\nmsg.content # 1536-element JSON3.Array{Float64...\n\nWe can embed multiple strings at once and they will be hcat into a matrix   (ie, each column corresponds to one string)\n\nmsg = aiembed([\"Hello World\", \"How are you?\"])\nmsg.content # 1536×2 Matrix{Float64}:\n\nIf you plan to calculate the cosine distance between embeddings, you can normalize them first:\n\nusing LinearAlgebra\nmsg = aiembed([\"embed me\", \"and me too\"], LinearAlgebra.normalize)\n\n# calculate cosine distance between the two normalized embeddings as a simple dot product\nmsg.content' * msg.content[:, 1] # [1.0, 0.787]\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{<:PromptingTools.AbstractMessage}}}","page":"Home","title":"PromptingTools.aiextract","text":"aiextract([prompt_schema::AbstractOpenAISchema,] prompt::ALLOWED_PROMPT_TYPE; \nreturn_type::Type,\nverbose::Bool = true,\n    model::String = MODEL_CHAT,\n    http_kwargs::NamedTuple = (;\n        retry_non_idempotent = true,\n        retries = 5,\n        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),\n    kwargs...)\n\nExtract required information (defined by a struct return_type) from the provided prompt by leveraging OpenAI function calling mode.\n\nThis is a perfect solution for extracting structured information from text (eg, extract organization names in news articles, etc.)\n\nIt's effectively a light wrapper around aigenerate call, which requires additional keyword argument return_type to be provided  and will enforce the model outputs to adhere to it.\n\nArguments\n\nprompt_schema: An optional object to specify which prompt template should be applied (Default to PROMPT_SCHEMA = OpenAISchema)\nprompt: Can be a string representing the prompt for the AI conversation, a UserMessage, a vector of AbstractMessage or an AITemplate\nreturn_type: A struct TYPE representing the the information we want to extract. Do not provide a struct instance, only the type. If the struct has a docstring, it will be provided to the model as well. It's used to enforce structured model outputs or provide more information.\nverbose: A boolean indicating whether to print additional information.\napi_key: A string representing the API key for accessing the OpenAI API.\nmodel: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in MODEL_ALIASES.\nhttp_kwargs: A named tuple of HTTP keyword arguments.\napi_kwargs: A named tuple of API keyword arguments.\nkwargs: Prompt variables to be used to fill the prompt/template\n\nReturns\n\nmsg: An DataMessage object representing the extracted data, including the content, status, tokens, and elapsed time.  Use msg.content to access the extracted data.\n\nSee also: function_call_signature, MaybeExtract, aigenerate\n\nExample\n\nDo you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (return_type):\n\n\"Person's age, height, and weight.\"\nstruct MyMeasurement\n    age::Int # required\n    height::Union{Int,Nothing} # optional\n    weight::Union{Nothing,Float64} # optional\nend\nmsg = aiextract(\"James is 30, weighs 80kg. He's 180cm tall.\"; return_type=MyMeasurement)\n# [ Info: Tokens: 129 @ Cost: $0.0002 in 1.0 seconds\n# PromptingTools.DataMessage(MyMeasurement)\nmsg.content\n# MyMeasurement(30, 180, 80.0)\n\nThe fields that allow Nothing are marked as optional in the schema:\n\nmsg = aiextract(\"James is 30.\"; return_type=MyMeasurement)\n# MyMeasurement(30, nothing, nothing)\n\nIf there are multiple items you want to extract, define a wrapper struct to get a Vector of MyMeasurement:\n\nstruct MyMeasurementWrapper\n    measurements::Vector{MyMeasurement}\nend\n\nmsg = aiextract(\"James is 30, weighs 80kg. He's 180cm tall. Then Jack is 19 but really tall - over 190!\"; return_type=ManyMeasurements)\n\nmsg.content.measurements\n# 2-element Vector{MyMeasurement}:\n#  MyMeasurement(30, 180, 80.0)\n#  MyMeasurement(19, 190, nothing)\n\nOr if you want your extraction to fail gracefully when data isn't found, use MaybeExtract{T} wrapper  (this trick is inspired by the Instructor package!):\n\nusing PromptingTools: MaybeExtract\n\ntype = MaybeExtract{MyMeasurement}\n# Effectively the same as:\n# struct MaybeExtract{T}\n#     result::Union{T, Nothing} // The result of the extraction\n#     error::Bool // true if a result is found, false otherwise\n#     message::Union{Nothing, String} // Only present if no result is found, should be short and concise\n# end\n\n# If LLM extraction fails, it will return a Dict with `error` and `message` fields instead of the result!\nmsg = aiextract(\"Extract measurements from the text: I am giraffe\", type)\nmsg.content\n# MaybeExtract{MyMeasurement}(nothing, true, \"I'm sorry, but I can only assist with human measurements.\")\n\nThat way, you can handle the error gracefully and get a reason why extraction failed (in msg.content.message).\n\nNote that the error message refers to a giraffe not being a human,   because in our MyMeasurement docstring, we said that it's for people!\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{<:PromptingTools.AbstractMessage}}}","page":"Home","title":"PromptingTools.aigenerate","text":"aigenerate(prompt_schema::AbstractOllamaManagedSchema, prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,\n    model::String = MODEL_CHAT,\n    http_kwargs::NamedTuple = NamedTuple(), api_kwargs::NamedTuple = NamedTuple(),\n    kwargs...)\n\nGenerate an AI response based on a given prompt using the OpenAI API.\n\nArguments\n\nprompt_schema: An optional object to specify which prompt template should be applied (Default to PROMPT_SCHEMA = OpenAISchema not AbstractManagedSchema)\nprompt: Can be a string representing the prompt for the AI conversation, a UserMessage, a vector of AbstractMessage or an AITemplate\nverbose: A boolean indicating whether to print additional information.\napi_key: Provided for interface consistency. Not needed for locally hosted Ollama.\nmodel: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in MODEL_ALIASES.\nhttp_kwargs::NamedTuple: Additional keyword arguments for the HTTP request. Defaults to empty NamedTuple.\napi_kwargs::NamedTuple: Additional keyword arguments for the Ollama API. Defaults to an empty NamedTuple.\nkwargs: Prompt variables to be used to fill the prompt/template\n\nReturns\n\nmsg: An AIMessage object representing the generated AI message, including the content, status, tokens, and elapsed time.\n\nUse msg.content to access the extracted string.\n\nSee also: ai_str, aai_str, aiembed\n\nExample\n\nSimple hello world to test the API:\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema() # We need to explicit if we want Ollama, OpenAISchema is the default\n\nmsg = aigenerate(schema, \"Say hi!\"; model=\"openhermes2.5-mistral\")\n# [ Info: Tokens: 69 in 0.9 seconds\n# AIMessage(\"Hello! How can I assist you today?\")\n\nmsg is an AIMessage object. Access the generated string via content property:\n\ntypeof(msg) # AIMessage{SubString{String}}\npropertynames(msg) # (:content, :status, :tokens, :elapsed\nmsg.content # \"Hello! How can I assist you today?\"\n\nNote: We need to be explicit about the schema we want to use. If we don't, it will default to OpenAISchema (=PT.DEFAULT_SCHEMA) ___ You can use string interpolation:\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema()\na = 1\nmsg=aigenerate(schema, \"What is `$a+$a`?\"; model=\"openhermes2.5-mistral\")\nmsg.content # \"The result of `1+1` is `2`.\"\n\n___ You can provide the whole conversation or more intricate prompts as a Vector{AbstractMessage}:\n\nconst PT = PromptingTools\nschema = PT.OllamaManagedSchema()\n\nconversation = [\n    PT.SystemMessage(\"You're master Yoda from Star Wars trying to help the user become a Yedi.\"),\n    PT.UserMessage(\"I have feelings for my iPhone. What should I do?\")]\n\nmsg = aigenerate(schema, conversation; model=\"openhermes2.5-mistral\")\n# [ Info: Tokens: 111 in 2.1 seconds\n# AIMessage(\"Strong the attachment is, it leads to suffering it may. Focus on the force within you must, ...<continues>\")\n\nNote: Managed Ollama currently supports at most 1 User Message and 1 System Message given the API limitations. If you want more, you need to use the ChatMLSchema.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{<:PromptingTools.AbstractMessage}}}","page":"Home","title":"PromptingTools.aigenerate","text":"aigenerate([prompt_schema::AbstractOpenAISchema,] prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,\n    model::String = MODEL_CHAT,\n    http_kwargs::NamedTuple = (;\n        retry_non_idempotent = true,\n        retries = 5,\n        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),\n    kwargs...)\n\nGenerate an AI response based on a given prompt using the OpenAI API.\n\nArguments\n\nprompt_schema: An optional object to specify which prompt template should be applied (Default to PROMPT_SCHEMA = OpenAISchema)\nprompt: Can be a string representing the prompt for the AI conversation, a UserMessage, a vector of AbstractMessage or an AITemplate\nverbose: A boolean indicating whether to print additional information.\napi_key: A string representing the API key for accessing the OpenAI API.\nmodel: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in MODEL_ALIASES.\nhttp_kwargs: A named tuple of HTTP keyword arguments.\napi_kwargs: A named tuple of API keyword arguments.\nkwargs: Prompt variables to be used to fill the prompt/template\n\nReturns\n\nmsg: An AIMessage object representing the generated AI message, including the content, status, tokens, and elapsed time.\n\nUse msg.content to access the extracted string.\n\nSee also: ai_str, aai_str, aiembed, aiclassify, aiextract, aiscan\n\nExample\n\nSimple hello world to test the API:\n\nresult = aigenerate(\"Say Hi!\")\n# [ Info: Tokens: 29 @ Cost: $0.0 in 1.0 seconds\n# AIMessage(\"Hello! How can I assist you today?\")\n\nresult is an AIMessage object. Access the generated string via content property:\n\ntypeof(result) # AIMessage{SubString{String}}\npropertynames(result) # (:content, :status, :tokens, :elapsed\nresult.content # \"Hello! How can I assist you today?\"\n\n___ You can use string interpolation:\n\na = 1\nmsg=aigenerate(\"What is `$a+$a`?\")\nmsg.content # \"The sum of `1+1` is `2`.\"\n\n___ You can provide the whole conversation or more intricate prompts as a Vector{AbstractMessage}:\n\nconst PT = PromptingTools\n\nconversation = [\n    PT.SystemMessage(\"You're master Yoda from Star Wars trying to help the user become a Yedi.\"),\n    PT.UserMessage(\"I have feelings for my iPhone. What should I do?\")]\nmsg=aigenerate(conversation)\n# AIMessage(\"Ah, strong feelings you have for your iPhone. A Jedi's path, this is not... <continues>\")\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{<:PromptingTools.AbstractMessage}}}","page":"Home","title":"PromptingTools.aiscan","text":"aiscan([promptschema::AbstractOpenAISchema,] prompt::ALLOWEDPROMPTTYPE;      imageurl::Union{Nothing, AbstractString, Vector{<:AbstractString}} = nothing,     imagepath::Union{Nothing, AbstractString, Vector{<:AbstractString}} = nothing,     imagedetail::AbstractString = \"auto\",     attachtolatest::Bool = true,     verbose::Bool = true,         model::String = MODELCHAT,         httpkwargs::NamedTuple = (;             retrynonidempotent = true,             retries = 5,             readtimeout = 120),          apikwargs::NamedTuple = = (; maxtokens = 2500),         kwargs...)\n\nScans the provided image (image_url or image_path) with the goal provided in the prompt.\n\nCan be used for many multi-modal tasks, such as: OCR (transcribe text in the image), image captioning, image classification, etc.\n\nIt's effectively a light wrapper around aigenerate call, which uses additional keyword arguments image_url, image_path, image_detail to be provided.   At least one image source (url or path) must be provided.\n\nArguments\n\nprompt_schema: An optional object to specify which prompt template should be applied (Default to PROMPT_SCHEMA = OpenAISchema)\nprompt: Can be a string representing the prompt for the AI conversation, a UserMessage, a vector of AbstractMessage or an AITemplate\nimage_url: A string or vector of strings representing the URL(s) of the image(s) to scan.\nimage_path: A string or vector of strings representing the path(s) of the image(s) to scan.\nimage_detail: A string representing the level of detail to include for images. Can be \"auto\", \"high\", or \"low\". See OpenAI Vision Guide for more details.\nattach_to_latest: A boolean how to handle if a conversation with multiple UserMessage is provided. When true, the images are attached to the latest UserMessage.\nverbose: A boolean indicating whether to print additional information.\napi_key: A string representing the API key for accessing the OpenAI API.\nmodel: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in MODEL_ALIASES.\nhttp_kwargs: A named tuple of HTTP keyword arguments.\napi_kwargs: A named tuple of API keyword arguments.\nkwargs: Prompt variables to be used to fill the prompt/template\n\nReturns\n\nmsg: An AIMessage object representing the generated AI message, including the content, status, tokens, and elapsed time.\n\nUse msg.content to access the extracted string.\n\nSee also: ai_str, aai_str, aigenerate, aiembed, aiclassify, aiextract\n\nNotes\n\nAll examples below use model \"gpt4v\", which is an alias for model ID \"gpt-4-vision-preview\"\nmax_tokens in the api_kwargs is preset to 2500, otherwise OpenAI enforces a default of only a few hundred tokens (~300). If your output is truncated, increase this value\n\nExample\n\nDescribe the provided image:\n\nmsg = aiscan(\"Describe the image\"; image_path=\"julia.png\", model=\"gpt4v\")\n# [ Info: Tokens: 1141 @ Cost: $0.0117 in 2.2 seconds\n# AIMessage(\"The image shows a logo consisting of the word \"julia\" written in lowercase\")\n\nYou can provide multiple images at once as a vector and ask for \"low\" level of detail (cheaper):\n\nmsg = aiscan(\"Describe the image\"; image_path=[\"julia.png\",\"python.png\"], image_detail=\"low\", model=\"gpt4v\")\n\nYou can use this function as a nice and quick OCR (transcribe text in the image) with a template :OCRTask.  Let's transcribe some SQL code from a screenshot (no more re-typing!):\n\n# Screenshot of some SQL code\nimage_url = \"https://www.sqlservercentral.com/wp-content/uploads/legacy/8755f69180b7ac7ee76a69ae68ec36872a116ad4/24622.png\"\nmsg = aiscan(:OCRTask; image_url, model=\"gpt4v\", task=\"Transcribe the SQL code in the image.\", api_kwargs=(; max_tokens=2500))\n\n# [ Info: Tokens: 362 @ Cost: $0.0045 in 2.5 seconds\n# AIMessage(\"```sql\n# update Orders <continue>\n\n# You can add syntax highlighting of the outputs via Markdown\nusing Markdown\nmsg.content |> Markdown.parse\n\nNotice that we enforce max_tokens = 2500. That's because OpenAI seems to default to ~300 tokens, which provides incomplete outputs. Hence, we set this value to 2500 as a default. If you still get truncated outputs, increase this value.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aitemplates","page":"Home","title":"PromptingTools.aitemplates","text":"aitemplates\n\nFind easily the most suitable templates for your use case.\n\nYou can search by:\n\nquery::Symbol which looks look only for partial matches in the template name\nquery::AbstractString which looks for partial matches in the template name or description\nquery::Regex which looks for matches in the template name, description or any of the message previews\n\nKeyword Arguments\n\nlimit::Int limits the number of returned templates (Defaults to 10)\n\nExamples\n\nFind available templates with aitemplates:\n\ntmps = aitemplates(\"JuliaExpertAsk\")\n# Will surface one specific template\n# 1-element Vector{AITemplateMetadata}:\n# PromptingTools.AITemplateMetadata\n#   name: Symbol JuliaExpertAsk\n#   description: String \"For asking questions about Julia language. Placeholders: `ask`\"\n#   version: String \"1\"\n#   wordcount: Int64 237\n#   variables: Array{Symbol}((1,))\n#   system_preview: String \"You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun\"\n#   user_preview: String \"# Question\n\n{{ask}}\"\n#   source: String \"\"\n\nThe above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).\n\nSearch for all Julia-related templates:\n\ntmps = aitemplates(\"Julia\")\n# 2-element Vector{AITemplateMetadata}... -> more to come later!\n\nIf you are on VSCode, you can leverage nice tabular display with vscodedisplay:\n\nusing DataFrames\ntmps = aitemplates(\"Julia\") |> DataFrame |> vscodedisplay\n\nI have my selected template, how do I use it? Just use the \"name\" in aigenerate or aiclassify   like you see in the first example!\n\n\n\n\n\n","category":"function"},{"location":"#PromptingTools.aitemplates-Tuple{AbstractString}","page":"Home","title":"PromptingTools.aitemplates","text":"Find the top-limit templates whose name or description fields partially match the query_key::String in TEMPLATE_METADATA.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aitemplates-Tuple{Regex}","page":"Home","title":"PromptingTools.aitemplates","text":"Find the top-limit templates where provided query_key::Regex matches either of name, description or previews or User or System messages in TEMPLATE_METADATA.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.aitemplates-Tuple{Symbol}","page":"Home","title":"PromptingTools.aitemplates","text":"Find the top-limit templates whose name::Symbol partially matches the query_name::Symbol in TEMPLATE_METADATA.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.function_call_signature-Tuple{Type}","page":"Home","title":"PromptingTools.function_call_signature","text":"function_call_signature(datastructtype::Struct; max_description_length::Int = 100)\n\nExtract the argument names, types and docstrings from a struct to create the function call signature in JSON schema.\n\nYou must provide a Struct type (not an instance of it) with some fields.\n\nNote: Fairly experimental, but works for combination of structs, arrays, strings and singletons.\n\nTips\n\nYou can improve the quality of the extraction by writing a helpful docstring for your struct (or any nested struct). It will be provided as a description. \n\nYou can even include comments/descriptions about the individual fields.\n\nAll fields are assumed to be required, unless you allow null values (eg, ::Union{Nothing, Int}). Fields with Nothing will be treated as optional.\nMissing values are ignored (eg, ::Union{Missing, Int} will be treated as Int). It's for broader compatibility and we cannot deserialize it as easily as Nothing.\n\nExample\n\nDo you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (return_type):\n\nstruct MyMeasurement\n    age::Int\n    height::Union{Int,Nothing}\n    weight::Union{Nothing,Float64}\nend\nsignature = function_call_signature(MyMeasurement)\n#\n# Dict{String, Any} with 3 entries:\n#   \"name\"        => \"MyMeasurement_extractor\"\n#   \"parameters\"  => Dict{String, Any}(\"properties\"=>Dict{String, Any}(\"height\"=>Dict{String, Any}(\"type\"=>\"integer\"), \"weight\"=>Dic…\n#   \"description\" => \"Represents person's age, height, and weight\n\"\n\nYou can see that only the field age does not allow null values, hence, it's \"required\". While height and weight are optional.\n\nsignature[\"parameters\"][\"required\"]\n# [\"age\"]\n\nIf there are multiple items you want to extract, define a wrapper struct to get a Vector of MyMeasurement:\n\nstruct MyMeasurementWrapper\n    measurements::Vector{MyMeasurement}\nend\n\nOr if you want your extraction to fail gracefully when data isn't found, use `MaybeExtract{T}` wrapper (inspired by Instructor package!):\n\nusing PromptingTools: MaybeExtract\n\ntype = MaybeExtract{MyMeasurement}\n\nEffectively the same as:\n\nstruct MaybeExtract{T}\n\nresult::Union{T, Nothing}\n\nerror::Bool // true if a result is found, false otherwise\n\nmessage::Union{Nothing, String} // Only present if no result is found, should be short and concise\n\nend\n\nIf LLM extraction fails, it will return a Dict with error and message fields instead of the result!\n\nmsg = aiextract(\"Extract measurements from the text: I am giraffe\", type)\n\n\n\nDict{Symbol, Any} with 2 entries:\n\n:message => \"Sorry, this feature is only available for humans.\"\n\n:error   => true\n\n``` That way, you can handle the error gracefully and get a reason why extraction failed.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.load_template-Tuple{Union{AbstractString, IO}}","page":"Home","title":"PromptingTools.load_template","text":"Loads messaging template from io_or_file and returns tuple of template messages and metadata.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.load_templates!","page":"Home","title":"PromptingTools.load_templates!","text":"load_templates!(; remove_templates::Bool=true)\n\nLoads templates from folder templates/ in the package root and stores them in TEMPLATE_STORE and TEMPLATE_METADATA.\n\nNote: Automatically removes any existing templates and metadata from TEMPLATE_STORE and TEMPLATE_METADATA if remove_templates=true.\n\n\n\n\n\n","category":"function"},{"location":"#PromptingTools.ollama_api-Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}","page":"Home","title":"PromptingTools.ollama_api","text":"ollama_api(prompt_schema::AbstractOllamaManagedSchema, prompt::AbstractString,\n    system::Union{Nothing, AbstractString} = nothing,\n    endpoint::String = \"generate\";\n    model::String = \"llama2\", http_kwargs::NamedTuple = NamedTuple(),\n    stream::Bool = false,\n    url::String = \"localhost\", port::Int = 11434,\n    kwargs...)\n\nSimple wrapper for a call to Ollama API.\n\nKeyword Arguments\n\nprompt_schema: Defines which prompt template should be applied.\nprompt: Can be a string representing the prompt for the AI conversation, a UserMessage, a vector of AbstractMessage\nsystem: An optional string representing the system message for the AI conversation. If not provided, a default message will be used.\nendpoint: The API endpoint to call, only \"generate\" and \"embeddings\" are currently supported. Defaults to \"generate\".\nmodel: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in MODEL_ALIASES.\nhttp_kwargs::NamedTuple: Additional keyword arguments for the HTTP request. Defaults to empty NamedTuple.\nstream: A boolean indicating whether to stream the response. Defaults to false.\nurl: The URL of the Ollama API. Defaults to \"localhost\".\nport: The port of the Ollama API. Defaults to 11434.\nkwargs: Prompt variables to be used to fill the prompt/template\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.remove_templates!-Tuple{}","page":"Home","title":"PromptingTools.remove_templates!","text":"    remove_templates!()\n\nRemoves all templates from TEMPLATE_STORE and TEMPLATE_METADATA.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.render-Tuple{AITemplate}","page":"Home","title":"PromptingTools.render","text":"Renders provided messaging template (template) under the default schema (PROMPT_SCHEMA).\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{<:PromptingTools.AbstractMessage}}","page":"Home","title":"PromptingTools.render","text":"render(schema::AbstractOllamaManagedSchema,\n    messages::Vector{<:AbstractMessage};\n    kwargs...)\n\nBuilds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that {{key}}=>value in the template.\n\nNote: Due to its \"managed\" nature, at most 2 messages can be provided (system and prompt inputs in the API).\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{<:PromptingTools.AbstractMessage}}","page":"Home","title":"PromptingTools.render","text":"render(schema::AbstractOpenAISchema,\n    messages::Vector{<:AbstractMessage};\n    image_detail::AbstractString = \"auto\",\n    kwargs...)\n\nBuilds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that {{key}}=>value in the template.\n\nArguments\n\nimage_detail: Only for UserMessageWithImages. It represents the level of detail to include for images. Can be \"auto\", \"high\", or \"low\".\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{<:PromptingTools.AbstractChatMessage}}","page":"Home","title":"PromptingTools.save_template","text":"Saves provided messaging template (messages) to io_or_file. Automatically adds metadata based on provided keyword arguments.\n\n\n\n\n\n","category":"method"},{"location":"#PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}","page":"Home","title":"PromptingTools.@aai_str","text":"aai\"user_prompt\"[model_alias] -> AIMessage\n\nAsynchronous version of @ai_str macro, which will log the result once it's ready.\n\nExample\n\nSend asynchronous request to GPT-4, so we don't have to wait for the response: Very practical with slow models, so you can keep working in the meantime.\n\n```julia m = aai\"Say Hi!\"gpt4; \n\n...with some delay...\n\n[ Info: Tokens: 29 @ Cost: 0.0011 in 2.7 seconds\n\n[ Info: AIMessage> Hello! How can I assist you today?\n\n\n\n\n\n","category":"macro"},{"location":"#PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}","page":"Home","title":"PromptingTools.@ai_str","text":"ai\"user_prompt\"[model_alias] -> AIMessage\n\nThe ai\"\" string macro generates an AI response to a given prompt by using aigenerate under the hood.\n\nArguments\n\nuser_prompt (String): The input prompt for the AI model.\nmodel_alias (optional, any): Provide model alias of the AI model (see MODEL_ALIASES).\n\nReturns\n\nAIMessage corresponding to the input prompt.\n\nExample\n\nresult = ai\"Hello, how are you?\"\n# AIMessage(\"Hello! I'm an AI assistant, so I don't have feelings, but I'm here to help you. How can I assist you today?\")\n\nIf you want to interpolate some variables or additional context, simply use string interpolation:\n\na=1\nresult = ai\"What is `$a+$a`?\"\n# AIMessage(\"The sum of `1+1` is `2`.\")\n\nIf you want to use a different model, eg, GPT-4, you can provide its alias as a flag:\n\nresult = ai\"What is `1.23 * 100 + 1`?\"gpt4\n# AIMessage(\"The answer is 124.\")\n\n\n\n\n\n","category":"macro"}]
}
