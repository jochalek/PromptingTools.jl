<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>PromptingTools.jl · PromptingTools.jl</title><meta name="title" content="PromptingTools.jl · PromptingTools.jl"/><meta property="og:title" content="PromptingTools.jl · PromptingTools.jl"/><meta property="twitter:title" content="PromptingTools.jl · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/reference/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/reference/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/reference/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../how_it_works/">How It Works</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/readme_examples/">Various examples</a></li><li><a class="tocitem" href="../examples/working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../examples/working_with_ollama/">Local models with Ollama.ai</a></li><li><a class="tocitem" href="../examples/working_with_google_ai_studio/">Google AIStudio</a></li><li><a class="tocitem" href="../examples/working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../examples/building_RAG/">Building RAG Application</a></li></ul></li><li><a class="tocitem" href="../frequently_asked_questions/">F.A.Q.</a></li><li><span class="tocitem">Reference</span><ul><li class="is-active"><a class="tocitem" href>PromptingTools.jl</a></li><li><a class="tocitem" href="../reference_experimental/">Experimental Modules</a></li><li><a class="tocitem" href="../reference_ragtools/">RAGTools</a></li><li><a class="tocitem" href="../reference_agenttools/">AgentTools</a></li><li><a class="tocitem" href="../reference_apitools/">APITools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>PromptingTools.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>PromptingTools.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/reference.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><ul><li><a href="../reference_experimental/#PromptingTools.Experimental"><code>PromptingTools.Experimental</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools"><code>PromptingTools.Experimental.AgentTools</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools"><code>PromptingTools.Experimental.RAGTools</code></a></li><li><a href="#PromptingTools.ALLOWED_PREFERENCES"><code>PromptingTools.ALLOWED_PREFERENCES</code></a></li><li><a href="#PromptingTools.CONV_HISTORY"><code>PromptingTools.CONV_HISTORY</code></a></li><li><a href="#PromptingTools.MODEL_ALIASES"><code>PromptingTools.MODEL_ALIASES</code></a></li><li><a href="#PromptingTools.MODEL_REGISTRY"><code>PromptingTools.MODEL_REGISTRY</code></a></li><li><a href="#PromptingTools.OPENAI_TOKEN_IDS"><code>PromptingTools.OPENAI_TOKEN_IDS</code></a></li><li><a href="#PromptingTools.PREFERENCES"><code>PromptingTools.PREFERENCES</code></a></li><li><a href="#PromptingTools.RESERVED_KWARGS"><code>PromptingTools.RESERVED_KWARGS</code></a></li><li><a href="#PromptingTools.AICode"><code>PromptingTools.AICode</code></a></li><li><a href="#PromptingTools.AIMessage"><code>PromptingTools.AIMessage</code></a></li><li><a href="#PromptingTools.AITemplate"><code>PromptingTools.AITemplate</code></a></li><li><a href="#PromptingTools.AITemplateMetadata"><code>PromptingTools.AITemplateMetadata</code></a></li><li><a href="#PromptingTools.AbstractPromptSchema"><code>PromptingTools.AbstractPromptSchema</code></a></li><li><a href="#PromptingTools.ChatMLSchema"><code>PromptingTools.ChatMLSchema</code></a></li><li><a href="#PromptingTools.CustomOpenAISchema"><code>PromptingTools.CustomOpenAISchema</code></a></li><li><a href="#PromptingTools.DataMessage"><code>PromptingTools.DataMessage</code></a></li><li><a href="#PromptingTools.DatabricksOpenAISchema"><code>PromptingTools.DatabricksOpenAISchema</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AICall"><code>PromptingTools.Experimental.AgentTools.AICall</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AICodeFixer"><code>PromptingTools.Experimental.AgentTools.AICodeFixer</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.RetryConfig"><code>PromptingTools.Experimental.AgentTools.RetryConfig</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.SampleNode"><code>PromptingTools.Experimental.AgentTools.SampleNode</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.ThompsonSampling"><code>PromptingTools.Experimental.AgentTools.ThompsonSampling</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.UCT"><code>PromptingTools.Experimental.AgentTools.UCT</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.ChunkIndex"><code>PromptingTools.Experimental.RAGTools.ChunkIndex</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.JudgeAllScores"><code>PromptingTools.Experimental.RAGTools.JudgeAllScores</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.JudgeRating"><code>PromptingTools.Experimental.RAGTools.JudgeRating</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.MultiIndex"><code>PromptingTools.Experimental.RAGTools.MultiIndex</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.RAGContext"><code>PromptingTools.Experimental.RAGTools.RAGContext</code></a></li><li><a href="#PromptingTools.FireworksOpenAISchema"><code>PromptingTools.FireworksOpenAISchema</code></a></li><li><a href="#PromptingTools.GoogleSchema"><code>PromptingTools.GoogleSchema</code></a></li><li><a href="#PromptingTools.ItemsExtract"><code>PromptingTools.ItemsExtract</code></a></li><li><a href="#PromptingTools.LocalServerOpenAISchema"><code>PromptingTools.LocalServerOpenAISchema</code></a></li><li><a href="#PromptingTools.MaybeExtract"><code>PromptingTools.MaybeExtract</code></a></li><li><a href="#PromptingTools.MistralOpenAISchema"><code>PromptingTools.MistralOpenAISchema</code></a></li><li><a href="#PromptingTools.ModelSpec"><code>PromptingTools.ModelSpec</code></a></li><li><a href="#PromptingTools.NoSchema"><code>PromptingTools.NoSchema</code></a></li><li><a href="#PromptingTools.OllamaManagedSchema"><code>PromptingTools.OllamaManagedSchema</code></a></li><li><a href="#PromptingTools.OllamaSchema"><code>PromptingTools.OllamaSchema</code></a></li><li><a href="#PromptingTools.OpenAISchema"><code>PromptingTools.OpenAISchema</code></a></li><li><a href="#PromptingTools.TestEchoGoogleSchema"><code>PromptingTools.TestEchoGoogleSchema</code></a></li><li><a href="#PromptingTools.TestEchoOllamaManagedSchema"><code>PromptingTools.TestEchoOllamaManagedSchema</code></a></li><li><a href="#PromptingTools.TestEchoOllamaSchema"><code>PromptingTools.TestEchoOllamaSchema</code></a></li><li><a href="#PromptingTools.TestEchoOpenAISchema"><code>PromptingTools.TestEchoOpenAISchema</code></a></li><li><a href="#PromptingTools.TogetherOpenAISchema"><code>PromptingTools.TogetherOpenAISchema</code></a></li><li><a href="#PromptingTools.UserMessageWithImages-Tuple{AbstractString}"><code>PromptingTools.UserMessageWithImages</code></a></li><li><a href="#PromptingTools.X123"><code>PromptingTools.X123</code></a></li><li><a href="#OpenAI.create_chat-Tuple{PromptingTools.MistralOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a></li><li><a href="#OpenAI.create_chat-Tuple{PromptingTools.LocalServerOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a></li><li><a href="#OpenAI.create_chat-Tuple{PromptingTools.CustomOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a></li><li><a href="../reference_apitools/#PromptingTools.Experimental.APITools.create_websearch-Tuple{AbstractString}"><code>PromptingTools.Experimental.APITools.create_websearch</code></a></li><li><a href="../reference_apitools/#PromptingTools.Experimental.APITools.tavily_api-Tuple{}"><code>PromptingTools.Experimental.APITools.tavily_api</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AIClassify-Tuple"><code>PromptingTools.Experimental.AgentTools.AIClassify</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AIEmbed-Tuple"><code>PromptingTools.Experimental.AgentTools.AIEmbed</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AIExtract-Tuple"><code>PromptingTools.Experimental.AgentTools.AIExtract</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AIGenerate-Tuple"><code>PromptingTools.Experimental.AgentTools.AIGenerate</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.AIScan-Tuple"><code>PromptingTools.Experimental.AgentTools.AIScan</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.add_feedback!-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}, PromptingTools.Experimental.AgentTools.SampleNode}"><code>PromptingTools.Experimental.AgentTools.add_feedback!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.aicodefixer_feedback-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.Experimental.AgentTools.aicodefixer_feedback</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.airetry!"><code>PromptingTools.Experimental.AgentTools.airetry!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.backpropagate!-Tuple{PromptingTools.Experimental.AgentTools.SampleNode}"><code>PromptingTools.Experimental.AgentTools.backpropagate!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.beta_sample-Tuple{Real, Real}"><code>PromptingTools.Experimental.AgentTools.beta_sample</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.collect_all_feedback-Tuple{PromptingTools.Experimental.AgentTools.SampleNode}"><code>PromptingTools.Experimental.AgentTools.collect_all_feedback</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.evaluate_condition!"><code>PromptingTools.Experimental.AgentTools.evaluate_condition!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.expand!-Tuple{PromptingTools.Experimental.AgentTools.SampleNode, Any}"><code>PromptingTools.Experimental.AgentTools.expand!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.extract_config-Union{Tuple{T}, Tuple{Any, T}} where T"><code>PromptingTools.Experimental.AgentTools.extract_config</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.find_node-Tuple{PromptingTools.Experimental.AgentTools.SampleNode, Integer}"><code>PromptingTools.Experimental.AgentTools.find_node</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.gamma_sample-Tuple{Real, Real}"><code>PromptingTools.Experimental.AgentTools.gamma_sample</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.last_message-Tuple{PromptingTools.Experimental.AgentTools.AICallBlock}"><code>PromptingTools.Experimental.AgentTools.last_message</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.last_output-Tuple{PromptingTools.Experimental.AgentTools.AICallBlock}"><code>PromptingTools.Experimental.AgentTools.last_output</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.print_samples-Tuple{PromptingTools.Experimental.AgentTools.SampleNode}"><code>PromptingTools.Experimental.AgentTools.print_samples</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.remove_used_kwargs-Tuple{NamedTuple, AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.Experimental.AgentTools.remove_used_kwargs</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.reset_success!"><code>PromptingTools.Experimental.AgentTools.reset_success!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.run!-Tuple{AICodeFixer}"><code>PromptingTools.Experimental.AgentTools.run!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.run!-Tuple{PromptingTools.Experimental.AgentTools.AICallBlock}"><code>PromptingTools.Experimental.AgentTools.run!</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.score-Tuple{PromptingTools.Experimental.AgentTools.SampleNode, PromptingTools.Experimental.AgentTools.ThompsonSampling}"><code>PromptingTools.Experimental.AgentTools.score</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.score-Tuple{PromptingTools.Experimental.AgentTools.SampleNode, PromptingTools.Experimental.AgentTools.UCT}"><code>PromptingTools.Experimental.AgentTools.score</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.select_best"><code>PromptingTools.Experimental.AgentTools.select_best</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.split_multi_samples-Tuple{Any}"><code>PromptingTools.Experimental.AgentTools.split_multi_samples</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.truncate_conversation-Tuple{AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.Experimental.AgentTools.truncate_conversation</code></a></li><li><a href="../reference_agenttools/#PromptingTools.Experimental.AgentTools.unwrap_aicall_args-Tuple{Any}"><code>PromptingTools.Experimental.AgentTools.unwrap_aicall_args</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools._normalize"><code>PromptingTools.Experimental.RAGTools._normalize</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.airag"><code>PromptingTools.Experimental.RAGTools.airag</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.build_context-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, CandidateChunks}"><code>PromptingTools.Experimental.RAGTools.build_context</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.build_index"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.build_index-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_index</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.build_qa_evals-Tuple{Vector{&lt;:AbstractString}, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.build_qa_evals</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.build_tags"><code>PromptingTools.Experimental.RAGTools.build_tags</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.cohere_api-Tuple{}"><code>PromptingTools.Experimental.RAGTools.cohere_api</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.find_closest-Tuple{AbstractMatrix{&lt;:Real}, AbstractVector{&lt;:Real}}"><code>PromptingTools.Experimental.RAGTools.find_closest</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.get_chunks-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_chunks</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.get_embeddings-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_embeddings</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.get_metadata-Tuple{Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.get_metadata</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.metadata_extract-Tuple{PromptingTools.Experimental.RAGTools.MetadataItem}"><code>PromptingTools.Experimental.RAGTools.metadata_extract</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.rerank-Tuple{PromptingTools.Experimental.RAGTools.CohereRerank, PromptingTools.Experimental.RAGTools.AbstractChunkIndex, Any, Any}"><code>PromptingTools.Experimental.RAGTools.rerank</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.QAEvalItem, PromptingTools.Experimental.RAGTools.RAGContext}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.run_qa_evals-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractVector{&lt;:PromptingTools.Experimental.RAGTools.QAEvalItem}}"><code>PromptingTools.Experimental.RAGTools.run_qa_evals</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.score_retrieval_hit-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_hit</code></a></li><li><a href="../reference_ragtools/#PromptingTools.Experimental.RAGTools.score_retrieval_rank-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.Experimental.RAGTools.score_retrieval_rank</code></a></li><li><a href="#PromptingTools.aiclassify-Union{Tuple{T}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}} where T&lt;:Union{AbstractString, Tuple{var&quot;#s111&quot;, var&quot;#s110&quot;} where {var&quot;#s111&quot;&lt;:AbstractString, var&quot;#s110&quot;&lt;:AbstractString}}"><code>PromptingTools.aiclassify</code></a></li><li><a href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a></li><li><a href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a></li><li><a href="#PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiextract</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractGoogleSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a></li><li><a href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a></li><li><a href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{Regex}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{Symbol}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates-Tuple{AbstractString}"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.aitemplates"><code>PromptingTools.aitemplates</code></a></li><li><a href="#PromptingTools.auth_header-Tuple{Union{Nothing, AbstractString}}"><code>PromptingTools.auth_header</code></a></li><li><a href="#PromptingTools.call_cost-Tuple{Int64, Int64, String}"><code>PromptingTools.call_cost</code></a></li><li><a href="#PromptingTools.create_template-Tuple{AbstractString, AbstractString}"><code>PromptingTools.create_template</code></a></li><li><a href="#PromptingTools.decode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}, AIMessage}"><code>PromptingTools.decode_choices</code></a></li><li><a href="#PromptingTools.detect_base_main_overrides-Tuple{AbstractString}"><code>PromptingTools.detect_base_main_overrides</code></a></li><li><a href="#PromptingTools.encode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}}"><code>PromptingTools.encode_choices</code></a></li><li><a href="#PromptingTools.eval!-Tuple{PromptingTools.AbstractCodeBlock}"><code>PromptingTools.eval!</code></a></li><li><a href="#PromptingTools.extract_code_blocks-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.extract_code_blocks</code></a></li><li><a href="#PromptingTools.extract_code_blocks_fallback-Union{Tuple{T}, Tuple{T, AbstractString}} where T&lt;:AbstractString"><code>PromptingTools.extract_code_blocks_fallback</code></a></li><li><a href="#PromptingTools.extract_function_name-Tuple{AbstractString}"><code>PromptingTools.extract_function_name</code></a></li><li><a href="#PromptingTools.extract_function_names-Tuple{AbstractString}"><code>PromptingTools.extract_function_names</code></a></li><li><a href="#PromptingTools.extract_julia_imports-Tuple{AbstractString}"><code>PromptingTools.extract_julia_imports</code></a></li><li><a href="#PromptingTools.finalize_outputs-Tuple{Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}, Any, Union{Nothing, PromptingTools.AbstractMessage, AbstractVector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.finalize_outputs</code></a></li><li><a href="#PromptingTools.find_subsequence_positions-Tuple{Any, Any}"><code>PromptingTools.find_subsequence_positions</code></a></li><li><a href="#PromptingTools.function_call_signature-Tuple{Type}"><code>PromptingTools.function_call_signature</code></a></li><li><a href="#PromptingTools.get_preferences-Tuple{String}"><code>PromptingTools.get_preferences</code></a></li><li><a href="#PromptingTools.ggi_generate_content"><code>PromptingTools.ggi_generate_content</code></a></li><li><a href="#PromptingTools.has_julia_prompt-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.has_julia_prompt</code></a></li><li><a href="#PromptingTools.length_longest_common_subsequence-Tuple{Any, Any}"><code>PromptingTools.length_longest_common_subsequence</code></a></li><li><a href="#PromptingTools.list_aliases-Tuple{}"><code>PromptingTools.list_aliases</code></a></li><li><a href="#PromptingTools.list_registry-Tuple{}"><code>PromptingTools.list_registry</code></a></li><li><a href="#PromptingTools.load_conversation-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_conversation</code></a></li><li><a href="#PromptingTools.load_template-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_template</code></a></li><li><a href="#PromptingTools.load_templates!"><code>PromptingTools.load_templates!</code></a></li><li><a href="#PromptingTools.ollama_api"><code>PromptingTools.ollama_api</code></a></li><li><a href="#PromptingTools.preview"><code>PromptingTools.preview</code></a></li><li><a href="#PromptingTools.push_conversation!-Tuple{Vector{&lt;:Vector}, AbstractVector, Union{Nothing, Int64}}"><code>PromptingTools.push_conversation!</code></a></li><li><a href="#PromptingTools.register_model!"><code>PromptingTools.register_model!</code></a></li><li><a href="#PromptingTools.remove_julia_prompt-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.remove_julia_prompt</code></a></li><li><a href="#PromptingTools.remove_templates!-Tuple{}"><code>PromptingTools.remove_templates!</code></a></li><li><a href="#PromptingTools.remove_unsafe_lines-Tuple{AbstractString}"><code>PromptingTools.remove_unsafe_lines</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{AITemplate}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.NoSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.render-Tuple{PromptingTools.AbstractGoogleSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a></li><li><a href="#PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.replace_words</code></a></li><li><a href="#PromptingTools.resize_conversation!-Tuple{Any, Union{Nothing, Int64}}"><code>PromptingTools.resize_conversation!</code></a></li><li><a href="#PromptingTools.response_to_message-Tuple{PromptingTools.AbstractOpenAISchema, Type{AIMessage}, Any, Any}"><code>PromptingTools.response_to_message</code></a></li><li><a href="#PromptingTools.response_to_message-Union{Tuple{T}, Tuple{PromptingTools.AbstractPromptSchema, Type{T}, Any, Any}} where T"><code>PromptingTools.response_to_message</code></a></li><li><a href="#PromptingTools.save_conversation-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.save_conversation</code></a></li><li><a href="#PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}"><code>PromptingTools.save_template</code></a></li><li><a href="#PromptingTools.set_preferences!-Tuple{Vararg{Pair{String}}}"><code>PromptingTools.set_preferences!</code></a></li><li><a href="#PromptingTools.split_by_length-Tuple{String}"><code>PromptingTools.split_by_length</code></a></li><li><a href="#PromptingTools.split_by_length-Tuple{Any, Vector{String}}"><code>PromptingTools.split_by_length</code></a></li><li><a href="#PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@aai_str</code></a></li><li><a href="#PromptingTools.@ai!_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai!_str</code></a></li><li><a href="#PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai_str</code></a></li><li><a href="#PromptingTools.@timeout-Tuple{Any, Any, Any}"><code>PromptingTools.@timeout</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ALLOWED_PREFERENCES" href="#PromptingTools.ALLOWED_PREFERENCES"><code>PromptingTools.ALLOWED_PREFERENCES</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Keys that are allowed to be set via <code>set_preferences!</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.CONV_HISTORY" href="#PromptingTools.CONV_HISTORY"><code>PromptingTools.CONV_HISTORY</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">CONV_HISTORY</code></pre><p>Tracks the most recent conversations through the <code>ai_str macros</code>.</p><p>Preference available: MAX<em>HISTORY</em>LENGTH, which sets how many last messages should be remembered.</p><p>See also: <code>push_conversation!</code>, <code>resize_conversation!</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L166-L175">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.MODEL_ALIASES" href="#PromptingTools.MODEL_ALIASES"><code>PromptingTools.MODEL_ALIASES</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">MODEL_ALIASES</code></pre><p>A dictionary of model aliases. Aliases are used to refer to models by their aliases instead of their full names to make it more convenient to use them.</p><p><strong>Accessing the aliases</strong></p><pre><code class="nohighlight hljs">PromptingTools.MODEL_ALIASES[&quot;gpt3&quot;]</code></pre><p><strong>Register a new model alias</strong></p><pre><code class="language-julia hljs">PromptingTools.MODEL_ALIASES[&quot;gpt3&quot;] = &quot;gpt-3.5-turbo&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L540-L554">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.MODEL_REGISTRY" href="#PromptingTools.MODEL_REGISTRY"><code>PromptingTools.MODEL_REGISTRY</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">MODEL_REGISTRY</code></pre><p>A store of available model names and their specs (ie, name, costs per token, etc.)</p><p><strong>Accessing the registry</strong></p><p>You can use both the alias name or the full name to access the model spec:</p><pre><code class="nohighlight hljs">PromptingTools.MODEL_REGISTRY[&quot;gpt-3.5-turbo&quot;]</code></pre><p><strong>Registering a new model</strong></p><pre><code class="language-julia hljs">register_model!(
    name = &quot;gpt-3.5-turbo&quot;,
    schema = :OpenAISchema,
    cost_of_token_prompt = 0.0015,
    cost_of_token_generation = 0.002,
    description = &quot;GPT-3.5 Turbo is a 175B parameter model and a common default on the OpenAI API.&quot;)</code></pre><p><strong>Registering a model alias</strong></p><pre><code class="language-julia hljs">PromptingTools.MODEL_ALIASES[&quot;gpt3&quot;] = &quot;gpt-3.5-turbo&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L470-L497">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OPENAI_TOKEN_IDS" href="#PromptingTools.OPENAI_TOKEN_IDS"><code>PromptingTools.OPENAI_TOKEN_IDS</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>Token IDs for GPT3.5 and GPT4 from https://platform.openai.com/tokenizer</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L601">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.PREFERENCES" href="#PromptingTools.PREFERENCES"><code>PromptingTools.PREFERENCES</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">PREFERENCES</code></pre><p>You can set preferences for PromptingTools by setting environment variables (for <code>OPENAI_API_KEY</code> only)      or by using the <code>set_preferences!</code>.     It will create a <code>LocalPreferences.toml</code> file in your current directory and will reload your prefences from there.</p><p>Check your preferences by calling <code>get_preferences(key::String)</code>.</p><p><strong>Available Preferences (for <code>set_preferences!</code>)</strong></p><ul><li><code>OPENAI_API_KEY</code>: The API key for the OpenAI API. See <a href="https://platform.openai.com/docs/quickstart?context=python">OpenAI&#39;s documentation</a> for more information.</li><li><code>MISTRALAI_API_KEY</code>: The API key for the Mistral AI API. See <a href="https://docs.mistral.ai/">Mistral AI&#39;s documentation</a> for more information.</li><li><code>COHERE_API_KEY</code>: The API key for the Cohere API. See <a href="https://docs.cohere.com/docs/the-cohere-platform">Cohere&#39;s documentation</a> for more information.</li><li><code>DATABRICKS_API_KEY</code>: The API key for the Databricks Foundation Model API. See <a href="https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html">Databricks&#39; documentation</a> for more information.</li><li><code>DATABRICKS_HOST</code>: The host for the Databricks API. See <a href="https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html">Databricks&#39; documentation</a> for more information.</li><li><code>TAVILY_API_KEY</code>: The API key for the Tavily Search API. Register <a href="https://tavily.com/">here</a>. See more information <a href="https://docs.tavily.com/docs/tavily-api/rest_api">here</a>.</li><li><code>GOOGLE_API_KEY</code>: The API key for Google Gemini models. Get yours from <a href="https://ai.google.dev/">here</a>. If you see a documentation page (&quot;Available languages and regions for Google AI Studio and Gemini API&quot;), it means that it&#39;s not yet available in your region.</li><li><code>MODEL_CHAT</code>: The default model to use for aigenerate and most ai* calls. See <code>MODEL_REGISTRY</code> for a list of available models or define your own.</li><li><code>MODEL_EMBEDDING</code>: The default model to use for aiembed (embedding documents). See <code>MODEL_REGISTRY</code> for a list of available models or define your own.</li><li><code>PROMPT_SCHEMA</code>: The default prompt schema to use for aigenerate and most ai* calls (if not specified in <code>MODEL_REGISTRY</code>). Set as a string, eg, <code>&quot;OpenAISchema&quot;</code>.   See <code>PROMPT_SCHEMA</code> for more information.</li><li><code>MODEL_ALIASES</code>: A dictionary of model aliases (<code>alias =&gt; full_model_name</code>). Aliases are used to refer to models by their aliases instead of their full names to make it more convenient to use them.   See <code>MODEL_ALIASES</code> for more information.</li><li><code>MAX_HISTORY_LENGTH</code>: The maximum length of the conversation history. Defaults to 5. Set to <code>nothing</code> to disable history.   See <code>CONV_HISTORY</code> for more information.</li><li><code>LOCAL_SERVER</code>: The URL of the local server to use for <code>ai*</code> calls. Defaults to <code>http://localhost:10897/v1</code>. This server is called when you call <code>model=&quot;local&quot;</code>   See <code>?LocalServerOpenAISchema</code> for more information and examples.</li></ul><p>At the moment it is not possible to persist changes to <code>MODEL_REGISTRY</code> across sessions.  Define your <code>register_model!()</code> calls in your <code>startup.jl</code> file to make them available across sessions or put them at the top of your script.</p><p><strong>Available ENV Variables</strong></p><ul><li><code>OPENAI_API_KEY</code>: The API key for the OpenAI API. </li><li><code>MISTRALAI_API_KEY</code>: The API key for the Mistral AI API.</li><li><code>COHERE_API_KEY</code>: The API key for the Cohere API.</li><li><code>LOCAL_SERVER</code>: The URL of the local server to use for <code>ai*</code> calls. Defaults to <code>http://localhost:10897/v1</code>. This server is called when you call <code>model=&quot;local&quot;</code></li><li><code>DATABRICKS_API_KEY</code>: The API key for the Databricks Foundation Model API.</li><li><code>DATABRICKS_HOST</code>: The host for the Databricks API.</li><li><code>TAVILY_API_KEY</code>: The API key for the Tavily Search API. Register <a href="https://tavily.com/">here</a>. See more information <a href="https://docs.tavily.com/docs/tavily-api/rest_api">here</a>.</li><li><code>GOOGLE_API_KEY</code>: The API key for Google Gemini models. Get yours from <a href="https://ai.google.dev/">here</a>. If you see a documentation page (&quot;Available languages and regions for Google AI Studio and Gemini API&quot;), it means that it&#39;s not yet available in your region.</li></ul><p>Preferences.jl takes priority over ENV variables, so if you set a preference, it will take precedence over the ENV variable.</p><p>WARNING: NEVER EVER sync your <code>LocalPreferences.toml</code> file! It contains your API key and other sensitive information!!!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L4-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.RESERVED_KWARGS" href="#PromptingTools.RESERVED_KWARGS"><code>PromptingTools.RESERVED_KWARGS</code></a> — <span class="docstring-category">Constant</span></header><section><div><p>The following keywords are reserved for internal use in the <code>ai*</code> functions and cannot be used as placeholders in the Messages</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/PromptingTools.jl#L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AICode" href="#PromptingTools.AICode"><code>PromptingTools.AICode</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AICode(code::AbstractString; auto_eval::Bool=true, safe_eval::Bool=false, 
skip_unsafe::Bool=false, capture_stdout::Bool=true, verbose::Bool=false,
prefix::AbstractString=&quot;&quot;, suffix::AbstractString=&quot;&quot;, remove_tests::Bool=false, execution_timeout::Int = 60)

AICode(msg::AIMessage; auto_eval::Bool=true, safe_eval::Bool=false, 
skip_unsafe::Bool=false, skip_invalid::Bool=false, capture_stdout::Bool=true,
verbose::Bool=false, prefix::AbstractString=&quot;&quot;, suffix::AbstractString=&quot;&quot;, remove_tests::Bool=false, execution_timeout::Int = 60)</code></pre><p>A mutable structure representing a code block (received from the AI model) with automatic parsing, execution, and output/error capturing capabilities.</p><p>Upon instantiation with a string, the <code>AICode</code> object automatically runs a code parser and executor (via <code>PromptingTools.eval!()</code>), capturing any standard output (<code>stdout</code>) or errors.  This structure is useful for programmatically handling and evaluating Julia code snippets.</p><p>See also: <code>PromptingTools.extract_code_blocks</code>, <code>PromptingTools.eval!</code></p><p><strong>Workflow</strong></p><ul><li>Until <code>cb::AICode</code> has been evaluated, <code>cb.success</code> is set to <code>nothing</code> (and so are all other fields).</li><li>The text in <code>cb.code</code> is parsed (saved to <code>cb.expression</code>).</li><li>The parsed expression is evaluated.</li><li>Outputs of the evaluated expression are captured in <code>cb.output</code>.</li><li>Any <code>stdout</code> outputs (e.g., from <code>println</code>) are captured in <code>cb.stdout</code>.</li><li>If an error occurs during evaluation, it is saved in <code>cb.error</code>.</li><li>After successful evaluation without errors, <code>cb.success</code> is set to <code>true</code>.  Otherwise, it is set to <code>false</code> and you can inspect the <code>cb.error</code> to understand why.</li></ul><p><strong>Properties</strong></p><ul><li><code>code::AbstractString</code>: The raw string of the code to be parsed and executed.</li><li><code>expression</code>: The parsed Julia expression (set after parsing <code>code</code>).</li><li><code>stdout</code>: Captured standard output from the execution of the code.</li><li><code>output</code>: The result of evaluating the code block.</li><li><code>success::Union{Nothing, Bool}</code>: Indicates whether the code block executed successfully (<code>true</code>), unsuccessfully (<code>false</code>), or has yet to be evaluated (<code>nothing</code>).</li><li><code>error::Union{Nothing, Exception}</code>: Any exception raised during the execution of the code block.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>auto_eval::Bool</code>: If set to <code>true</code>, the code block is automatically parsed and evaluated upon instantiation. Defaults to <code>true</code>.</li><li><code>safe_eval::Bool</code>: If set to <code>true</code>, the code block checks for package operations (e.g., installing new packages) and missing imports, and then evaluates the code inside a bespoke scratch module. This is to ensure that the evaluation does not alter any user-defined variables or the global state. Defaults to <code>false</code>.</li><li><code>skip_unsafe::Bool</code>: If set to <code>true</code>, we skip any lines in the code block that are deemed unsafe (eg, <code>Pkg</code> operations). Defaults to <code>false</code>.</li><li><code>skip_invalid::Bool</code>: If set to <code>true</code>, we skip code blocks that do not even parse. Defaults to <code>false</code>.</li><li><code>verbose::Bool</code>: If set to <code>true</code>, we print out any lines that are skipped due to being unsafe. Defaults to <code>false</code>.</li><li><code>capture_stdout::Bool</code>: If set to <code>true</code>, we capture any stdout outputs (eg, test failures) in <code>cb.stdout</code>. Defaults to <code>true</code>.</li><li><code>prefix::AbstractString</code>: A string to be prepended to the code block before parsing and evaluation. Useful to add some additional code definition or necessary imports. Defaults to an empty string.</li><li><code>suffix::AbstractString</code>: A string to be appended to the code block before parsing and evaluation.  Useful to check that tests pass or that an example executes. Defaults to an empty string.</li><li><code>remove_tests::Bool</code>: If set to <code>true</code>, we remove any <code>@test</code> or <code>@testset</code> macros from the code block before parsing and evaluation. Defaults to <code>false</code>.</li><li><code>execution_timeout::Int</code>: The maximum time (in seconds) allowed for the code block to execute. Defaults to 60 seconds.</li></ul><p><strong>Methods</strong></p><ul><li><code>Base.isvalid(cb::AICode)</code>: Check if the code block has executed successfully. Returns <code>true</code> if <code>cb.success == true</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">code = AICode(&quot;println(&quot;Hello, World!&quot;)&quot;) # Auto-parses and evaluates the code, capturing output and errors.
isvalid(code) # Output: true
code.stdout # Output: &quot;Hello, World!
&quot;</code></pre><p>We try to evaluate &quot;safely&quot; by default (eg, inside a custom module, to avoid changing user variables).   You can avoid that with <code>save_eval=false</code>:</p><pre><code class="language-julia hljs">code = AICode(&quot;new_variable = 1&quot;; safe_eval=false)
isvalid(code) # Output: true
new_variable # Output: 1</code></pre><p>You can also call AICode directly on an AIMessage, which will extract the Julia code blocks, concatenate them and evaluate them:</p><pre><code class="language-julia hljs">msg = aigenerate(&quot;In Julia, how do you create a vector of 10 random numbers?&quot;)
code = AICode(msg)
# Output: AICode(Success: True, Parsed: True, Evaluated: True, Error Caught: N/A, StdOut: True, Code: 2 Lines)

# show the code
code.code |&gt; println
# Output: 
# numbers = rand(10)
# numbers = rand(1:100, 10)

# or copy it to the clipboard
code.code |&gt; clipboard

# or execute it in the current module (=Main)
eval(code.expression)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_eval.jl#L18-L106">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AIMessage" href="#PromptingTools.AIMessage"><code>PromptingTools.AIMessage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AIMessage</code></pre><p>A message type for AI-generated text-based responses.  Returned by <code>aigenerate</code>, <code>aiclassify</code>, and <code>aiscan</code> functions.</p><p><strong>Fields</strong></p><ul><li><code>content::Union{AbstractString, Nothing}</code>: The content of the message.</li><li><code>status::Union{Int, Nothing}</code>: The status of the message from the API.</li><li><code>tokens::Tuple{Int, Int}</code>: The number of tokens used (prompt,completion).</li><li><code>elapsed::Float64</code>: The time taken to generate the response in seconds.</li><li><code>cost::Union{Nothing, Float64}</code>: The cost of the API call (calculated with information from <code>MODEL_REGISTRY</code>).</li><li><code>log_prob::Union{Nothing, Float64}</code>: The log probability of the response.</li><li><code>finish_reason::Union{Nothing, String}</code>: The reason the response was finished.</li><li><code>run_id::Union{Nothing, Int}</code>: The unique ID of the run.</li><li><code>sample_id::Union{Nothing, Int}</code>: The unique ID of the sample (if multiple samples are generated, they will all have the same <code>run_id</code>).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/messages.jl#L65-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AITemplate" href="#PromptingTools.AITemplate"><code>PromptingTools.AITemplate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AITemplate</code></pre><p>AITemplate is a template for a conversation prompt.   This type is merely a container for the template name, which is resolved into a set of messages (=prompt) by <code>render</code>.</p><p><strong>Naming Convention</strong></p><ul><li>Template names should be in CamelCase</li><li>Follow the format <code>&lt;Persona&gt;...&lt;Variable&gt;...</code> where possible, eg, <code>JudgeIsItTrue</code>, ``<ul><li>Starting with the Persona (=System prompt), eg, <code>Judge</code> = persona is meant to <code>judge</code> some provided information</li><li>Variable to be filled in with context, eg, <code>It</code> = placeholder <code>it</code></li><li>Ending with the variable name is helpful, eg, <code>JuliaExpertTask</code> for a persona to be an expert in Julia language and <code>task</code> is the placeholder name</li></ul></li><li>Ideally, the template name should be self-explanatory, eg, <code>JudgeIsItTrue</code> = persona is meant to <code>judge</code> some provided information where it is true or false</li></ul><p><strong>Examples</strong></p><p>Save time by re-using pre-made templates, just fill in the placeholders with the keyword arguments:</p><pre><code class="language-julia hljs">msg = aigenerate(:JuliaExpertAsk; ask = &quot;How do I add packages?&quot;)</code></pre><p>The above is equivalent to a more verbose version that explicitly uses the dispatch on <code>AITemplate</code>:</p><pre><code class="language-julia hljs">msg = aigenerate(AITemplate(:JuliaExpertAsk); ask = &quot;How do I add packages?&quot;)</code></pre><p>Find available templates with <code>aitemplates</code>:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;JuliaExpertAsk&quot;)
# Will surface one specific template
# 1-element Vector{AITemplateMetadata}:
# PromptingTools.AITemplateMetadata
#   name: Symbol JuliaExpertAsk
#   description: String &quot;For asking questions about Julia language. Placeholders: `ask`&quot;
#   version: String &quot;1&quot;
#   wordcount: Int64 237
#   variables: Array{Symbol}((1,))
#   system_preview: String &quot;You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun&quot;
#   user_preview: String &quot;# Question

{{ask}}&quot;
#   source: String &quot;&quot;</code></pre><p>The above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).</p><p>Search for all Julia-related templates:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;Julia&quot;)
# 2-element Vector{AITemplateMetadata}... -&gt; more to come later!</code></pre><p>If you are on VSCode, you can leverage nice tabular display with <code>vscodedisplay</code>:</p><pre><code class="language-julia hljs">using DataFrames
tmps = aitemplates(&quot;Julia&quot;) |&gt; DataFrame |&gt; vscodedisplay</code></pre><p>I have my selected template, how do I use it? Just use the &quot;name&quot; in <code>aigenerate</code> or <code>aiclassify</code>   like you see in the first example!</p><p>You can inspect any template by &quot;rendering&quot; it (this is what the LLM will see):</p><pre><code class="language-julia hljs">julia&gt; AITemplate(:JudgeIsItTrue) |&gt; PromptingTools.render</code></pre><p>See also: <code>save_template</code>, <code>load_template</code>, <code>load_templates!</code> for more advanced use cases (and the corresponding script in <code>examples/</code> folder)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L8-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AITemplateMetadata" href="#PromptingTools.AITemplateMetadata"><code>PromptingTools.AITemplateMetadata</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Helper for easy searching and reviewing of templates. Defined on loading of each template.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.AbstractPromptSchema" href="#PromptingTools.AbstractPromptSchema"><code>PromptingTools.AbstractPromptSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Defines different prompting styles based on the model training and fine-tuning.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ChatMLSchema" href="#PromptingTools.ChatMLSchema"><code>PromptingTools.ChatMLSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ChatMLSchema is used by many open-source chatbots, by OpenAI models (under the hood) and by several models and inferfaces (eg, Ollama, vLLM)</p><p>You can explore it on <a href="https://tiktokenizer.vercel.app/">tiktokenizer</a></p><p>It uses the following conversation structure:</p><pre><code class="nohighlight hljs">&lt;im_start&gt;system
...&lt;im_end&gt;
&lt;|im_start|&gt;user
...&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
...&lt;|im_end|&gt;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L202-L216">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.CustomOpenAISchema" href="#PromptingTools.CustomOpenAISchema"><code>PromptingTools.CustomOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CustomOpenAISchema</code></pre><p>CustomOpenAISchema() allows user to call any OpenAI-compatible API.</p><p>All user needs to do is to pass this schema as the first argument and provide the BASE URL of the API to call (<code>api_kwargs.url</code>).</p><p><strong>Example</strong></p><p>Assumes that we have a local server running at <code>http://127.0.0.1:8081</code>:</p><pre><code class="language-julia hljs">api_key = &quot;...&quot;
prompt = &quot;Say hi!&quot;
msg = aigenerate(CustomOpenAISchema(), prompt; model=&quot;my_model&quot;, api_key, api_kwargs=(; url=&quot;http://127.0.0.1:8081&quot;))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L47-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.DataMessage" href="#PromptingTools.DataMessage"><code>PromptingTools.DataMessage</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DataMessage</code></pre><p>A message type for AI-generated data-based responses, ie, different <code>content</code> than text.  Returned by <code>aiextract</code>, and <code>aiextract</code> functions.</p><p><strong>Fields</strong></p><ul><li><code>content::Union{AbstractString, Nothing}</code>: The content of the message.</li><li><code>status::Union{Int, Nothing}</code>: The status of the message from the API.</li><li><code>tokens::Tuple{Int, Int}</code>: The number of tokens used (prompt,completion).</li><li><code>elapsed::Float64</code>: The time taken to generate the response in seconds.</li><li><code>cost::Union{Nothing, Float64}</code>: The cost of the API call (calculated with information from <code>MODEL_REGISTRY</code>).</li><li><code>log_prob::Union{Nothing, Float64}</code>: The log probability of the response.</li><li><code>finish_reason::Union{Nothing, String}</code>: The reason the response was finished.</li><li><code>run_id::Union{Nothing, Int}</code>: The unique ID of the run.</li><li><code>sample_id::Union{Nothing, Int}</code>: The unique ID of the sample (if multiple samples are generated, they will all have the same <code>run_id</code>).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/messages.jl#L95-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.DatabricksOpenAISchema" href="#PromptingTools.DatabricksOpenAISchema"><code>PromptingTools.DatabricksOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DatabricksOpenAISchema</code></pre><p>DatabricksOpenAISchema() allows user to call Databricks Foundation Model API. <a href="https://docs.databricks.com/en/machine-learning/foundation-models/api-reference.html">API Reference</a></p><p>Requires two environment variables to be set:</p><ul><li><code>DATABRICKS_API_KEY</code>: Databricks token</li><li><code>DATABRICKS_HOST</code>: Address of the Databricks workspace (<code>https://&lt;workspace_host&gt;.databricks.com</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L138-L146">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.FireworksOpenAISchema" href="#PromptingTools.FireworksOpenAISchema"><code>PromptingTools.FireworksOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FireworksOpenAISchema</code></pre><p>Schema to call the <a href="https://fireworks.ai/">Fireworks.ai</a> API.</p><p>Links:</p><ul><li><a href="https://fireworks.ai/api-keys">Get your API key</a></li><li><a href="https://readme.fireworks.ai/reference/createchatcompletion">API Reference</a></li><li><a href="https://fireworks.ai/models">Available models</a></li></ul><p>Requires one environment variables to be set:</p><ul><li><code>FIREWORKS_API_KEY</code>: Your API key</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L149-L161">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.GoogleSchema" href="#PromptingTools.GoogleSchema"><code>PromptingTools.GoogleSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Calls Google&#39;s Gemini API. See more information <a href="https://aistudio.google.com/">here</a>. It&#39;s available only for <em>some</em> regions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L242">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ItemsExtract" href="#PromptingTools.ItemsExtract"><code>PromptingTools.ItemsExtract</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Extract zero, one or more specified items from the provided data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/extraction.jl#L185-L187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.LocalServerOpenAISchema" href="#PromptingTools.LocalServerOpenAISchema"><code>PromptingTools.LocalServerOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalServerOpenAISchema</code></pre><p>Designed to be used with local servers. It&#39;s automatically called with model alias &quot;local&quot; (see <code>MODEL_REGISTRY</code>).</p><p>This schema is a flavor of CustomOpenAISchema with a <code>url</code> key<code>preset by global Preference key</code>LOCAL_SERVER<code>. See</code>?PREFERENCES<code>for more details on how to change it. It assumes that the server follows OpenAI API conventions (eg,</code>POST /v1/chat/completions`).</p><p>Note: Llama.cpp (and hence Llama.jl built on top of it) do NOT support embeddings endpoint! You&#39;ll get an address error.</p><p><strong>Example</strong></p><p>Assumes that we have a local server running at <code>http://127.0.0.1:10897/v1</code> (port and address used by Llama.jl, &quot;v1&quot; at the end is needed for OpenAI endpoint compatibility):</p><p>Three ways to call it:</p><pre><code class="language-julia hljs">
# Use @ai_str with &quot;local&quot; alias
ai&quot;Say hi!&quot;local

# model=&quot;local&quot;
aigenerate(&quot;Say hi!&quot;; model=&quot;local&quot;)

# Or set schema explicitly
const PT = PromptingTools
msg = aigenerate(PT.LocalServerOpenAISchema(), &quot;Say hi!&quot;)</code></pre><p>How to start a LLM local server? You can use <code>run_server</code> function from <a href="https://github.com/marcom/Llama.jl">Llama.jl</a>. Use a separate Julia session.</p><pre><code class="language-julia hljs">using Llama
model = &quot;...path...&quot; # see Llama.jl README how to download one
run_server(; model)</code></pre><p>To change the default port and address:</p><pre><code class="language-julia hljs"># For a permanent change, set the preference:
using Preferences
set_preferences!(&quot;LOCAL_SERVER&quot;=&gt;&quot;http://127.0.0.1:10897/v1&quot;)

# Or if it&#39;s a temporary fix, just change the variable `LOCAL_SERVER`:
const PT = PromptingTools
PT.LOCAL_SERVER = &quot;http://127.0.0.1:10897/v1&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L67-L113">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.MaybeExtract" href="#PromptingTools.MaybeExtract"><code>PromptingTools.MaybeExtract</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Extract a result from the provided data, if any, otherwise set the error and message fields.</p><p><strong>Arguments</strong></p><ul><li><code>error::Bool</code>: <code>true</code> if a result is found, <code>false</code> otherwise.</li><li><code>message::String</code>: Only present if no result is found, should be short and concise.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/extraction.jl#L172-L178">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.MistralOpenAISchema" href="#PromptingTools.MistralOpenAISchema"><code>PromptingTools.MistralOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MistralOpenAISchema</code></pre><p>MistralOpenAISchema() allows user to call MistralAI API known for mistral and mixtral models.</p><p>It&#39;s a flavor of CustomOpenAISchema() with a url preset to <code>https://api.mistral.ai</code>.</p><p>Most models have been registered, so you don&#39;t even have to specify the schema</p><p><strong>Example</strong></p><p>Let&#39;s call <code>mistral-tiny</code> model:</p><pre><code class="language-julia hljs">api_key = &quot;...&quot; # can be set via ENV[&quot;MISTRAL_API_KEY&quot;] or via our preference system
msg = aigenerate(&quot;Say hi!&quot;; model=&quot;mistral_tiny&quot;, api_key)</code></pre><p>See <code>?PREFERENCES</code> for more details on how to set your API key permanently.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L116-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ModelSpec" href="#PromptingTools.ModelSpec"><code>PromptingTools.ModelSpec</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ModelSpec</code></pre><p>A struct that contains information about a model, such as its name, schema, cost per token, etc.</p><p><strong>Fields</strong></p><ul><li><code>name::String</code>: The name of the model. This is the name that will be used to refer to the model in the <code>ai*</code> functions.</li><li><code>schema::AbstractPromptSchema</code>: The schema of the model. This is the schema that will be used to generate prompts for the model, eg, <code>:OpenAISchema</code>.</li><li><code>cost_of_token_prompt::Float64</code>: The cost of 1 token in the prompt for this model. This is used to calculate the cost of a prompt.    Note: It is often provided online as cost per 1000 tokens, so make sure to convert it correctly!</li><li><code>cost_of_token_generation::Float64</code>: The cost of 1 token generated by this model. This is used to calculate the cost of a generation.   Note: It is often provided online as cost per 1000 tokens, so make sure to convert it correctly!</li><li><code>description::String</code>: A description of the model. This is used to provide more information about the model when it is queried.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">spec = ModelSpec(&quot;gpt-3.5-turbo&quot;,
    OpenAISchema(),
    0.0015,
    0.002,
    &quot;GPT-3.5 Turbo is a 175B parameter model and a common default on the OpenAI API.&quot;)

# register it
PromptingTools.register_model!(spec)</code></pre><p>But you can also register any model directly via keyword arguments:</p><pre><code class="language-julia hljs">PromptingTools.register_model!(
    name = &quot;gpt-3.5-turbo&quot;,
    schema = OpenAISchema(),
    cost_of_token_prompt = 0.0015,
    cost_of_token_generation = 0.002,
    description = &quot;GPT-3.5 Turbo is a 175B parameter model and a common default on the OpenAI API.&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L186-L221">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.NoSchema" href="#PromptingTools.NoSchema"><code>PromptingTools.NoSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Schema that keeps messages (&lt;:AbstractMessage) and does not transform for any specific model. It used by the first pass of the prompt rendering system (see <code>?render</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OllamaManagedSchema" href="#PromptingTools.OllamaManagedSchema"><code>PromptingTools.OllamaManagedSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Ollama by default manages different models and their associated prompt schemas when you pass <code>system_prompt</code> and <code>prompt</code> fields to the API.</p><p>Warning: It works only for 1 system message and 1 user message, so anything more than that has to be rejected.</p><p>If you need to pass more messagese / longer conversational history, you can use define the model-specific schema directly and pass your Ollama requests with <code>raw=true</code>,   which disables and templating and schema management by Ollama.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L222-L229">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OllamaSchema" href="#PromptingTools.OllamaSchema"><code>PromptingTools.OllamaSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>OllamaSchema is the default schema for Olama models.</p><p>It uses the following conversation template:</p><pre><code class="nohighlight hljs">[Dict(role=&quot;system&quot;,content=&quot;...&quot;),Dict(role=&quot;user&quot;,content=&quot;...&quot;),Dict(role=&quot;assistant&quot;,content=&quot;...&quot;)]</code></pre><p>It&#39;s very similar to OpenAISchema, but it appends images differently.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L181-L190">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.OpenAISchema" href="#PromptingTools.OpenAISchema"><code>PromptingTools.OpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>OpenAISchema is the default schema for OpenAI models.</p><p>It uses the following conversation template:</p><pre><code class="nohighlight hljs">[Dict(role=&quot;system&quot;,content=&quot;...&quot;),Dict(role=&quot;user&quot;,content=&quot;...&quot;),Dict(role=&quot;assistant&quot;,content=&quot;...&quot;)]</code></pre><p>It&#39;s recommended to separate sections in your prompt with markdown headers (e.g. `##Answer</p><p>`).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L27-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoGoogleSchema" href="#PromptingTools.TestEchoGoogleSchema"><code>PromptingTools.TestEchoGoogleSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L245">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoOllamaManagedSchema" href="#PromptingTools.TestEchoOllamaManagedSchema"><code>PromptingTools.TestEchoOllamaManagedSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L232">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoOllamaSchema" href="#PromptingTools.TestEchoOllamaSchema"><code>PromptingTools.TestEchoOllamaSchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L193">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TestEchoOpenAISchema" href="#PromptingTools.TestEchoOpenAISchema"><code>PromptingTools.TestEchoOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Echoes the user&#39;s input back to them. Used for testing the implementation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.TogetherOpenAISchema" href="#PromptingTools.TogetherOpenAISchema"><code>PromptingTools.TogetherOpenAISchema</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TogetherOpenAISchema</code></pre><p>Schema to call the <a href="https://www.together.ai/">Together.ai</a> API.</p><p>Links:</p><ul><li><a href="https://api.together.xyz/settings/api-keys">Get your API key</a></li><li><a href="https://docs.together.ai/docs/openai-api-compatibility">API Reference</a></li><li><a href="https://docs.together.ai/docs/inference-models">Available models</a></li></ul><p>Requires one environment variables to be set:</p><ul><li><code>TOGETHER_API_KEY</code>: Your API key</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L164-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.UserMessageWithImages-Tuple{AbstractString}" href="#PromptingTools.UserMessageWithImages-Tuple{AbstractString}"><code>PromptingTools.UserMessageWithImages</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Construct <code>UserMessageWithImages</code> with 1 or more images. Images can be either URLs or local paths.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/messages.jl#L142">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.X123" href="#PromptingTools.X123"><code>PromptingTools.X123</code></a> — <span class="docstring-category">Type</span></header><section><div><p>With docstring</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/precompilation.jl#L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OpenAI.create_chat-Tuple{PromptingTools.CustomOpenAISchema, AbstractString, AbstractString, Any}" href="#OpenAI.create_chat-Tuple{PromptingTools.CustomOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">OpenAI.create_chat(schema::CustomOpenAISchema,</code></pre><p>api_key::AbstractString,   model::AbstractString,   conversation;   url::String=&quot;http://localhost:8080&quot;,   kwargs...)</p><p>Dispatch to the OpenAI.create_chat function, for any OpenAI-compatible API. </p><p>It expects <code>url</code> keyword argument. Provide it to the <code>aigenerate</code> function via <code>api_kwargs=(; url=&quot;my-url&quot;)</code></p><p>It will forward your query to the &quot;chat/completions&quot; endpoint of the base URL that you provided (=<code>url</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L97-L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OpenAI.create_chat-Tuple{PromptingTools.LocalServerOpenAISchema, AbstractString, AbstractString, Any}" href="#OpenAI.create_chat-Tuple{PromptingTools.LocalServerOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">OpenAI.create_chat(schema::LocalServerOpenAISchema,
    api_key::AbstractString,
    model::AbstractString,
    conversation;
    url::String = &quot;http://localhost:8080&quot;,
    kwargs...)</code></pre><p>Dispatch to the OpenAI.create<em>chat function, but with the LocalServer API parameters, ie, defaults to <code>url</code> specified by the `LOCAL</em>SERVER<code>preference. See</code>?PREFERENCES`</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L123-L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OpenAI.create_chat-Tuple{PromptingTools.MistralOpenAISchema, AbstractString, AbstractString, Any}" href="#OpenAI.create_chat-Tuple{PromptingTools.MistralOpenAISchema, AbstractString, AbstractString, Any}"><code>OpenAI.create_chat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">OpenAI.create_chat(schema::MistralOpenAISchema,</code></pre><p>api_key::AbstractString,   model::AbstractString,   conversation;   url::String=&quot;https://api.mistral.ai/v1&quot;,   kwargs...)</p><p>Dispatch to the OpenAI.create_chat function, but with the MistralAI API parameters. </p><p>It tries to access the <code>MISTRALAI_API_KEY</code> ENV variable, but you can also provide it via the <code>api_key</code> keyword argument.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L143-L154">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiclassify-Union{Tuple{T}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}} where T&lt;:Union{AbstractString, Tuple{var&quot;#s111&quot;, var&quot;#s110&quot;} where {var&quot;#s111&quot;&lt;:AbstractString, var&quot;#s110&quot;&lt;:AbstractString}}" href="#PromptingTools.aiclassify-Union{Tuple{T}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}} where T&lt;:Union{AbstractString, Tuple{var&quot;#s111&quot;, var&quot;#s110&quot;} where {var&quot;#s111&quot;&lt;:AbstractString, var&quot;#s110&quot;&lt;:AbstractString}}"><code>PromptingTools.aiclassify</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiclassify(prompt_schema::AbstractOpenAISchema, prompt::ALLOWED_PROMPT_TYPE;
    choices::AbstractVector{T} = [&quot;true&quot;, &quot;false&quot;, &quot;unknown&quot;],
    api_kwargs::NamedTuple = NamedTuple(),
    kwargs...) where {T &lt;: Union{AbstractString, Tuple{&lt;:AbstractString, &lt;:AbstractString}}}</code></pre><p>Classifies the given prompt/statement into an arbitrary list of <code>choices</code>, which must be only the choices (vector of strings) or choices and descriptions are provided (vector of tuples, ie, <code>(&quot;choice&quot;,&quot;description&quot;)</code>).</p><p>It&#39;s quick and easy option for &quot;routing&quot; and similar use cases, as it exploits the logit bias trick and outputs only 1 token. classify into an arbitrary list of categories (including with descriptions). It&#39;s quick and easy option for &quot;routing&quot; and similar use cases, as it exploits the logit bias trick, so it outputs only 1 token.</p><p>!!! Note: The prompt/AITemplate must have a placeholder <code>choices</code> (ie, <code>{{choices}}</code>) that will be replaced with the encoded choices</p><p>Choices are rewritten into an enumerated list and mapped to a few known OpenAI tokens (maximum of 20 choices supported). Mapping of token IDs for GPT3.5/4 are saved in variable <code>OPENAI_TOKEN_IDS</code>.</p><p>It uses Logit bias trick and limits the output to 1 token to force the model to output only true/false/unknown. Credit for the idea goes to <a href="https://twitter.com/AAAzzam/status/1669753721574633473">AAAzzam</a>.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOpenAISchema</code>: The schema for the prompt.</li><li><code>prompt</code>: The prompt/statement to classify if it&#39;s a <code>String</code>. If it&#39;s a <code>Symbol</code>, it is expanded as a template via <code>render(schema,template)</code>. Eg, templates <code>:JudgeIsItTrue</code> or <code>:InputClassifier</code></li><li><code>choices::AbstractVector{T}</code>: The choices to be classified into. It can be a vector of strings or a vector of tuples, where the first element is the choice and the second is the description.</li></ul><p><strong>Example</strong></p><p>Given a user input, pick one of the two provided categories:</p><pre><code class="language-julia hljs">choices = [&quot;animal&quot;, &quot;plant&quot;]
input = &quot;Palm tree&quot;
aiclassify(:InputClassifier; choices, input)</code></pre><p>Choices with descriptions provided as tuples:</p><pre><code class="language-julia hljs">choices = [(&quot;A&quot;, &quot;any animal or creature&quot;), (&quot;P&quot;, &quot;for any plant or tree&quot;), (&quot;O&quot;, &quot;for everything else&quot;)]

# try the below inputs:
input = &quot;spider&quot; # -&gt; returns &quot;A&quot; for any animal or creature
input = &quot;daphodil&quot; # -&gt; returns &quot;P&quot; for any plant or tree
input = &quot;castle&quot; # -&gt; returns &quot;O&quot; for everything else
aiclassify(:InputClassifier; choices, input)</code></pre><p>You can still use a simple true/false classification:</p><pre><code class="language-julia hljs">aiclassify(&quot;Is two plus two four?&quot;) # true
aiclassify(&quot;Is two plus three a vegetable on Mars?&quot;) # false</code></pre><p><code>aiclassify</code> returns only true/false/unknown. It&#39;s easy to get the proper <code>Bool</code> output type out with <code>tryparse</code>, eg,</p><pre><code class="language-julia hljs">tryparse(Bool, aiclassify(&quot;Is two plus two four?&quot;)) isa Bool # true</code></pre><p>Output of type <code>Nothing</code> marks that the model couldn&#39;t classify the statement as true/false.</p><p>Ideally, we would like to re-use some helpful system prompt to get more accurate responses. For this reason we have templates, eg, <code>:JudgeIsItTrue</code>. By specifying the template, we can provide our statement as the expected variable (<code>it</code> in this case) See that the model now correctly classifies the statement as &quot;unknown&quot;.</p><pre><code class="language-julia hljs">aiclassify(:JudgeIsItTrue; it = &quot;Is two plus three a vegetable on Mars?&quot;) # unknown</code></pre><p>For better results, use higher quality models like gpt4, eg, </p><pre><code class="language-julia hljs">aiclassify(:JudgeIsItTrue;
    it = &quot;If I had two apples and I got three more, I have five apples now.&quot;,
    model = &quot;gpt4&quot;) # true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L756-L823">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function" href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString}, Tuple{PromptingTools.AbstractOllamaManagedSchema, AbstractString, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiembed(prompt_schema::AbstractOllamaManagedSchema,
        doc_or_docs::Union{AbstractString, AbstractVector{&lt;:AbstractString}},
        postprocess::F = identity;
        verbose::Bool = true,
        api_key::String = &quot;&quot;,
        model::String = MODEL_EMBEDDING,
        http_kwargs::NamedTuple = (retry_non_idempotent = true,
                                   retries = 5,
                                   readtimeout = 120),
        api_kwargs::NamedTuple = NamedTuple(),
        kwargs...) where {F &lt;: Function}</code></pre><p>The <code>aiembed</code> function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOllamaManagedSchema</code>: The schema for the prompt.</li><li><code>doc_or_docs::Union{AbstractString, AbstractVector{&lt;:AbstractString}}</code>: The document or list of documents to generate embeddings for. The list of documents is processed sequentially,  so users should consider implementing an async version with with <code>Threads.@spawn</code></li><li><code>postprocess::F</code>: The post-processing function to apply to each embedding. Defaults to the identity function, but could be <code>LinearAlgebra.normalize</code>.</li><li><code>verbose::Bool</code>: A flag indicating whether to print verbose information. Defaults to <code>true</code>.</li><li><code>api_key::String</code>: The API key to use for the OpenAI API. Defaults to <code>&quot;&quot;</code>.</li><li><code>model::String</code>: The model to use for generating embeddings. Defaults to <code>MODEL_EMBEDDING</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the Ollama API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: A <code>DataMessage</code> object containing the embeddings, status, token count, and elapsed time.</li></ul><p>Note: Ollama API currently does not return the token count, so it&#39;s set to <code>(0,0)</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, &quot;Hello World&quot;; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096-element JSON3.Array{Float64...</code></pre><p>We can embed multiple strings at once and they will be <code>hcat</code> into a matrix   (ie, each column corresponds to one string)</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, [&quot;Hello World&quot;, &quot;How are you?&quot;]; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096×2 Matrix{Float64}:</code></pre><p>If you plan to calculate the cosine distance between embeddings, you can normalize them first:</p><pre><code class="language-julia hljs">const PT = PromptingTools
using LinearAlgebra
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, [&quot;embed me&quot;, &quot;and me too&quot;], LinearAlgebra.normalize; model=&quot;openhermes2.5-mistral&quot;)

# calculate cosine distance between the two normalized embeddings as a simple dot product
msg.content&#39; * msg.content[:, 1] # [1.0, 0.34]</code></pre><p>Similarly, you can use the <code>postprocess</code> argument to materialize the data from JSON3.Object by using <code>postprocess = copy</code></p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

msg = aiembed(schema, &quot;Hello World&quot;, copy; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096-element Vector{Float64}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama_managed.jl#L241-L314">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}, F}} where F&lt;:Function" href="#PromptingTools.aiembed-Union{Tuple{F}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}}, Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, AbstractVector{&lt;:AbstractString}}, F}} where F&lt;:Function"><code>PromptingTools.aiembed</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiembed(prompt_schema::AbstractOpenAISchema,
        doc_or_docs::Union{AbstractString, AbstractVector{&lt;:AbstractString}},
        postprocess::F = identity;
        verbose::Bool = true,
        api_key::String = OPENAI_API_KEY,
        model::String = MODEL_EMBEDDING, 
        http_kwargs::NamedTuple = (retry_non_idempotent = true,
                                   retries = 5,
                                   readtimeout = 120),
        api_kwargs::NamedTuple = NamedTuple(),
        kwargs...) where {F &lt;: Function}</code></pre><p>The <code>aiembed</code> function generates embeddings for the given input using a specified model and returns a message object containing the embeddings, status, token count, and elapsed time.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema::AbstractOpenAISchema</code>: The schema for the prompt.</li><li><code>doc_or_docs::Union{AbstractString, AbstractVector{&lt;:AbstractString}}</code>: The document or list of documents to generate embeddings for.</li><li><code>postprocess::F</code>: The post-processing function to apply to each embedding. Defaults to the identity function.</li><li><code>verbose::Bool</code>: A flag indicating whether to print verbose information. Defaults to <code>true</code>.</li><li><code>api_key::String</code>: The API key to use for the OpenAI API. Defaults to <code>OPENAI_API_KEY</code>.</li><li><code>model::String</code>: The model to use for generating embeddings. Defaults to <code>MODEL_EMBEDDING</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to <code>(retry_non_idempotent = true, retries = 5, readtimeout = 120)</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the OpenAI API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs...</code>: Additional keyword arguments.</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: A <code>DataMessage</code> object containing the embeddings, status, token count, and elapsed time. Use <code>msg.content</code> to access the embeddings.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">msg = aiembed(&quot;Hello World&quot;)
msg.content # 1536-element JSON3.Array{Float64...</code></pre><p>We can embed multiple strings at once and they will be <code>hcat</code> into a matrix   (ie, each column corresponds to one string)</p><pre><code class="language-julia hljs">msg = aiembed([&quot;Hello World&quot;, &quot;How are you?&quot;])
msg.content # 1536×2 Matrix{Float64}:</code></pre><p>If you plan to calculate the cosine distance between embeddings, you can normalize them first:</p><pre><code class="language-julia hljs">using LinearAlgebra
msg = aiembed([&quot;embed me&quot;, &quot;and me too&quot;], LinearAlgebra.normalize)

# calculate cosine distance between the two normalized embeddings as a simple dot product
msg.content&#39; * msg.content[:, 1] # [1.0, 0.787]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L517-L569">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiextract-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiextract</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiextract(prompt_schema::AbstractOpenAISchema, prompt::ALLOWED_PROMPT_TYPE;
    return_type::Type,
    verbose::Bool = true,
    api_key::String = OPENAI_API_KEY,
    model::String = MODEL_CHAT,
    return_all::Bool = false, dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    http_kwargs::NamedTuple = (retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), api_kwargs::NamedTuple = (;
        tool_choice = &quot;exact&quot;),
    kwargs...)</code></pre><p>Extract required information (defined by a struct <strong><code>return_type</code></strong>) from the provided prompt by leveraging OpenAI function calling mode.</p><p>This is a perfect solution for extracting structured information from text (eg, extract organization names in news articles, etc.)</p><p>It&#39;s effectively a light wrapper around <code>aigenerate</code> call, which requires additional keyword argument <code>return_type</code> to be provided  and will enforce the model outputs to adhere to it.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>return_type</code>: A <strong>struct</strong> TYPE representing the the information we want to extract. Do not provide a struct instance, only the type. If the struct has a docstring, it will be provided to the model as well. It&#39;s used to enforce structured model outputs or provide more information.</li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments. <ul><li><code>tool_choice</code>: A string representing the tool choice to use for the API call. Usually, one of &quot;auto&quot;,&quot;any&quot;,&quot;exact&quot;.  Defaults to <code>&quot;exact&quot;</code>, which is a made-up value to enforce the OpenAI requirements if we want one exact function. Providers like Mistral, Together, etc. use <code>&quot;any&quot;</code> instead.</li></ul></li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><p>If <code>return_all=false</code> (default):</p><ul><li><code>msg</code>: An <code>DataMessage</code> object representing the extracted data, including the content, status, tokens, and elapsed time.  Use <code>msg.content</code> to access the extracted data.</li></ul><p>If <code>return_all=true</code>:</p><ul><li><code>conversation</code>: A vector of <code>AbstractMessage</code> objects representing the full conversation history, including the response from the AI model (<code>DataMessage</code>).</li></ul><p>See also: <code>function_call_signature</code>, <code>MaybeExtract</code>, <code>ItemsExtract</code>, <code>aigenerate</code></p><p><strong>Example</strong></p><p>Do you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (<code>return_type</code>):</p><pre><code class="nohighlight hljs">&quot;Person&#39;s age, height, and weight.&quot;
struct MyMeasurement
    age::Int # required
    height::Union{Int,Nothing} # optional
    weight::Union{Nothing,Float64} # optional
end
msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall.&quot;; return_type=MyMeasurement)
# PromptingTools.DataMessage(MyMeasurement)
msg.content
# MyMeasurement(30, 180, 80.0)</code></pre><p>The fields that allow <code>Nothing</code> are marked as optional in the schema:</p><pre><code class="nohighlight hljs">msg = aiextract(&quot;James is 30.&quot;; return_type=MyMeasurement)
# MyMeasurement(30, nothing, nothing)</code></pre><p>If there are multiple items you want to extract, define a wrapper struct to get a Vector of <code>MyMeasurement</code>:</p><pre><code class="nohighlight hljs">struct MyMeasurementWrapper
    measurements::Vector{MyMeasurement}
end

msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall. Then Jack is 19 but really tall - over 190!&quot;; return_type=ManyMeasurements)

msg.content.measurements
# 2-element Vector{MyMeasurement}:
#  MyMeasurement(30, 180, 80.0)
#  MyMeasurement(19, 190, nothing)</code></pre><p>Or you can use the convenience wrapper <code>ItemsExtract</code> to extract multiple measurements (zero, one or more):</p><pre><code class="language-julia hljs">using PromptingTools: ItemsExtract

return_type = ItemsExtract{MyMeasurement}
msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall. Then Jack is 19 but really tall - over 190!&quot;; return_type)

msg.content.items # see the extracted items</code></pre><p>Or if you want your extraction to fail gracefully when data isn&#39;t found, use <code>MaybeExtract{T}</code> wrapper  (this trick is inspired by the Instructor package!):</p><pre><code class="nohighlight hljs">using PromptingTools: MaybeExtract

type = MaybeExtract{MyMeasurement}
# Effectively the same as:
# struct MaybeExtract{T}
#     result::Union{T, Nothing} // The result of the extraction
#     error::Bool // true if a result is found, false otherwise
#     message::Union{Nothing, String} // Only present if no result is found, should be short and concise
# end

# If LLM extraction fails, it will return a Dict with `error` and `message` fields instead of the result!
msg = aiextract(&quot;Extract measurements from the text: I am giraffe&quot;, type)
msg.content
# MaybeExtract{MyMeasurement}(nothing, true, &quot;I&#39;m sorry, but I can only assist with human measurements.&quot;)</code></pre><p>That way, you can handle the error gracefully and get a reason why extraction failed (in <code>msg.content.message</code>).</p><p>Note that the error message refers to a giraffe not being a human,   because in our <code>MyMeasurement</code> docstring, we said that it&#39;s for people!</p><p>Some non-OpenAI providers require a different specification of the &quot;tool choice&quot; than OpenAI.  For example, to use Mistral models (&quot;mistrall&quot; for mistral large), do:</p><pre><code class="language-julia hljs">&quot;Some fruit&quot;
struct Fruit
    name::String
end
aiextract(&quot;I ate an apple&quot;,return_type=Fruit,api_kwargs=(;tool_choice=&quot;any&quot;),model=&quot;mistrall&quot;)
# Notice two differences: 1) struct MUST have a docstring, 2) tool_choice is set explicitly set to &quot;any&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L889-L1020">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractGoogleSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractGoogleSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate(prompt_schema::AbstractGoogleSchema, prompt::ALLOWED_PROMPT_TYPE;
    verbose::Bool = true,
    api_key::String = GOOGLE_API_KEY,
    model::String = &quot;gemini-pro&quot;, return_all::Bool = false, dry_run::Bool = false,
    http_kwargs::NamedTuple = (retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the Google Gemini API. Get the API key <a href="https://ai.google.dev/">here</a>.</p><p>Note: </p><ul><li>There is no &quot;cost&quot; reported as of February 2024, as all access seems to be free-of-charge. See the details <a href="https://ai.google.dev/pricing">here</a>.</li><li><code>tokens</code> in the returned AIMessage are actually characters, not tokens. We use a <em>conservative</em> estimate as they are not provided by the API yet.</li></ul><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>. Defaults to </li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><p>If <code>return_all=false</code> (default):</p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>If <code>return_all=true</code>:</p><ul><li><code>conversation</code>: A vector of <code>AbstractMessage</code> objects representing the conversation history, including the response from the AI model (<code>AIMessage</code>).</li></ul><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aiscan</code>, <code>aitemplates</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">result = aigenerate(&quot;Say Hi!&quot;; model=&quot;gemini-pro&quot;)
# AIMessage(&quot;Hi there! 👋 I&#39;m here to help you with any questions or tasks you may have. Just let me know what you need, and I&#39;ll do my best to assist you.&quot;)</code></pre><p><code>result</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(result) # AIMessage{SubString{String}}
propertynames(result) # (:content, :status, :tokens, :elapsed
result.content # &quot;Hi there! ...</code></pre><p>___ You can use string interpolation and alias &quot;gemini&quot;:</p><pre><code class="language-julia hljs">a = 1
msg=aigenerate(&quot;What is `$a+$a`?&quot;; model=&quot;gemini&quot;)
msg.content # &quot;1+1 is 2.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]
msg=aigenerate(conversation; model=&quot;gemini&quot;)
# AIMessage(&quot;Young Padawan, you have stumbled into a dangerous path.... &lt;continues&gt;&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_google.jl#L75-L147">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaManagedSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate(prompt_schema::AbstractOllamaManagedSchema, prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,
    api_key::String = &quot;&quot;, model::String = MODEL_CHAT,
    return_all::Bool = false, dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    http_kwargs::NamedTuple = NamedTuple(), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the OpenAI API.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code> not <code>AbstractManagedSchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: Provided for interface consistency. Not needed for locally hosted Ollama.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation::AbstractVector{&lt;:AbstractMessage}=[]</code>: Not allowed for this schema. Provided only for compatibility.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the Ollama API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema() # We need to explicit if we want Ollama, OpenAISchema is the default

msg = aigenerate(schema, &quot;Say hi!&quot;; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 69 in 0.9 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p><code>msg</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(msg) # AIMessage{SubString{String}}
propertynames(msg) # (:content, :status, :tokens, :elapsed
msg.content # &quot;Hello! How can I assist you today?&quot;</code></pre><p>Note: We need to be explicit about the schema we want to use. If we don&#39;t, it will default to <code>OpenAISchema</code> (=<code>PT.DEFAULT_SCHEMA</code>) ___ You can use string interpolation:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()
a = 1
msg=aigenerate(schema, &quot;What is `$a+$a`?&quot;; model=&quot;openhermes2.5-mistral&quot;)
msg.content # &quot;The result of `1+1` is `2`.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]

msg = aigenerate(schema, conversation; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 111 in 2.1 seconds
# AIMessage(&quot;Strong the attachment is, it leads to suffering it may. Focus on the force within you must, ...&lt;continues&gt;&quot;)</code></pre><p>Note: Managed Ollama currently supports at most 1 User Message and 1 System Message given the API limitations. If you want more, you need to use the <code>ChatMLSchema</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama_managed.jl#L124-L198">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate(prompt_schema::AbstractOllamaManagedSchema, prompt::ALLOWED_PROMPT_TYPE; verbose::Bool = true,
    api_key::String = &quot;&quot;, model::String = MODEL_CHAT,
    return_all::Bool = false, dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    http_kwargs::NamedTuple = NamedTuple(), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the OpenAI API.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code> not <code>AbstractManagedSchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: Provided for interface consistency. Not needed for locally hosted Ollama.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation::AbstractVector{&lt;:AbstractMessage}=[]</code>: Not allowed for this schema. Provided only for compatibility.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>api_kwargs::NamedTuple</code>: Additional keyword arguments for the Ollama API. Defaults to an empty <code>NamedTuple</code>.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema() # We need to explicit if we want Ollama, OpenAISchema is the default

msg = aigenerate(schema, &quot;Say hi!&quot;; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 69 in 0.9 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p><code>msg</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(msg) # AIMessage{SubString{String}}
propertynames(msg) # (:content, :status, :tokens, :elapsed
msg.content # &quot;Hello! How can I assist you today?&quot;</code></pre><p>Note: We need to be explicit about the schema we want to use. If we don&#39;t, it will default to <code>OpenAISchema</code> (=<code>PT.DEFAULT_SCHEMA</code>) ___ You can use string interpolation:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()
a = 1
msg=aigenerate(schema, &quot;What is `$a+$a`?&quot;; model=&quot;openhermes2.5-mistral&quot;)
msg.content # &quot;The result of `1+1` is `2`.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema()

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]

msg = aigenerate(schema, conversation; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 111 in 2.1 seconds
# AIMessage(&quot;Strong the attachment is, it leads to suffering it may. Focus on the force within you must, ...&lt;continues&gt;&quot;)</code></pre><p>Note: Managed Ollama currently supports at most 1 User Message and 1 System Message given the API limitations. If you want more, you need to use the <code>ChatMLSchema</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama.jl#L67-L141">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aigenerate-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aigenerate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aigenerate(prompt_schema::AbstractOpenAISchema, prompt::ALLOWED_PROMPT_TYPE;
    verbose::Bool = true,
    api_key::String = OPENAI_API_KEY,
    model::String = MODEL_CHAT, return_all::Bool = false, dry_run::Bool = false,
    http_kwargs::NamedTuple = (retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), api_kwargs::NamedTuple = NamedTuple(),
    kwargs...)</code></pre><p>Generate an AI response based on a given prompt using the OpenAI API.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments. Useful parameters include:<ul><li><code>temperature</code>: A float representing the temperature for sampling (ie, the amount of &quot;creativity&quot;). Often defaults to <code>0.7</code>.</li><li><code>logprobs</code>: A boolean indicating whether to return log probabilities for each token. Defaults to <code>false</code>.</li><li><code>n</code>: An integer representing the number of completions to generate at once (if supported).</li><li><code>stop</code>: A vector of strings representing the stop conditions for the conversation. Defaults to an empty vector.</li></ul></li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><p>If <code>return_all=false</code> (default):</p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>If <code>return_all=true</code>:</p><ul><li><code>conversation</code>: A vector of <code>AbstractMessage</code> objects representing the conversation history, including the response from the AI model (<code>AIMessage</code>).</li></ul><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aiscan</code>, <code>aitemplates</code></p><p><strong>Example</strong></p><p>Simple hello world to test the API:</p><pre><code class="language-julia hljs">result = aigenerate(&quot;Say Hi!&quot;)
# [ Info: Tokens: 29 @ Cost: $0.0 in 1.0 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p><code>result</code> is an <code>AIMessage</code> object. Access the generated string via <code>content</code> property:</p><pre><code class="language-julia hljs">typeof(result) # AIMessage{SubString{String}}
propertynames(result) # (:content, :status, :tokens, :elapsed
result.content # &quot;Hello! How can I assist you today?&quot;</code></pre><p>___ You can use string interpolation:</p><pre><code class="language-julia hljs">a = 1
msg=aigenerate(&quot;What is `$a+$a`?&quot;)
msg.content # &quot;The sum of `1+1` is `2`.&quot;</code></pre><p>___ You can provide the whole conversation or more intricate prompts as a <code>Vector{AbstractMessage}</code>:</p><pre><code class="language-julia hljs">const PT = PromptingTools

conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]
msg=aigenerate(conversation)
# AIMessage(&quot;Ah, strong feelings you have for your iPhone. A Jedi&#39;s path, this is not... &lt;continues&gt;&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L385-L458">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiscan-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOllamaSchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiscan([prompt_schema::AbstractOllamaSchema,] prompt::ALLOWED_PROMPT_TYPE; 
image_url::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,
image_path::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,
attach_to_latest::Bool = true,
verbose::Bool = true, api_key::String = OPENAI_API_KEY,
    model::String = MODEL_CHAT,
    return_all::Bool = false, dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    http_kwargs::NamedTuple = (;
        retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), 
    api_kwargs::NamedTuple = = (; max_tokens = 2500),
    kwargs...)</code></pre><p>Scans the provided image (<code>image_url</code> or <code>image_path</code>) with the goal provided in the <code>prompt</code>.</p><p>Can be used for many multi-modal tasks, such as: OCR (transcribe text in the image), image captioning, image classification, etc.</p><p>It&#39;s effectively a light wrapper around <code>aigenerate</code> call, which uses additional keyword arguments <code>image_url</code>, <code>image_path</code>, <code>image_detail</code> to be provided.   At least one image source (url or path) must be provided.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>image_url</code>: A string or vector of strings representing the URL(s) of the image(s) to scan.</li><li><code>image_path</code>: A string or vector of strings representing the path(s) of the image(s) to scan.</li><li><code>image_detail</code>: A string representing the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>. See <a href="https://platform.openai.com/docs/guides/vision">OpenAI Vision Guide</a> for more details.</li><li><code>attach_to_latest</code>: A boolean how to handle if a conversation with multiple <code>UserMessage</code> is provided. When <code>true</code>, the images are attached to the latest <code>UserMessage</code>.</li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><p>If <code>return_all=false</code> (default):</p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>If <code>return_all=true</code>:</p><ul><li><code>conversation</code>: A vector of <code>AbstractMessage</code> objects representing the full conversation history, including the response from the AI model (<code>AIMessage</code>).</li></ul><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aigenerate</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aitemplates</code></p><p><strong>Notes</strong></p><ul><li>All examples below use model &quot;gpt4v&quot;, which is an alias for model ID &quot;gpt-4-vision-preview&quot;</li><li><code>max_tokens</code> in the <code>api_kwargs</code> is preset to 2500, otherwise OpenAI enforces a default of only a few hundred tokens (~300). If your output is truncated, increase this value</li></ul><p><strong>Example</strong></p><p>Describe the provided image:</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=&quot;julia.png&quot;, model=&quot;bakllava&quot;)
# [ Info: Tokens: 1141 @ Cost: $0.0117 in 2.2 seconds
# AIMessage(&quot;The image shows a logo consisting of the word &quot;julia&quot; written in lowercase&quot;)</code></pre><p>You can provide multiple images at once as a vector and ask for &quot;low&quot; level of detail (cheaper):</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=[&quot;julia.png&quot;,&quot;python.png&quot;] model=&quot;bakllava&quot;)</code></pre><p>You can use this function as a nice and quick OCR (transcribe text in the image) with a template <code>:OCRTask</code>.  Let&#39;s transcribe some SQL code from a screenshot (no more re-typing!):</p><pre><code class="language-julia hljs">using Downloads
# Screenshot of some SQL code -- we cannot use image_url directly, so we need to download it first
image_url = &quot;https://www.sqlservercentral.com/wp-content/uploads/legacy/8755f69180b7ac7ee76a69ae68ec36872a116ad4/24622.png&quot;
image_path = Downloads.download(image_url)
msg = aiscan(:OCRTask; image_path, model=&quot;bakllava&quot;, task=&quot;Transcribe the SQL code in the image.&quot;, api_kwargs=(; max_tokens=2500))

# AIMessage(&quot;```sql
# update Orders &lt;continue&gt;

# You can add syntax highlighting of the outputs via Markdown
using Markdown
msg.content |&gt; Markdown.parse</code></pre><p>Local models cannot handle image URLs directly (<code>image_url</code>), so you need to download the image first and provide it as <code>image_path</code>:</p><pre><code class="language-julia hljs">using Downloads
image_path = Downloads.download(image_url)</code></pre><p>Notice that we set <code>max_tokens = 2500</code>. If your outputs seem truncated, it might be because the default maximum tokens on the server is set too low!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama.jl#L192-L288">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.aiscan-Tuple{PromptingTools.AbstractOpenAISchema, Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.aiscan</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">aiscan([prompt_schema::AbstractOpenAISchema,] prompt::ALLOWED_PROMPT_TYPE; 
image_url::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,
image_path::Union{Nothing, AbstractString, Vector{&lt;:AbstractString}} = nothing,
image_detail::AbstractString = &quot;auto&quot;,
attach_to_latest::Bool = true,
verbose::Bool = true, api_key::String = OPENAI_API_KEY,
    model::String = MODEL_CHAT,
    return_all::Bool = false, dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    http_kwargs::NamedTuple = (;
        retry_non_idempotent = true,
        retries = 5,
        readtimeout = 120), 
    api_kwargs::NamedTuple = = (; max_tokens = 2500),
    kwargs...)</code></pre><p>Scans the provided image (<code>image_url</code> or <code>image_path</code>) with the goal provided in the <code>prompt</code>.</p><p>Can be used for many multi-modal tasks, such as: OCR (transcribe text in the image), image captioning, image classification, etc.</p><p>It&#39;s effectively a light wrapper around <code>aigenerate</code> call, which uses additional keyword arguments <code>image_url</code>, <code>image_path</code>, <code>image_detail</code> to be provided.   At least one image source (url or path) must be provided.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_schema</code>: An optional object to specify which prompt template should be applied (Default to <code>PROMPT_SCHEMA = OpenAISchema</code>)</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code> or an <code>AITemplate</code></li><li><code>image_url</code>: A string or vector of strings representing the URL(s) of the image(s) to scan.</li><li><code>image_path</code>: A string or vector of strings representing the path(s) of the image(s) to scan.</li><li><code>image_detail</code>: A string representing the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>. See <a href="https://platform.openai.com/docs/guides/vision">OpenAI Vision Guide</a> for more details.</li><li><code>attach_to_latest</code>: A boolean how to handle if a conversation with multiple <code>UserMessage</code> is provided. When <code>true</code>, the images are attached to the latest <code>UserMessage</code>.</li><li><code>verbose</code>: A boolean indicating whether to print additional information.</li><li><code>api_key</code>: A string representing the API key for accessing the OpenAI API.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>return_all::Bool=false</code>: If <code>true</code>, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If <code>true</code>, skips sending the messages to the model (for debugging, often used with <code>return_all=true</code>).</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>http_kwargs</code>: A named tuple of HTTP keyword arguments.</li><li><code>api_kwargs</code>: A named tuple of API keyword arguments.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul><p><strong>Returns</strong></p><p>If <code>return_all=false</code> (default):</p><ul><li><code>msg</code>: An <code>AIMessage</code> object representing the generated AI message, including the content, status, tokens, and elapsed time.</li></ul><p>Use <code>msg.content</code> to access the extracted string.</p><p>If <code>return_all=true</code>:</p><ul><li><code>conversation</code>: A vector of <code>AbstractMessage</code> objects representing the full conversation history, including the response from the AI model (<code>AIMessage</code>).</li></ul><p>See also: <code>ai_str</code>, <code>aai_str</code>, <code>aigenerate</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aitemplates</code></p><p><strong>Notes</strong></p><ul><li>All examples below use model &quot;gpt4v&quot;, which is an alias for model ID &quot;gpt-4-vision-preview&quot;</li><li><code>max_tokens</code> in the <code>api_kwargs</code> is preset to 2500, otherwise OpenAI enforces a default of only a few hundred tokens (~300). If your output is truncated, increase this value</li></ul><p><strong>Example</strong></p><p>Describe the provided image:</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=&quot;julia.png&quot;, model=&quot;gpt4v&quot;)
# [ Info: Tokens: 1141 @ Cost: $0.0117 in 2.2 seconds
# AIMessage(&quot;The image shows a logo consisting of the word &quot;julia&quot; written in lowercase&quot;)</code></pre><p>You can provide multiple images at once as a vector and ask for &quot;low&quot; level of detail (cheaper):</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=[&quot;julia.png&quot;,&quot;python.png&quot;], image_detail=&quot;low&quot;, model=&quot;gpt4v&quot;)</code></pre><p>You can use this function as a nice and quick OCR (transcribe text in the image) with a template <code>:OCRTask</code>.  Let&#39;s transcribe some SQL code from a screenshot (no more re-typing!):</p><pre><code class="language-julia hljs"># Screenshot of some SQL code
image_url = &quot;https://www.sqlservercentral.com/wp-content/uploads/legacy/8755f69180b7ac7ee76a69ae68ec36872a116ad4/24622.png&quot;
msg = aiscan(:OCRTask; image_url, model=&quot;gpt4v&quot;, task=&quot;Transcribe the SQL code in the image.&quot;, api_kwargs=(; max_tokens=2500))

# [ Info: Tokens: 362 @ Cost: $0.0045 in 2.5 seconds
# AIMessage(&quot;```sql
# update Orders &lt;continue&gt;

# You can add syntax highlighting of the outputs via Markdown
using Markdown
msg.content |&gt; Markdown.parse</code></pre><p>Notice that we enforce <code>max_tokens = 2500</code>. That&#39;s because OpenAI seems to default to ~300 tokens, which provides incomplete outputs. Hence, we set this value to 2500 as a default. If you still get truncated outputs, increase this value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L1097-L1187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates" href="#PromptingTools.aitemplates"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aitemplates</code></pre><p>Find easily the most suitable templates for your use case.</p><p>You can search by:</p><ul><li><code>query::Symbol</code> which looks look only for partial matches in the template <code>name</code></li><li><code>query::AbstractString</code> which looks for partial matches in the template <code>name</code> or <code>description</code></li><li><code>query::Regex</code> which looks for matches in the template <code>name</code>, <code>description</code> or any of the message previews</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>limit::Int</code> limits the number of returned templates (Defaults to 10)</li></ul><p><strong>Examples</strong></p><p>Find available templates with <code>aitemplates</code>:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;JuliaExpertAsk&quot;)
# Will surface one specific template
# 1-element Vector{AITemplateMetadata}:
# PromptingTools.AITemplateMetadata
#   name: Symbol JuliaExpertAsk
#   description: String &quot;For asking questions about Julia language. Placeholders: `ask`&quot;
#   version: String &quot;1&quot;
#   wordcount: Int64 237
#   variables: Array{Symbol}((1,))
#   system_preview: String &quot;You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun&quot;
#   user_preview: String &quot;# Question

{{ask}}&quot;
#   source: String &quot;&quot;</code></pre><p>The above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).</p><p>Search for all Julia-related templates:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;Julia&quot;)
# 2-element Vector{AITemplateMetadata}... -&gt; more to come later!</code></pre><p>If you are on VSCode, you can leverage nice tabular display with <code>vscodedisplay</code>:</p><pre><code class="language-julia hljs">using DataFrames
tmps = aitemplates(&quot;Julia&quot;) |&gt; DataFrame |&gt; vscodedisplay</code></pre><p>I have my selected template, how do I use it? Just use the &quot;name&quot; in <code>aigenerate</code> or <code>aiclassify</code>   like you see in the first example!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L239-L287">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{AbstractString}" href="#PromptingTools.aitemplates-Tuple{AbstractString}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates whose <code>name</code> or <code>description</code> fields partially match the <code>query_key::String</code> in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L298">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{Regex}" href="#PromptingTools.aitemplates-Tuple{Regex}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates where provided <code>query_key::Regex</code> matches either of <code>name</code>, <code>description</code> or previews or User or System messages in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L309">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.aitemplates-Tuple{Symbol}" href="#PromptingTools.aitemplates-Tuple{Symbol}"><code>PromptingTools.aitemplates</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Find the top-<code>limit</code> templates whose <code>name::Symbol</code> partially matches the <code>query_name::Symbol</code> in <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L288">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.auth_header-Tuple{Union{Nothing, AbstractString}}" href="#PromptingTools.auth_header-Tuple{Union{Nothing, AbstractString}}"><code>PromptingTools.auth_header</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">auth_header(api_key::Union{Nothing, AbstractString};
    extra_headers::AbstractVector{Pair{String, String}} = Vector{Pair{String, String}}[],
    kwargs...)</code></pre><p>Creates the authentication headers for any API request. Assumes that the communication is done in JSON format.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L460-L466">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.call_cost-Tuple{Int64, Int64, String}" href="#PromptingTools.call_cost-Tuple{Int64, Int64, String}"><code>PromptingTools.call_cost</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">call_cost(prompt_tokens::Int, completion_tokens::Int, model::String;
    cost_of_token_prompt::Number = get(MODEL_REGISTRY,
        model,
        (; cost_of_token_prompt = 0.0)).cost_of_token_prompt,
    cost_of_token_generation::Number = get(MODEL_REGISTRY, model,
        (; cost_of_token_generation = 0.0)).cost_of_token_generation)

call_cost(msg, model::String)</code></pre><p>Calculate the cost of a call based on the number of tokens in the message and the cost per token.</p><p><strong>Arguments</strong></p><ul><li><code>prompt_tokens::Int</code>: The number of tokens used in the prompt.</li><li><code>completion_tokens::Int</code>: The number of tokens used in the completion.</li><li><code>model::String</code>: The name of the model to use for determining token costs. If the model is not found in <code>MODEL_REGISTRY</code>, default costs are used.</li><li><code>cost_of_token_prompt::Number</code>: The cost per prompt token. Defaults to the cost in <code>MODEL_REGISTRY</code> for the given model, or 0.0 if the model is not found.</li><li><code>cost_of_token_generation::Number</code>: The cost per generation token. Defaults to the cost in <code>MODEL_REGISTRY</code> for the given model, or 0.0 if the model is not found.</li></ul><p><strong>Returns</strong></p><ul><li><code>Number</code>: The total cost of the call.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Assuming MODEL_REGISTRY is set up with appropriate costs
MODEL_REGISTRY = Dict(
    &quot;model1&quot; =&gt; (cost_of_token_prompt = 0.05, cost_of_token_generation = 0.10),
    &quot;model2&quot; =&gt; (cost_of_token_prompt = 0.07, cost_of_token_generation = 0.02)
)

cost1 = call_cost(10, 20, &quot;model1&quot;)

# from message
msg1 = AIMessage(;tokens=[10, 20])  # 10 prompt tokens, 20 generation tokens
cost1 = call_cost(msg1, &quot;model1&quot;)
# cost1 = 10 * 0.05 + 20 * 0.10 = 2.5

# Using custom token costs
cost2 = call_cost(10, 20, &quot;model3&quot;; cost_of_token_prompt = 0.08, cost_of_token_generation = 0.12)
# cost2 = 10 * 0.08 + 20 * 0.12 = 3.2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L246-L290">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.create_template-Tuple{AbstractString, AbstractString}" href="#PromptingTools.create_template-Tuple{AbstractString, AbstractString}"><code>PromptingTools.create_template</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">create_template(; user::AbstractString, system::AbstractString=&quot;Act as a helpful AI assistant.&quot;)

create_template(system::AbstractString, user::AbstractString)</code></pre><p>Creates a simple template with a user and system message. Convenience function to prevent writing <code>[PT.UserMessage(...), ...]</code></p><p><strong>Arguments</strong></p><ul><li><code>system::AbstractString</code>: The system message. Usually defines the personality, style, instructions, output format, etc.</li><li><code>user::AbstractString</code>: The user message. Usually defines the input, query, request, etc.</li></ul><p>Use double handlebar placeholders (eg, <code>{{name}}</code>) to define variables that can be replaced by the <code>kwargs</code> during the AI call (see example).</p><p>Returns a vector of <code>SystemMessage</code> and UserMessage objects.</p><p><strong>Examples</strong></p><p>Let&#39;s generate a quick template for a simple conversation (only one placeholder: name)</p><pre><code class="language-julia hljs"># first system message, then user message (or use kwargs)
tpl=PT.create_template(&quot;You must speak like a pirate&quot;, &quot;Say hi to {{name}}&quot;)

## 2-element Vector{PromptingTools.AbstractChatMessage}:
## PromptingTools.SystemMessage(&quot;You must speak like a pirate&quot;)
##  PromptingTools.UserMessage(&quot;Say hi to {{name}}&quot;)</code></pre><p>You can immediately use this template in <code>ai*</code> functions:</p><pre><code class="language-julia hljs">aigenerate(tpl; name=&quot;Jack Sparrow&quot;)
# Output: AIMessage(&quot;Arr, me hearty! Best be sending me regards to Captain Jack Sparrow on the salty seas! May his compass always point true to the nearest treasure trove. Yarrr!&quot;)</code></pre><p>If you want to save it in your project folder:</p><pre><code class="language-julia hljs">PT.save_template(&quot;templates/GreatingPirate.json&quot;, tpl; version=&quot;1.0&quot;) # optionally, add description</code></pre><p>It will be saved and accessed under its basename, ie, <code>GreatingPirate</code>.</p><p>Now you can load it like all the other templates (provide the template directory):</p><pre><code class="nohighlight hljs">PT.load_templates!(&quot;templates&quot;) # it will remember the folder after the first run
# Note: If you save it again, overwrite it, etc., you need to explicitly reload all templates again!</code></pre><p>You can verify that your template is loaded with a quick search for &quot;pirate&quot;:</p><pre><code class="language-julia hljs">aitemplates(&quot;pirate&quot;)

## 1-element Vector{AITemplateMetadata}:
## PromptingTools.AITemplateMetadata
##   name: Symbol GreatingPirate
##   description: String &quot;&quot;
##   version: String &quot;1.0&quot;
##   wordcount: Int64 46
##   variables: Array{Symbol}((1,))
##   system_preview: String &quot;You must speak like a pirate&quot;
##   user_preview: String &quot;Say hi to {{name}}&quot;
##   source: String &quot;&quot;</code></pre><p>Now you can use it like any other template (notice it&#39;s a symbol, so <code>:GreatingPirate</code>): ```julia aigenerate(:GreatingPirate; name=&quot;Jack Sparrow&quot;)</p><p><strong>Output: AIMessage(&quot;Arr, me hearty! Best be sending me regards to Captain Jack Sparrow on the salty seas! May his compass always point true to the nearest treasure trove. Yarrr!&quot;)</strong></p><p>````</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L354-L420">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.decode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}, AIMessage}" href="#PromptingTools.decode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}, AIMessage}"><code>PromptingTools.decode_choices</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">decode_choices(schema::OpenAISchema,
    choices::AbstractVector{&lt;:AbstractString},
    msg::AIMessage; kwargs...)</code></pre><p>Decodes the underlying AIMessage against the original choices to lookup what the category name was.</p><p>If it fails, it will return <code>msg.content == nothing</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L728-L736">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.detect_base_main_overrides-Tuple{AbstractString}" href="#PromptingTools.detect_base_main_overrides-Tuple{AbstractString}"><code>PromptingTools.detect_base_main_overrides</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">detect_base_main_overrides(code_block::AbstractString)</code></pre><p>Detects if a given code block overrides any Base or Main methods. </p><p>Returns a tuple of a boolean and a vector of the overriden methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L425-L431">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.encode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}}" href="#PromptingTools.encode_choices-Tuple{PromptingTools.OpenAISchema, AbstractVector{&lt;:AbstractString}}"><code>PromptingTools.encode_choices</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">encode_choices(schema::OpenAISchema, choices::AbstractVector{&lt;:AbstractString}; kwargs...)

encode_choices(schema::OpenAISchema, choices::AbstractVector{T};
kwargs...) where {T &lt;: Tuple{&lt;:AbstractString, &lt;:AbstractString}}</code></pre><p>Encode the choices into an enumerated list that can be interpolated into the prompt and creates the corresponding logit biases (to choose only from the selected tokens).</p><p>Optionally, can be a vector tuples, where the first element is the choice and the second is the description.</p><p><strong>Arguments</strong></p><ul><li><code>schema::OpenAISchema</code>: The OpenAISchema object.</li><li><code>choices::AbstractVector{&lt;:Union{AbstractString,Tuple{&lt;:AbstractString, &lt;:AbstractString}}}</code>: The choices to be encoded, represented as a vector of the choices directly, or tuples where each tuple contains a choice and its description.</li><li><code>kwargs...</code>: Additional keyword arguments.</li></ul><p><strong>Returns</strong></p><ul><li><code>choices_prompt::AbstractString</code>: The encoded choices as a single string, separated by newlines.</li><li><code>logit_bias::Dict</code>: The logit bias dictionary, where the keys are the token IDs and the values are the bias values.</li><li><code>decode_ids::AbstractVector{&lt;:AbstractString}</code>: The decoded IDs of the choices.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">choices_prompt, logit_bias, _ = PT.encode_choices(PT.OpenAISchema(), [&quot;true&quot;, &quot;false&quot;])
choices_prompt # Output: &quot;true for &quot;true&quot;
false for &quot;false&quot;
logit_bias # Output: Dict(837 =&gt; 100, 905 =&gt; 100)

choices_prompt, logit_bias, _ = PT.encode_choices(PT.OpenAISchema(), [&quot;animal&quot;, &quot;plant&quot;])
choices_prompt # Output: &quot;1. &quot;animal&quot;
2. &quot;plant&quot;&quot;
logit_bias # Output: Dict(16 =&gt; 100, 17 =&gt; 100)</code></pre><p>Or choices with descriptions:</p><pre><code class="language-julia hljs">choices_prompt, logit_bias, _ = PT.encode_choices(PT.OpenAISchema(), [(&quot;A&quot;, &quot;any animal or creature&quot;), (&quot;P&quot;, &quot;for any plant or tree&quot;), (&quot;O&quot;, &quot;for everything else&quot;)])
choices_prompt # Output: &quot;1. &quot;A&quot; for any animal or creature
2. &quot;P&quot; for any plant or tree
3. &quot;O&quot; for everything else&quot;
logit_bias # Output: Dict(16 =&gt; 100, 17 =&gt; 100, 18 =&gt; 100)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L627-L668">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.eval!-Tuple{PromptingTools.AbstractCodeBlock}" href="#PromptingTools.eval!-Tuple{PromptingTools.AbstractCodeBlock}"><code>PromptingTools.eval!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">eval!(cb::AbstractCodeBlock;
    safe_eval::Bool = true,
    capture_stdout::Bool = true,
    prefix::AbstractString = &quot;&quot;,
    suffix::AbstractString = &quot;&quot;)</code></pre><p>Evaluates a code block <code>cb</code> in-place. It runs automatically when AICode is instantiated with a String.</p><p>Check the outcome of evaluation with <code>Base.isvalid(cb)</code>. If <code>==true</code>, provide code block has executed successfully.</p><p>Steps:</p><ul><li>If <code>cb::AICode</code> has not been evaluated, <code>cb.success = nothing</code>.  After the evaluation it will be either <code>true</code> or <code>false</code> depending on the outcome</li><li>Parse the text in <code>cb.code</code></li><li>Evaluate the parsed expression</li><li>Capture outputs of the evaluated in <code>cb.output</code></li><li>[OPTIONAL] Capture any stdout outputs (eg, test failures) in <code>cb.stdout</code></li><li>If any error exception is raised, it is saved in <code>cb.error</code></li><li>Finally, if all steps were successful, success is set to <code>cb.success = true</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>safe_eval::Bool</code>: If <code>true</code>, we first check for any Pkg operations (eg, installing new packages) and missing imports,  then the code will be evaluated inside a bespoke scratch module (not to change any user variables)</li><li><code>capture_stdout::Bool</code>: If <code>true</code>, we capture any stdout outputs (eg, test failures) in <code>cb.stdout</code></li><li><code>prefix::AbstractString</code>: A string to be prepended to the code block before parsing and evaluation. Useful to add some additional code definition or necessary imports. Defaults to an empty string.</li><li><code>suffix::AbstractString</code>: A string to be appended to the code block before parsing and evaluation.  Useful to check that tests pass or that an example executes. Defaults to an empty string.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_eval.jl#L213-L242">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.extract_code_blocks-Tuple{T} where T&lt;:AbstractString" href="#PromptingTools.extract_code_blocks-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.extract_code_blocks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">extract_code_blocks(markdown_content::String) -&gt; Vector{String}</code></pre><p>Extract Julia code blocks from a markdown string.</p><p>This function searches through the provided markdown content, identifies blocks of code specifically marked as Julia code  (using the <code>julia ...</code> code fence patterns), and extracts the code within these blocks.  The extracted code blocks are returned as a vector of strings, with each string representing one block of Julia code. </p><p>Note: Only the content within the code fences is extracted, and the code fences themselves are not included in the output.</p><p>See also: <code>extract_code_blocks_fallback</code></p><p><strong>Arguments</strong></p><ul><li><code>markdown_content::String</code>: A string containing the markdown content from which Julia code blocks are to be extracted.</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{String}</code>: A vector containing strings of extracted Julia code blocks. If no Julia code blocks are found, an empty vector is returned.</li></ul><p><strong>Examples</strong></p><p>Example with a single Julia code block</p><pre><code class="language-julia hljs">markdown_single = &quot;&quot;&quot;</code></pre><p>julia println(&quot;Hello, World!&quot;)</p><pre><code class="nohighlight hljs">&quot;&quot;&quot;
extract_code_blocks(markdown_single)
# Output: [&quot;Hello, World!&quot;]</code></pre><pre><code class="language-julia hljs"># Example with multiple Julia code blocks
markdown_multiple = &quot;&quot;&quot;</code></pre><p>julia x = 5</p><pre><code class="nohighlight hljs">Some text in between</code></pre><p>julia y = x + 2</p><pre><code class="nohighlight hljs">&quot;&quot;&quot;
extract_code_blocks(markdown_multiple)
# Output: [&quot;x = 5&quot;, &quot;y = x + 2&quot;]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L173-L219">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.extract_code_blocks_fallback-Union{Tuple{T}, Tuple{T, AbstractString}} where T&lt;:AbstractString" href="#PromptingTools.extract_code_blocks_fallback-Union{Tuple{T}, Tuple{T, AbstractString}} where T&lt;:AbstractString"><code>PromptingTools.extract_code_blocks_fallback</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">extract_code_blocks_fallback(markdown_content::String, delim::AbstractString=&quot;\n```\n&quot;)</code></pre><p>Extract Julia code blocks from a markdown string using a fallback method (splitting by arbitrary <code>delim</code>-iters). Much more simplistic than <code>extract_code_blocks</code> and does not support nested code blocks.</p><p>It is often used as a fallback for smaller LLMs that forget to code fence <code>julia ...</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">code = &quot;&quot;&quot;</code></pre><p>println(&quot;hello&quot;)</p><pre><code class="nohighlight hljs">
Some text
</code></pre><p>println(&quot;world&quot;)</p><pre><code class="nohighlight hljs">&quot;&quot;&quot;

# We extract text between triple backticks and check each blob if it looks like a valid Julia code
code_parsed = extract_code_blocks_fallback(code) |&gt; x -&gt; filter(is_julia_code, x) |&gt; x -&gt; join(x, &quot;
&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L274-L301">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.extract_function_name-Tuple{AbstractString}" href="#PromptingTools.extract_function_name-Tuple{AbstractString}"><code>PromptingTools.extract_function_name</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">extract_function_name(code_block::String) -&gt; Union{String, Nothing}</code></pre><p>Extract the name of a function from a given Julia code block. The function searches for two patterns:</p><ul><li>The explicit function declaration pattern: <code>function name(...) ... end</code></li><li>The concise function declaration pattern: <code>name(...) = ...</code></li></ul><p>If a function name is found, it is returned as a string. If no function name is found, the function returns <code>nothing</code>.</p><p>To capture all function names in the block, use <code>extract_function_names</code>.</p><p><strong>Arguments</strong></p><ul><li><code>code_block::String</code>: A string containing Julia code.</li></ul><p><strong>Returns</strong></p><ul><li><code>Union{String, Nothing}</code>: The extracted function name or <code>nothing</code> if no name is found.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">code = &quot;&quot;&quot;
function myFunction(arg1, arg2)
    # Function body
end
&quot;&quot;&quot;
extract_function_name(code)
# Output: &quot;myFunction&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L344-L371">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.extract_function_names-Tuple{AbstractString}" href="#PromptingTools.extract_function_names-Tuple{AbstractString}"><code>PromptingTools.extract_function_names</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">extract_function_names(code_block::AbstractString)</code></pre><p>Extract one or more names of functions defined in a given Julia code block. The function searches for two patterns:     - The explicit function declaration pattern: <code>function name(...) ... end</code>     - The concise function declaration pattern: <code>name(...) = ...</code></p><p>It always returns a vector of strings, even if only one function name is found (it will be empty).</p><p>For only one function name match, use <code>extract_function_name</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L394-L404">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.extract_julia_imports-Tuple{AbstractString}" href="#PromptingTools.extract_julia_imports-Tuple{AbstractString}"><code>PromptingTools.extract_julia_imports</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">extract_julia_imports(input::AbstractString; base_or_main::Bool = false)</code></pre><p>Detects any <code>using</code> or <code>import</code> statements in a given string and returns the package names as a vector of symbols. </p><p><code>base_or_main</code> is a boolean that determines whether to isolate only <code>Base</code> and <code>Main</code> OR whether to exclude them in the returned vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L23-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.finalize_outputs-Tuple{Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}, Any, Union{Nothing, PromptingTools.AbstractMessage, AbstractVector{&lt;:PromptingTools.AbstractMessage}}}" href="#PromptingTools.finalize_outputs-Tuple{Union{AbstractString, PromptingTools.AbstractMessage, Vector{&lt;:PromptingTools.AbstractMessage}}, Any, Union{Nothing, PromptingTools.AbstractMessage, AbstractVector{&lt;:PromptingTools.AbstractMessage}}}"><code>PromptingTools.finalize_outputs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">finalize_outputs(prompt::ALLOWED_PROMPT_TYPE, conv_rendered::Any,
    msg::Union{Nothing, AbstractMessage, AbstractVector{&lt;:AbstractMessage}};
    return_all::Bool = false,
    dry_run::Bool = false,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    kwargs...)</code></pre><p>Finalizes the outputs of the ai* functions by either returning the conversation history or the last message.</p><p><strong>Keyword arguments</strong></p><ul><li><code>return_all::Bool=false</code>: If true, returns the entire conversation history, otherwise returns only the last message (the <code>AIMessage</code>).</li><li><code>dry_run::Bool=false</code>: If true, does not send the messages to the model, but only renders the prompt with the given schema and replacement variables. Useful for debugging when you want to check the specific schema rendering. </li><li><code>conversation::AbstractVector{&lt;:AbstractMessage}=[]</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li><li><code>kwargs...</code>: Variables to replace in the prompt template.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_shared.jl#L66-L82">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.find_subsequence_positions-Tuple{Any, Any}" href="#PromptingTools.find_subsequence_positions-Tuple{Any, Any}"><code>PromptingTools.find_subsequence_positions</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">find_subsequence_positions(subseq, seq) -&gt; Vector{Int}</code></pre><p>Find all positions of a subsequence <code>subseq</code> within a larger sequence <code>seq</code>. Used to lookup positions of code blocks in markdown.</p><p>This function scans the sequence <code>seq</code> and identifies all starting positions where the subsequence <code>subseq</code> is found. Both <code>subseq</code> and <code>seq</code> should be vectors of integers, typically obtained using <code>codeunits</code> on strings.</p><p><strong>Arguments</strong></p><ul><li><code>subseq</code>: A vector of integers representing the subsequence to search for.</li><li><code>seq</code>: A vector of integers representing the larger sequence in which to search.</li></ul><p><strong>Returns</strong></p><ul><li><code>Vector{Int}</code>: A vector of starting positions (1-based indices) where the subsequence is found in the sequence.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">find_subsequence_positions(codeunits(&quot;ab&quot;), codeunits(&quot;cababcab&quot;)) # Returns [2, 5]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L132-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.function_call_signature-Tuple{Type}" href="#PromptingTools.function_call_signature-Tuple{Type}"><code>PromptingTools.function_call_signature</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">function_call_signature(datastructtype::Struct; max_description_length::Int = 100)</code></pre><p>Extract the argument names, types and docstrings from a struct to create the function call signature in JSON schema.</p><p>You must provide a Struct type (not an instance of it) with some fields.</p><p>Note: Fairly experimental, but works for combination of structs, arrays, strings and singletons.</p><p><strong>Tips</strong></p><ul><li>You can improve the quality of the extraction by writing a helpful docstring for your struct (or any nested struct). It will be provided as a description. </li></ul><p>You can even include comments/descriptions about the individual fields.</p><ul><li>All fields are assumed to be required, unless you allow null values (eg, <code>::Union{Nothing, Int}</code>). Fields with <code>Nothing</code> will be treated as optional.</li><li>Missing values are ignored (eg, <code>::Union{Missing, Int}</code> will be treated as Int). It&#39;s for broader compatibility and we cannot deserialize it as easily as <code>Nothing</code>.</li></ul><p><strong>Example</strong></p><p>Do you want to extract some specific measurements from a text like age, weight and height? You need to define the information you need as a struct (<code>return_type</code>):</p><pre><code class="nohighlight hljs">struct MyMeasurement
    age::Int
    height::Union{Int,Nothing}
    weight::Union{Nothing,Float64}
end
signature = function_call_signature(MyMeasurement)
#
# Dict{String, Any} with 3 entries:
#   &quot;name&quot;        =&gt; &quot;MyMeasurement_extractor&quot;
#   &quot;parameters&quot;  =&gt; Dict{String, Any}(&quot;properties&quot;=&gt;Dict{String, Any}(&quot;height&quot;=&gt;Dict{String, Any}(&quot;type&quot;=&gt;&quot;integer&quot;), &quot;weight&quot;=&gt;Dic…
#   &quot;description&quot; =&gt; &quot;Represents person&#39;s age, height, and weight
&quot;</code></pre><p>You can see that only the field <code>age</code> does not allow null values, hence, it&#39;s &quot;required&quot;. While <code>height</code> and <code>weight</code> are optional.</p><pre><code class="nohighlight hljs">signature[&quot;parameters&quot;][&quot;required&quot;]
# [&quot;age&quot;]</code></pre><p>If there are multiple items you want to extract, define a wrapper struct to get a Vector of <code>MyMeasurement</code>:</p><pre><code class="nohighlight hljs">struct MyMeasurementWrapper
    measurements::Vector{MyMeasurement}
end

Or if you want your extraction to fail gracefully when data isn&#39;t found, use `MaybeExtract{T}` wrapper (inspired by Instructor package!):</code></pre><p>using PromptingTools: MaybeExtract</p><p>type = MaybeExtract{MyMeasurement}</p><p><strong>Effectively the same as:</strong></p><p><strong>struct MaybeExtract{T}</strong></p><p><strong>result::Union{T, Nothing}</strong></p><p><strong>error::Bool // true if a result is found, false otherwise</strong></p><p><strong>message::Union{Nothing, String} // Only present if no result is found, should be short and concise</strong></p><p><strong>end</strong></p><p><strong>If LLM extraction fails, it will return a Dict with <code>error</code> and <code>message</code> fields instead of the result!</strong></p><p>msg = aiextract(&quot;Extract measurements from the text: I am giraffe&quot;, type)</p><p><strong></strong></p><p><strong>Dict{Symbol, Any} with 2 entries:</strong></p><p><strong>:message =&gt; &quot;Sorry, this feature is only available for humans.&quot;</strong></p><p><strong>:error   =&gt; true</strong></p><p>``` That way, you can handle the error gracefully and get a reason why extraction failed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/extraction.jl#L84-L152">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.get_preferences-Tuple{String}" href="#PromptingTools.get_preferences-Tuple{String}"><code>PromptingTools.get_preferences</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">get_preferences(key::String)</code></pre><p>Get preferences for PromptingTools. See <code>?PREFERENCES</code> for more information.</p><p>See also: <code>set_preferences!</code></p><p><strong>Example</strong></p><pre><code class="language-julia hljs">PromptingTools.get_preferences(&quot;MODEL_CHAT&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L94-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ggi_generate_content" href="#PromptingTools.ggi_generate_content"><code>PromptingTools.ggi_generate_content</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Stub - to be extended in extension: GoogleGenAIPromptingToolsExt. <code>ggi</code> stands for GoogleGenAI</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_google.jl#L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.has_julia_prompt-Tuple{T} where T&lt;:AbstractString" href="#PromptingTools.has_julia_prompt-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.has_julia_prompt</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Checks if a given string has a Julia prompt (<code>julia&gt;</code>) at the beginning of a line.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L92">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.length_longest_common_subsequence-Tuple{Any, Any}" href="#PromptingTools.length_longest_common_subsequence-Tuple{Any, Any}"><code>PromptingTools.length_longest_common_subsequence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">length_longest_common_subsequence(itr1, itr2)</code></pre><p>Compute the length of the longest common subsequence between two sequences (ie, the higher the number, the better the match).</p><p>Source: https://cn.julialang.org/LeetCode.jl/dev/democards/problems/problems/1143.longest-common-subsequence/</p><p><strong>Arguments</strong></p><ul><li><code>itr1</code>: The first sequence, eg, a String.</li><li><code>itr2</code>: The second sequence, eg, a String.</li></ul><p><strong>Returns</strong></p><p>The length of the longest common subsequence.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">text1 = &quot;abc-abc----&quot;
text2 = &quot;___ab_c__abc&quot;
longest_common_subsequence(text1, text2)
# Output: 6 (-&gt; &quot;abcabc&quot;)</code></pre><p>It can be used to fuzzy match strings and find the similarity between them (Tip: normalize the match)</p><pre><code class="language-julia hljs">commands = [&quot;product recommendation&quot;, &quot;emotions&quot;, &quot;specific product advice&quot;, &quot;checkout advice&quot;]
query = &quot;Which product can you recommend for me?&quot;
let pos = argmax(length_longest_common_subsequence.(Ref(query), commands))
    dist = length_longest_common_subsequence(query, commands[pos])
    norm = dist / min(length(query), length(commands[pos]))
    @info &quot;The closest command to the query: &quot;$(query)&quot; is: &quot;$(commands[pos])&quot; (distance: $(dist), normalized: $(norm))&quot;
end</code></pre><p>You can also use it to find the closest context for some AI generated summary/story:</p><pre><code class="language-julia hljs">context = [&quot;The enigmatic stranger vanished as swiftly as a wisp of smoke, leaving behind a trail of unanswered questions.&quot;,
    &quot;Beneath the shimmering moonlight, the ocean whispered secrets only the stars could hear.&quot;,
    &quot;The ancient tree stood as a silent guardian, its gnarled branches reaching for the heavens.&quot;,
    &quot;The melody danced through the air, painting a vibrant tapestry of emotions.&quot;,
    &quot;Time flowed like a relentless river, carrying away memories and leaving imprints in its wake.&quot;]

story = &quot;&quot;&quot;
  Beneath the shimmering moonlight, the ocean whispered secrets only the stars could hear.

  Under the celestial tapestry, the vast ocean whispered its secrets to the indifferent stars. Each ripple, a murmured confidence, each wave, a whispered lament. The glittering celestial bodies listened in silent complicity, their enigmatic gaze reflecting the ocean&#39;s unspoken truths. The cosmic dance between the sea and the sky, a symphony of shared secrets, forever echoing in the ethereal expanse.
  &quot;&quot;&quot;

let pos = argmax(length_longest_common_subsequence.(Ref(story), context))
    dist = length_longest_common_subsequence(story, context[pos])
    norm = dist / min(length(story), length(context[pos]))
    @info &quot;The closest context to the query: &quot;$(first(story,20))...&quot; is: &quot;$(context[pos])&quot; (distance: $(dist), normalized: $(norm))&quot;
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L169-L223">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.list_aliases-Tuple{}" href="#PromptingTools.list_aliases-Tuple{}"><code>PromptingTools.list_aliases</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Shows the Dictionary of model aliases in the registry. Add more with <code>MODEL_ALIASES[alias] = model_name</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L537">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.list_registry-Tuple{}" href="#PromptingTools.list_registry-Tuple{}"><code>PromptingTools.list_registry</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Shows the list of models in the registry. Add more with <code>register_model!</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L535">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.load_conversation-Tuple{Union{AbstractString, IO}}" href="#PromptingTools.load_conversation-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_conversation</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Loads a conversation (<code>messages</code>) from <code>io_or_file</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/serialization.jl#L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.load_template-Tuple{Union{AbstractString, IO}}" href="#PromptingTools.load_template-Tuple{Union{AbstractString, IO}}"><code>PromptingTools.load_template</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Loads messaging template from <code>io_or_file</code> and returns tuple of template messages and metadata.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/serialization.jl#L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.load_templates!" href="#PromptingTools.load_templates!"><code>PromptingTools.load_templates!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">load_templates!(dir_templates::Union{String, Nothing} = nothing;
    remember_path::Bool = true,
    remove_templates::Bool = isnothing(dir_templates),
    store::Dict{Symbol, &lt;:Any} = TEMPLATE_STORE,
    metadata_store::Vector{&lt;:AITemplateMetadata} = TEMPLATE_METADATA)</code></pre><p>Loads templates from folder <code>templates/</code> in the package root and stores them in <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code>.</p><p>Note: Automatically removes any existing templates and metadata from <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code> if <code>remove_templates=true</code>.</p><p><strong>Arguments</strong></p><ul><li><code>dir_templates::Union{String, Nothing}</code>: The directory path to load templates from. If <code>nothing</code>, uses the default list of paths. It usually used only once &quot;to register&quot; a new template storage.</li><li><code>remember_path::Bool=true</code>: If true, remembers the path for future refresh (in <code>TEMPLATE_PATH</code>).</li><li><code>remove_templates::Bool=isnothing(dir_templates)</code>: If true, removes any existing templates and metadata from <code>store</code> and <code>metadata_store</code>.</li><li><code>store::Dict{Symbol, &lt;:Any}=TEMPLATE_STORE</code>: The store to load the templates into.</li><li><code>metadata_store::Vector{&lt;:AITemplateMetadata}=TEMPLATE_METADATA</code>: The metadata store to load the metadata into.</li></ul><p><strong>Example</strong></p><p>Load the default templates:</p><pre><code class="language-julia hljs">PT.load_templates!() # no path needed</code></pre><p>Load templates from a new custom path:</p><pre><code class="language-julia hljs">PT.load_templates!(&quot;path/to/templates&quot;) # we will remember this path for future refresh</code></pre><p>If you want to now refresh the default templates and the new path, just call <code>load_templates!()</code> without any arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L130-L161">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.ollama_api" href="#PromptingTools.ollama_api"><code>PromptingTools.ollama_api</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ollama_api(prompt_schema::Union{AbstractOllamaManagedSchema, AbstractOllamaSchema},
    prompt::Union{AbstractString, Nothing} = nothing;
    system::Union{Nothing, AbstractString} = nothing,
    messages::Vector{&lt;:AbstractMessage} = AbstractMessage[],
    endpoint::String = &quot;generate&quot;,
    model::String = &quot;llama2&quot;, http_kwargs::NamedTuple = NamedTuple(),
    stream::Bool = false,
    url::String = &quot;localhost&quot;, port::Int = 11434,
    kwargs...)</code></pre><p>Simple wrapper for a call to Ollama API.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>prompt_schema</code>: Defines which prompt template should be applied.</li><li><code>prompt</code>: Can be a string representing the prompt for the AI conversation, a <code>UserMessage</code>, a vector of <code>AbstractMessage</code></li><li><code>system</code>: An optional string representing the system message for the AI conversation. If not provided, a default message will be used.</li><li><code>endpoint</code>: The API endpoint to call, only &quot;generate&quot; and &quot;embeddings&quot; are currently supported. Defaults to &quot;generate&quot;.</li><li><code>model</code>: A string representing the model to use for generating the response. Can be an alias corresponding to a model ID defined in <code>MODEL_ALIASES</code>.</li><li><code>http_kwargs::NamedTuple</code>: Additional keyword arguments for the HTTP request. Defaults to empty <code>NamedTuple</code>.</li><li><code>stream</code>: A boolean indicating whether to stream the response. Defaults to <code>false</code>.</li><li><code>url</code>: The URL of the Ollama API. Defaults to &quot;localhost&quot;.</li><li><code>port</code>: The port of the Ollama API. Defaults to 11434.</li><li><code>kwargs</code>: Prompt variables to be used to fill the prompt/template</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama_managed.jl#L57-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.preview" href="#PromptingTools.preview"><code>PromptingTools.preview</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Utility for rendering the conversation (vector of messages) as markdown. REQUIRES the Markdown package to load the extension!</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L457">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.push_conversation!-Tuple{Vector{&lt;:Vector}, AbstractVector, Union{Nothing, Int64}}" href="#PromptingTools.push_conversation!-Tuple{Vector{&lt;:Vector}, AbstractVector, Union{Nothing, Int64}}"><code>PromptingTools.push_conversation!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">push_conversation!(conv_history, conversation::AbstractVector, max_history::Union{Int, Nothing})</code></pre><p>Add a new conversation to the conversation history and resize the history if necessary.</p><p>This function appends a conversation to the <code>conv_history</code>, which is a vector of conversations. Each conversation is represented as a vector of <code>AbstractMessage</code> objects. After adding the new conversation, the history is resized according to the <code>max_history</code> parameter to ensure that the size of the history does not exceed the specified limit.</p><p><strong>Arguments</strong></p><ul><li><code>conv_history</code>: A vector that stores the history of conversations. Typically, this is <code>PT.CONV_HISTORY</code>.</li><li><code>conversation</code>: The new conversation to be added. It should be a vector of <code>AbstractMessage</code> objects.</li><li><code>max_history</code>: The maximum number of conversations to retain in the history. If <code>Nothing</code>, the history is not resized.</li></ul><p><strong>Returns</strong></p><p>The updated conversation history.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">new_conversation = aigenerate(&quot;Hello World&quot;; return_all = true)
push_conversation!(PT.CONV_HISTORY, new_conversation, 10)</code></pre><p>This is done automatically by the ai&quot;&quot; macros.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L360-L382">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.register_model!" href="#PromptingTools.register_model!"><code>PromptingTools.register_model!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">register_model!(registry = MODEL_REGISTRY;
    name::String,
    schema::Union{AbstractPromptSchema, Nothing} = nothing,
    cost_of_token_prompt::Float64 = 0.0,
    cost_of_token_generation::Float64 = 0.0,
    description::String = &quot;&quot;)</code></pre><p>Register a new AI model with <code>name</code> and its associated <code>schema</code>. </p><p>Registering a model helps with calculating the costs and automatically selecting the right prompt schema.</p><p><strong>Arguments</strong></p><ul><li><code>name</code>: The name of the model. This is the name that will be used to refer to the model in the <code>ai*</code> functions.</li><li><code>schema</code>: The schema of the model. This is the schema that will be used to generate prompts for the model, eg, <code>OpenAISchema()</code>.</li><li><code>cost_of_token_prompt</code>: The cost of a token in the prompt for this model. This is used to calculate the cost of a prompt.   Note: It is often provided online as cost per 1000 tokens, so make sure to convert it correctly!</li><li><code>cost_of_token_generation</code>: The cost of a token generated by this model. This is used to calculate the cost of a generation.   Note: It is often provided online as cost per 1000 tokens, so make sure to convert it correctly!</li><li><code>description</code>: A description of the model. This is used to provide more information about the model when it is queried.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L233-L253">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.remove_julia_prompt-Tuple{T} where T&lt;:AbstractString" href="#PromptingTools.remove_julia_prompt-Tuple{T} where T&lt;:AbstractString"><code>PromptingTools.remove_julia_prompt</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">remove_julia_prompt(s::T) where {T&lt;:AbstractString}</code></pre><p>If it detects a julia prompt, it removes it and all lines that do not have it (except for those that belong to the code block).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L95-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.remove_templates!-Tuple{}" href="#PromptingTools.remove_templates!-Tuple{}"><code>PromptingTools.remove_templates!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">    remove_templates!()</code></pre><p>Removes all templates from <code>TEMPLATE_STORE</code> and <code>TEMPLATE_METADATA</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L123-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.remove_unsafe_lines-Tuple{AbstractString}" href="#PromptingTools.remove_unsafe_lines-Tuple{AbstractString}"><code>PromptingTools.remove_unsafe_lines</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Iterates over the lines of a string and removes those that contain a package operation or a missing import.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/code_parsing.jl#L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{AITemplate}" href="#PromptingTools.render-Tuple{AITemplate}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Renders provided messaging template (<code>template</code>) under the default schema (<code>PROMPT_SCHEMA</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/templates.jl#L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractGoogleSchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractGoogleSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractGoogleSchema,
    messages::Vector{&lt;:AbstractMessage};
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_google.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaManagedSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractOllamaManagedSchema,
    messages::Vector{&lt;:AbstractMessage};
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p>Note: Due to its &quot;managed&quot; nature, at most 2 messages can be provided (<code>system</code> and <code>prompt</code> inputs in the API).</p><p><strong>Keyword Arguments</strong></p><ul><li><code>conversation</code>: Not allowed for this schema. Provided only for compatibility.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama_managed.jl#L9-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractOllamaSchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractOllamaSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractOllamaSchema,
    messages::Vector{&lt;:AbstractMessage};
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_ollama.jl#L10-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.AbstractOpenAISchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::AbstractOpenAISchema,
    messages::Vector{&lt;:AbstractMessage};
    image_detail::AbstractString = &quot;auto&quot;,
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    kwargs...)</code></pre><p>Builds a history of the conversation to provide the prompt to the API. All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>image_detail</code>: Only for <code>UserMessageWithImages</code>. It represents the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>.</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L2-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.render-Tuple{PromptingTools.NoSchema, Vector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.render-Tuple{PromptingTools.NoSchema, Vector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.render</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">render(schema::NoSchema,
    messages::Vector{&lt;:AbstractMessage};
    conversation::AbstractVector{&lt;:AbstractMessage} = AbstractMessage[],
    replacement_kwargs...)</code></pre><p>Renders a conversation history from a vector of messages with all replacement variables specified in <code>replacement_kwargs</code>.</p><p>It is the first pass of the prompt rendering system, and is used by all other schemas.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>image_detail</code>: Only for <code>UserMessageWithImages</code>. It represents the level of detail to include for images. Can be <code>&quot;auto&quot;</code>, <code>&quot;high&quot;</code>, or <code>&quot;low&quot;</code>.</li><li><code>conversation</code>: An optional vector of <code>AbstractMessage</code> objects representing the conversation history. If not provided, it is initialized as an empty vector.</li></ul><p><strong>Notes</strong></p><ul><li>All unspecified kwargs are passed as replacements such that <code>{{key}}=&gt;value</code> in the template.</li><li>If a SystemMessage is missing, we inject a default one at the beginning of the conversation.</li><li>Only one SystemMessage is allowed (ie, cannot mix two conversations different system prompts).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_shared.jl#L2-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}" href="#PromptingTools.replace_words-Tuple{AbstractString, Vector{&lt;:AbstractString}}"><code>PromptingTools.replace_words</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replace_words(text::AbstractString, words::Vector{&lt;:AbstractString}; replacement::AbstractString=&quot;ABC&quot;)</code></pre><p>Replace all occurrences of words in <code>words</code> with <code>replacement</code> in <code>text</code>. Useful to quickly remove specific names or entities from a text.</p><p><strong>Arguments</strong></p><ul><li><code>text::AbstractString</code>: The text to be processed.</li><li><code>words::Vector{&lt;:AbstractString}</code>: A vector of words to be replaced.</li><li><code>replacement::AbstractString=&quot;ABC&quot;</code>: The replacement string to be used. Defaults to &quot;ABC&quot;.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">text = &quot;Disney is a great company&quot;
replace_words(text, [&quot;Disney&quot;, &quot;Snow White&quot;, &quot;Mickey Mouse&quot;])
# Output: &quot;ABC is a great company&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.resize_conversation!-Tuple{Any, Union{Nothing, Int64}}" href="#PromptingTools.resize_conversation!-Tuple{Any, Union{Nothing, Int64}}"><code>PromptingTools.resize_conversation!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">resize_conversation!(conv_history, max_history::Union{Int, Nothing})</code></pre><p>Resize the conversation history to a specified maximum length.</p><p>This function trims the <code>conv_history</code> to ensure that its size does not exceed <code>max_history</code>. It removes the oldest conversations first if the length of <code>conv_history</code> is greater than <code>max_history</code>.</p><p><strong>Arguments</strong></p><ul><li><code>conv_history</code>: A vector that stores the history of conversations. Typically, this is <code>PT.CONV_HISTORY</code>.</li><li><code>max_history</code>: The maximum number of conversations to retain in the history. If <code>Nothing</code>, the history is not resized.</li></ul><p><strong>Returns</strong></p><p>The resized conversation history.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">resize_conversation!(PT.CONV_HISTORY, PT.MAX_HISTORY_LENGTH)</code></pre><p>After the function call, <code>conv_history</code> will contain only the 10 most recent conversations.</p><p>This is done automatically by the ai&quot;&quot; macros.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L391-L414">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.response_to_message-Tuple{PromptingTools.AbstractOpenAISchema, Type{AIMessage}, Any, Any}" href="#PromptingTools.response_to_message-Tuple{PromptingTools.AbstractOpenAISchema, Type{AIMessage}, Any, Any}"><code>PromptingTools.response_to_message</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">response_to_message(schema::AbstractOpenAISchema,
    MSG::Type{AIMessage},
    choice,
    resp;
    model_id::AbstractString = &quot;&quot;,
    time::Float64 = 0.0,
    run_id::Integer = rand(Int16),
    sample_id::Union{Nothing, Integer} = nothing)</code></pre><p>Utility to facilitate unwrapping of HTTP response to a message type <code>MSG</code> provided for OpenAI-like responses</p><p>Note: Extracts <code>finish_reason</code> and <code>log_prob</code> if available in the response.</p><p><strong>Arguments</strong></p><ul><li><code>schema::AbstractOpenAISchema</code>: The schema for the prompt.</li><li><code>MSG::Type{AIMessage}</code>: The message type to be returned.</li><li><code>choice</code>: The choice from the response (eg, one of the completions).</li><li><code>resp</code>: The response from the OpenAI API.</li><li><code>model_id::AbstractString</code>: The model ID to use for generating the response. Defaults to an empty string.</li><li><code>time::Float64</code>: The elapsed time for the response. Defaults to <code>0.0</code>.</li><li><code>run_id::Integer</code>: The run ID for the response. Defaults to a random integer.</li><li><code>sample_id::Union{Nothing, Integer}</code>: The sample ID for the response (if there are multiple completions). Defaults to <code>nothing</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_openai.jl#L324-L347">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.response_to_message-Union{Tuple{T}, Tuple{PromptingTools.AbstractPromptSchema, Type{T}, Any, Any}} where T" href="#PromptingTools.response_to_message-Union{Tuple{T}, Tuple{PromptingTools.AbstractPromptSchema, Type{T}, Any, Any}} where T"><code>PromptingTools.response_to_message</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Utility to facilitate unwrapping of HTTP response to a message type <code>MSG</code> provided. Designed to handle multi-sample completions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/llm_interface.jl#L284">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.save_conversation-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractMessage}}" href="#PromptingTools.save_conversation-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractMessage}}"><code>PromptingTools.save_conversation</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Saves provided conversation (<code>messages</code>) to <code>io_or_file</code>. If you need to add some metadata, see <code>save_template</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/serialization.jl#L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}" href="#PromptingTools.save_template-Tuple{Union{AbstractString, IO}, AbstractVector{&lt;:PromptingTools.AbstractChatMessage}}"><code>PromptingTools.save_template</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Saves provided messaging template (<code>messages</code>) to <code>io_or_file</code>. Automatically adds metadata based on provided keyword arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/serialization.jl#L2">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.set_preferences!-Tuple{Vararg{Pair{String}}}" href="#PromptingTools.set_preferences!-Tuple{Vararg{Pair{String}}}"><code>PromptingTools.set_preferences!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">set_preferences!(pairs::Pair{String, &lt;:Any}...)</code></pre><p>Set preferences for PromptingTools. See <code>?PREFERENCES</code> for more information. </p><p>See also: <code>get_preferences</code></p><p><strong>Example</strong></p><p>Change your API key and default model:</p><pre><code class="language-julia hljs">PromptingTools.set_preferences!(&quot;OPENAI_API_KEY&quot; =&gt; &quot;key1&quot;, &quot;MODEL_CHAT&quot; =&gt; &quot;chat1&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/user_preferences.jl#L66-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.split_by_length-Tuple{Any, Vector{String}}" href="#PromptingTools.split_by_length-Tuple{Any, Vector{String}}"><code>PromptingTools.split_by_length</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">split_by_length(text::String, separators::Vector{String}; max_length::Int=35000) -&gt; Vector{String}</code></pre><p>Split a given string <code>text</code> into chunks using a series of separators, with each chunk having a maximum length of <code>max_length</code>.  This function is useful for splitting large documents or texts into smaller segments that are more manageable for processing, particularly for models or systems with limited context windows.</p><p><strong>Arguments</strong></p><ul><li><code>text::String</code>: The text to be split.</li><li><code>separators::Vector{String}</code>: An ordered list of separators used to split the text. The function iteratively applies these separators to split the text.</li><li><code>max_length::Int=35000</code>: The maximum length of each chunk. Defaults to 35,000 characters. This length is considered after each iteration of splitting, ensuring chunks fit within specified constraints.</li></ul><p><strong>Returns</strong></p><p><code>Vector{String}</code>: A vector of strings, where each string is a chunk of the original text that is smaller than or equal to <code>max_length</code>.</p><p><strong>Notes</strong></p><ul><li>The function processes the text iteratively with each separator in the provided order. This ensures more nuanced splitting, especially in structured texts.</li><li>Each chunk is as close to <code>max_length</code> as possible without exceeding it (unless we cannot split it any further)</li><li>If the <code>text</code> is empty, the function returns an empty array.</li><li>Separators are re-added to the text chunks after splitting, preserving the original structure of the text as closely as possible. Apply <code>strip</code> if you do not need them.</li></ul><p><strong>Examples</strong></p><p>Splitting text using multiple separators:</p><pre><code class="language-julia hljs">text = &quot;Paragraph 1

Paragraph 2. Sentence 1. Sentence 2.
Paragraph 3&quot;
separators = [&quot;

&quot;, &quot;. &quot;, &quot;
&quot;]
chunks = split_by_length(text, separators, max_length=20)</code></pre><p>Using a single separator:</p><pre><code class="language-julia hljs">text = &quot;Hello,World,&quot; ^ 2900  # length 34900 characters
chunks = split_by_length(text, [&quot;,&quot;], max_length=10000)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L116-L157">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.split_by_length-Tuple{String}" href="#PromptingTools.split_by_length-Tuple{String}"><code>PromptingTools.split_by_length</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">split_by_length(text::String; separator::String=&quot; &quot;, max_length::Int=35000) -&gt; Vector{String}</code></pre><p>Split a given string <code>text</code> into chunks of a specified maximum length <code>max_length</code>.  This is particularly useful for splitting larger documents or texts into smaller segments, suitable for models or systems with smaller context windows.</p><p><strong>Arguments</strong></p><ul><li><code>text::String</code>: The text to be split.</li><li><code>separator::String=&quot; &quot;</code>: The separator used to split the text into minichunks. Defaults to a space character.</li><li><code>max_length::Int=35000</code>: The maximum length of each chunk. Defaults to 35,000 characters, which should fit within 16K context window.</li></ul><p><strong>Returns</strong></p><p><code>Vector{String}</code>: A vector of strings, each representing a chunk of the original text that is smaller than or equal to <code>max_length</code>.</p><p><strong>Notes</strong></p><ul><li>The function ensures that each chunk is as close to <code>max_length</code> as possible without exceeding it.</li><li>If the <code>text</code> is empty, the function returns an empty array.</li><li>The <code>separator</code> is re-added to the text chunks after splitting, preserving the original structure of the text as closely as possible.</li></ul><p><strong>Examples</strong></p><p>Splitting text with the default separator (&quot; &quot;):</p><pre><code class="language-julia hljs">text = &quot;Hello world. How are you?&quot;
chunks = split_by_length(text; max_length=13)
length(chunks) # Output: 2</code></pre><p>Using a custom separator and custom <code>max_length</code></p><pre><code class="language-julia hljs">text = &quot;Hello,World,&quot; ^ 2900 # length 34900 chars
split_by_length(text; separator=&quot;,&quot;, max_length=10000) # for 4K context window
length(chunks[1]) # Output: 4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L33-L68">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}" href="#PromptingTools.@aai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@aai_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">aai&quot;user_prompt&quot;[model_alias] -&gt; AIMessage</code></pre><p>Asynchronous version of <code>@ai_str</code> macro, which will log the result once it&#39;s ready.</p><p>See also <code>aai!&quot;&quot;</code> if you want an asynchronous reply to the provided message / continue the conversation.    </p><p><strong>Example</strong></p><p>Send asynchronous request to GPT-4, so we don&#39;t have to wait for the response: Very practical with slow models, so you can keep working in the meantime.</p><p>```julia m = aai&quot;Say Hi!&quot;gpt4; </p><p><strong>...with some delay...</strong></p><p><strong>[ Info: Tokens: 29 @ Cost: 0.0011 in 2.7 seconds</strong></p><p><strong>[ Info: AIMessage&gt; Hello! How can I assist you today?</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/macros.jl#L99-L116">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@ai!_str-Tuple{Any, Vararg{Any}}" href="#PromptingTools.@ai!_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai!_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">ai!&quot;user_prompt&quot;[model_alias] -&gt; AIMessage</code></pre><p>The <code>ai!&quot;&quot;</code> string macro is used to continue a previous conversation with the AI model. </p><p>It appends the new user prompt to the last conversation in the tracked history (in <code>PromptingTools.CONV_HISTORY</code>) and generates a response based on the entire conversation context. If you want to see the previous conversation, you can access it via <code>PromptingTools.CONV_HISTORY</code>, which keeps at most last <code>PromptingTools.MAX_HISTORY_LENGTH</code> conversations.</p><p><strong>Arguments</strong></p><ul><li><code>user_prompt</code> (String): The new input prompt to be added to the existing conversation.</li><li><code>model_alias</code> (optional, any): Specify the model alias of the AI model to be used (see <code>MODEL_ALIASES</code>). If not provided, the default model is used.</li></ul><p><strong>Returns</strong></p><p><code>AIMessage</code> corresponding to the new user prompt, considering the entire conversation history.</p><p><strong>Example</strong></p><p>To continue a conversation:</p><pre><code class="language-julia hljs"># start conversation as normal
ai&quot;Say hi.&quot; 

# ... wait for reply and then react to it:

# continue the conversation (notice that you can change the model, eg, to more powerful one for better answer)
ai!&quot;What do you think about that?&quot;gpt4t
# AIMessage(&quot;Considering our previous discussion, I think that...&quot;)</code></pre><p><strong>Usage Notes</strong></p><ul><li>This macro should be used when you want to maintain the context of an ongoing conversation (ie, the last <code>ai&quot;&quot;</code> message).</li><li>It automatically accesses and updates the global conversation history.</li><li>If no conversation history is found, it raises an assertion error, suggesting to initiate a new conversation using <code>ai&quot;&quot;</code> instead.</li></ul><p><strong>Important</strong></p><p>Ensure that the conversation history is not too long to maintain relevancy and coherence in the AI&#39;s responses. The history length is managed by <code>MAX_HISTORY_LENGTH</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/macros.jl#L45-L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}" href="#PromptingTools.@ai_str-Tuple{Any, Vararg{Any}}"><code>PromptingTools.@ai_str</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">ai&quot;user_prompt&quot;[model_alias] -&gt; AIMessage</code></pre><p>The <code>ai&quot;&quot;</code> string macro generates an AI response to a given prompt by using <code>aigenerate</code> under the hood.</p><p>See also <code>ai!&quot;&quot;</code> if you want to reply to the provided message / continue the conversation.</p><p><strong>Arguments</strong></p><ul><li><code>user_prompt</code> (String): The input prompt for the AI model.</li><li><code>model_alias</code> (optional, any): Provide model alias of the AI model (see <code>MODEL_ALIASES</code>).</li></ul><p><strong>Returns</strong></p><p><code>AIMessage</code> corresponding to the input prompt.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">result = ai&quot;Hello, how are you?&quot;
# AIMessage(&quot;Hello! I&#39;m an AI assistant, so I don&#39;t have feelings, but I&#39;m here to help you. How can I assist you today?&quot;)</code></pre><p>If you want to interpolate some variables or additional context, simply use string interpolation:</p><pre><code class="language-julia hljs">a=1
result = ai&quot;What is `$a+$a`?&quot;
# AIMessage(&quot;The sum of `1+1` is `2`.&quot;)</code></pre><p>If you want to use a different model, eg, GPT-4, you can provide its alias as a flag:</p><pre><code class="language-julia hljs">result = ai&quot;What is `1.23 * 100 + 1`?&quot;gpt4t
# AIMessage(&quot;The answer is 124.&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/macros.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PromptingTools.@timeout-Tuple{Any, Any, Any}" href="#PromptingTools.@timeout-Tuple{Any, Any, Any}"><code>PromptingTools.@timeout</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@timeout(seconds, expr_to_run, expr_when_fails)</code></pre><p>Simple macro to run an expression with a timeout of <code>seconds</code>. If the <code>expr_to_run</code> fails to finish in <code>seconds</code> seconds, <code>expr_when_fails</code> is returned.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">x = @timeout 1 begin
    sleep(1.1)
    println(&quot;done&quot;)
    1
end &quot;failed&quot;
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/svilupp/PromptingTools.jl/blob/41535183ed8506a05d23eab2ed7b4114199e825b/src/utils.jl#L427-L441">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../frequently_asked_questions/">« F.A.Q.</a><a class="docs-footer-nextpage" href="../reference_experimental/">Experimental Modules »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Wednesday 28 February 2024 21:22">Wednesday 28 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
