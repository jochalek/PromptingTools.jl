<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>F.A.Q. · PromptingTools.jl</title><meta name="title" content="F.A.Q. · PromptingTools.jl"/><meta property="og:title" content="F.A.Q. · PromptingTools.jl"/><meta property="twitter:title" content="F.A.Q. · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/frequently_asked_questions/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/frequently_asked_questions/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/frequently_asked_questions/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/readme_examples/">Various examples</a></li><li><a class="tocitem" href="../examples/working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../examples/working_with_ollama/">Local models with Ollama.ai</a></li><li><a class="tocitem" href="../examples/working_with_google_ai_studio/">Google AIStudio</a></li><li><a class="tocitem" href="../examples/working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../examples/building_RAG/">Building RAG Application</a></li></ul></li><li class="is-active"><a class="tocitem" href>F.A.Q.</a><ul class="internal"><li><a class="tocitem" href="#Why-OpenAI"><span>Why OpenAI</span></a></li><li><a class="tocitem" href="#Data-Privacy-and-OpenAI"><span>Data Privacy and OpenAI</span></a></li><li><a class="tocitem" href="#Creating-OpenAI-API-Key"><span>Creating OpenAI API Key</span></a></li><li><a class="tocitem" href="#Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?"><span>Getting an error &quot;ArgumentError: api<em>key cannot be empty&quot; despite having set `OPENAI</em>API_KEY`?</span></a></li><li><a class="tocitem" href="#Setting-OpenAI-Spending-Limits"><span>Setting OpenAI Spending Limits</span></a></li><li><a class="tocitem" href="#Configuring-the-Environment-Variable-for-API-Key"><span>Configuring the Environment Variable for API Key</span></a></li><li><a class="tocitem" href="#Setting-the-API-Key-via-Preferences.jl"><span>Setting the API Key via Preferences.jl</span></a></li><li><a class="tocitem" href="#Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)"><span>Understanding the API Keyword Arguments in <code>aigenerate</code> (<code>api_kwargs</code>)</span></a></li><li><a class="tocitem" href="#Instant-Access-from-Anywhere"><span>Instant Access from Anywhere</span></a></li><li><a class="tocitem" href="#Open-Source-Alternatives"><span>Open Source Alternatives</span></a></li><li><a class="tocitem" href="#Setup-Guide-for-Ollama"><span>Setup Guide for Ollama</span></a></li><li><a class="tocitem" href="#Changing-the-Default-Model-or-Schema"><span>Changing the Default Model or Schema</span></a></li><li><a class="tocitem" href="#How-to-have-Multi-turn-Conversations?"><span>How to have Multi-turn Conversations?</span></a></li><li><a class="tocitem" href="#Explain-What-Happens-Under-the-Hood"><span>Explain What Happens Under the Hood</span></a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../reference/">PromptingTools.jl</a></li><li><a class="tocitem" href="../reference_experimental/">Experimental Modules</a></li><li><a class="tocitem" href="../reference_ragtools/">RAGTools</a></li><li><a class="tocitem" href="../reference_agenttools/">AgentTools</a></li><li><a class="tocitem" href="../reference_apitools/">APITools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>F.A.Q.</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>F.A.Q.</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/frequently_asked_questions.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Frequently-Asked-Questions"><a class="docs-heading-anchor" href="#Frequently-Asked-Questions">Frequently Asked Questions</a><a id="Frequently-Asked-Questions-1"></a><a class="docs-heading-anchor-permalink" href="#Frequently-Asked-Questions" title="Permalink"></a></h1><h2 id="Why-OpenAI"><a class="docs-heading-anchor" href="#Why-OpenAI">Why OpenAI</a><a id="Why-OpenAI-1"></a><a class="docs-heading-anchor-permalink" href="#Why-OpenAI" title="Permalink"></a></h2><p>OpenAI&#39;s models are at the forefront of AI research and provide robust, state-of-the-art capabilities for many tasks.</p><p>There will be situations not or cannot use it (eg, privacy, cost, etc.). In that case, you can use local models (eg, Ollama) or other APIs (eg, Anthropic).</p><p>Note: To get started with <a href="https://ollama.ai/">Ollama.ai</a>, see the <a href="#setup-guide-for-ollama">Setup Guide for Ollama</a> section below.</p><h2 id="Data-Privacy-and-OpenAI"><a class="docs-heading-anchor" href="#Data-Privacy-and-OpenAI">Data Privacy and OpenAI</a><a id="Data-Privacy-and-OpenAI-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Privacy-and-OpenAI" title="Permalink"></a></h2><p>At the time of writing, OpenAI does NOT use the API calls for training their models.</p><blockquote><p><strong>API</strong></p><p>OpenAI does not use data submitted to and generated by our API to train OpenAI models or improve OpenAI’s service offering. In order to support the continuous improvement of our models, you can fill out this form to opt-in to share your data with us. – <a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">How your data is used to improve our models</a></p></blockquote><p>You can always double-check the latest information on the <a href="https://platform.openai.com/docs/models/how-we-use-your-data">OpenAI&#39;s How we use your data</a> page.</p><p>Resources:</p><ul><li><a href="https://platform.openai.com/docs/models/how-we-use-your-data">OpenAI&#39;s How we use your data</a></li><li><a href="https://help.openai.com/en/articles/7039943-data-usage-for-consumer-services-faq">Data usage for consumer services FAQ</a></li><li><a href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">How your data is used to improve our models</a></li></ul><h2 id="Creating-OpenAI-API-Key"><a class="docs-heading-anchor" href="#Creating-OpenAI-API-Key">Creating OpenAI API Key</a><a id="Creating-OpenAI-API-Key-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-OpenAI-API-Key" title="Permalink"></a></h2><p>You can get your API key from OpenAI by signing up for an account and accessing the API section of the OpenAI website.</p><ol><li>Create an account with <a href="https://platform.openai.com/signup">OpenAI</a></li><li>Go to <a href="https://platform.openai.com/account/api-keys">API Key page</a></li><li>Click on “Create new secret key”</li></ol><p>!!! Do not share it with anyone and do NOT save it to any files that get synced online.</p><p>Resources:</p><ul><li><a href="https://platform.openai.com/docs/quickstart?context=python">OpenAI Documentation</a></li><li><a href="https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt">Visual tutorial</a></li></ul><p>Pro tip: Always set the spending limits!</p><h2 id="Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?"><a class="docs-heading-anchor" href="#Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?">Getting an error &quot;ArgumentError: api<em>key cannot be empty&quot; despite having set `OPENAI</em>API_KEY`?</a><a id="Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-an-error-&quot;ArgumentError:-api*key-cannot-be-empty&quot;-despite-having-set-OPENAI*API_KEY?" title="Permalink"></a></h2><p>Quick fix: just provide kwarg <code>api_key</code> with your key to the <code>aigenerate</code> function (and other <code>ai*</code> functions).</p><p>This error is thrown when the OpenAI API key is not available in 1) local preferences or 2) environment variables (<code>ENV[&quot;OPENAI_API_KEY&quot;]</code>).</p><p>First, check if you can access the key by running <code>ENV[&quot;OPENAI_API_KEY&quot;]</code> in the Julia REPL. If it returns <code>nothing</code>, the key is not set.</p><p>If the key is set, but you still get the error, there was a rare bug in earlier versions where if you first precompiled PromptingTools without the API key, it would remember it and &quot;compile away&quot; the <code>get(ENV,...)</code> function call. If you&#39;re experiencing this bug on the latest version of PromptingTools, please open an issue on GitHub.</p><p>The solution is to force a new precompilation, so you can do any of the below:</p><ol><li>Force precompilation (run <code>Pkg.precompile()</code> in the Julia REPL)</li><li>Update the PromptingTools package (runs precompilation automatically)</li><li>Delete your compiled cache in <code>.julia</code> DEPOT (usually <code>.julia/compiled/v1.10/PromptingTools</code>). You can do it manually in the file explorer or via Julia REPL: <code>rm(&quot;~/.julia/compiled/v1.10/PromptingTools&quot;, recursive=true, force=true)</code></li></ol><h2 id="Setting-OpenAI-Spending-Limits"><a class="docs-heading-anchor" href="#Setting-OpenAI-Spending-Limits">Setting OpenAI Spending Limits</a><a id="Setting-OpenAI-Spending-Limits-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-OpenAI-Spending-Limits" title="Permalink"></a></h2><p>OpenAI allows you to set spending limits directly on your account dashboard to prevent unexpected costs.</p><ol><li>Go to <a href="https://platform.openai.com/account/billing">OpenAI Billing</a></li><li>Set Soft Limit (you’ll receive a notification) and Hard Limit (API will stop working not to spend more money)</li></ol><p>A good start might be a soft limit of c.<span>$5 and a hard limit of c.$</span>10 - you can always increase it later in the month.</p><p>Resources:</p><ul><li><a href="https://community.openai.com/t/how-to-set-a-price-limit/13086">OpenAI Forum</a></li></ul><h3 id="How-much-does-it-cost?-Is-it-worth-paying-for?"><a class="docs-heading-anchor" href="#How-much-does-it-cost?-Is-it-worth-paying-for?">How much does it cost? Is it worth paying for?</a><a id="How-much-does-it-cost?-Is-it-worth-paying-for?-1"></a><a class="docs-heading-anchor-permalink" href="#How-much-does-it-cost?-Is-it-worth-paying-for?" title="Permalink"></a></h3><p>If you use a local model (eg, with Ollama), it&#39;s free. If you use any commercial APIs (eg, OpenAI), you will likely pay per &quot;token&quot; (a sub-word unit).</p><p>For example, a simple request with a simple question and 1 sentence response in return (”Is statement XYZ a positive comment”) will cost you ~0.0001 (ie, one-hundredth of a cent)</p><p><strong>Is it worth paying for?</strong></p><p>GenAI is a way to buy time! You can pay cents to save tens of minutes every day.</p><p>Continuing the example above, imagine you have a table with 200 comments. Now, you can parse each one of them with an LLM for the features/checks you need.  Assuming the price per call was 0.0001, you&#39;d pay 2 cents for the job and save 30-60 minutes of your time!</p><p>Resources:</p><ul><li><a href="https://openai.com/pricing">OpenAI Pricing per 1000 tokens</a></li></ul><h2 id="Configuring-the-Environment-Variable-for-API-Key"><a class="docs-heading-anchor" href="#Configuring-the-Environment-Variable-for-API-Key">Configuring the Environment Variable for API Key</a><a id="Configuring-the-Environment-Variable-for-API-Key-1"></a><a class="docs-heading-anchor-permalink" href="#Configuring-the-Environment-Variable-for-API-Key" title="Permalink"></a></h2><p>This is a guide for OpenAI&#39;s API key, but it works for any other API key you might need (eg, <code>MISTRALAI_API_KEY</code> for MistralAI API).</p><p>To use the OpenAI API with PromptingTools.jl, set your API key as an environment variable:</p><pre><code class="language-julia hljs">ENV[&quot;OPENAI_API_KEY&quot;] = &quot;your-api-key&quot;</code></pre><p>As a one-off, you can: </p><ul><li>set it in the terminal before launching Julia: <code>export OPENAI_API_KEY = &lt;your key&gt;</code></li><li>set it in your <code>setup.jl</code> (make sure not to commit it to GitHub!)</li></ul><p>Make sure to start Julia from the same terminal window where you set the variable. Easy check in Julia, run <code>ENV[&quot;OPENAI_API_KEY&quot;]</code> and you should see your key!</p><p>A better way:</p><ul><li>On a Mac, add the configuration line to your terminal&#39;s configuration file (eg, <code>~/.zshrc</code>). It will get automatically loaded every time you launch the terminal</li><li>On Windows, set it as a system variable in &quot;Environment Variables&quot; settings (see the Resources)</li></ul><p>Resources: </p><ul><li><a href="https://platform.openai.com/docs/quickstart?context=python">OpenAI Guide</a></li></ul><h2 id="Setting-the-API-Key-via-Preferences.jl"><a class="docs-heading-anchor" href="#Setting-the-API-Key-via-Preferences.jl">Setting the API Key via Preferences.jl</a><a id="Setting-the-API-Key-via-Preferences.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-the-API-Key-via-Preferences.jl" title="Permalink"></a></h2><p>You can also set the API key in <code>LocalPreferences.toml</code>, so it persists across sessions and projects.</p><p>Use: <code>PromptingTools.set_preferences!(&quot;OPENAI_API_KEY&quot;=&quot;your-api-key&quot;)</code></p><p>To double-check, run <code>PromptingTools.get_preferences(&quot;OPENAI_API_KEY&quot;)</code> and you should see your key!</p><p>See more detail in the <code>?PromptingTools.PREFERENCES</code> docstring.</p><h2 id="Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)"><a class="docs-heading-anchor" href="#Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)">Understanding the API Keyword Arguments in <code>aigenerate</code> (<code>api_kwargs</code>)</a><a id="Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-the-API-Keyword-Arguments-in-aigenerate-(api_kwargs)" title="Permalink"></a></h2><p>See <a href="https://platform.openai.com/docs/guides/text-generation/chat-completions-api">OpenAI API reference</a> for more information.</p><h2 id="Instant-Access-from-Anywhere"><a class="docs-heading-anchor" href="#Instant-Access-from-Anywhere">Instant Access from Anywhere</a><a id="Instant-Access-from-Anywhere-1"></a><a class="docs-heading-anchor-permalink" href="#Instant-Access-from-Anywhere" title="Permalink"></a></h2><p>For easy access from anywhere, add PromptingTools into your <code>startup.jl</code> (can be found in <code>~/.julia/config/startup.jl</code>).</p><p>Add the following snippet:</p><pre><code class="nohighlight hljs">using PromptingTools
const PT = PromptingTools # to access unexported functions and types</code></pre><p>Now, you can just use <code>ai&quot;Help me do X to achieve Y&quot;</code> from any REPL session!</p><h2 id="Open-Source-Alternatives"><a class="docs-heading-anchor" href="#Open-Source-Alternatives">Open Source Alternatives</a><a id="Open-Source-Alternatives-1"></a><a class="docs-heading-anchor-permalink" href="#Open-Source-Alternatives" title="Permalink"></a></h2><p>The ethos of PromptingTools.jl is to allow you to use whatever model you want, which includes Open Source LLMs. The most popular and easiest to setup is <a href="https://ollama.ai/">Ollama.ai</a> - see below for more information.</p><h2 id="Setup-Guide-for-Ollama"><a class="docs-heading-anchor" href="#Setup-Guide-for-Ollama">Setup Guide for Ollama</a><a id="Setup-Guide-for-Ollama-1"></a><a class="docs-heading-anchor-permalink" href="#Setup-Guide-for-Ollama" title="Permalink"></a></h2><p>Ollama runs a background service hosting LLMs that you can access via a simple API. It&#39;s especially useful when you&#39;re working with some sensitive data that should not be sent anywhere.</p><p>Installation is very easy, just download the latest version <a href="https://ollama.ai/download">here</a>.</p><p>Once you&#39;ve installed it, just launch the app and you&#39;re ready to go!</p><p>To check if it&#39;s running, go to your browser and open <code>127.0.0.1:11434</code>. You should see the message &quot;Ollama is running&quot;.  Alternatively, you can run <code>ollama serve</code> in your terminal and you&#39;ll get a message that it&#39;s already running.</p><p>There are many models available in <a href="https://ollama.ai/library">Ollama Library</a>, including Llama2, CodeLlama, SQLCoder, or my personal favorite <code>openhermes2.5-mistral</code>.</p><p>Download new models with <code>ollama pull &lt;model_name&gt;</code> (eg, <code>ollama pull openhermes2.5-mistral</code>). </p><p>Show currently available models with <code>ollama list</code>.</p><p>See <a href="https://ollama.ai/">Ollama.ai</a> for more information.</p><h2 id="Changing-the-Default-Model-or-Schema"><a class="docs-heading-anchor" href="#Changing-the-Default-Model-or-Schema">Changing the Default Model or Schema</a><a id="Changing-the-Default-Model-or-Schema-1"></a><a class="docs-heading-anchor-permalink" href="#Changing-the-Default-Model-or-Schema" title="Permalink"></a></h2><p>If you tend to use non-default options, it can get tedious to specify <code>PT.*</code> every time.</p><p>There are three ways how you can customize your workflows (especially when you use Ollama or other local models):</p><ol><li>Import the functions/types you need explicitly at the top (eg, <code>using PromptingTools: OllamaSchema</code>)</li><li>Register your model and its associated schema  (<code>PT.register_model!(; name=&quot;123&quot;, schema=PT.OllamaSchema())</code>). You won&#39;t have to specify the schema anymore only the model name. See <a href="#working-with-ollama">Working with Ollama</a> for more information.</li><li>Override your default model (<code>PT.MODEL_CHAT</code>) and schema (<code>PT.PROMPT_SCHEMA</code>). It can be done persistently with Preferences, eg, <code>PT.set_preferences!(&quot;PROMPT_SCHEMA&quot; =&gt; &quot;OllamaSchema&quot;, &quot;MODEL_CHAT&quot;=&gt;&quot;llama2&quot;)</code>.</li></ol><h2 id="How-to-have-Multi-turn-Conversations?"><a class="docs-heading-anchor" href="#How-to-have-Multi-turn-Conversations?">How to have Multi-turn Conversations?</a><a id="How-to-have-Multi-turn-Conversations?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-have-Multi-turn-Conversations?" title="Permalink"></a></h2><p>Let&#39;s say you would like to respond back to a model&#39;s response. How to do it?</p><ol><li>With <code>ai&quot;&quot;</code> macro</li></ol><p>The simplest way if you used <code>ai&quot;&quot;</code> macro, is to send a reply with the <code>ai!&quot;&quot;</code> macro. It will use the last response as the conversation.</p><pre><code class="language-julia hljs">ai&quot;Hi! I&#39;m John&quot;

ai!&quot;What&#39;s my name?&quot;
# Return: &quot;Your name is John.&quot;</code></pre><ol><li>With <code>aigenerate</code> function</li></ol><p>You can use the <code>conversation</code> keyword argument to pass the previous conversation (in all <code>ai*</code> functions). It will prepend the past <code>conversation</code> before sending the new request to the model.</p><p>To get the conversation, set <code>return_all=true</code> and store the whole conversation thread (not just the last message) in a variable. Then, use it as a keyword argument in the next call.</p><pre><code class="language-julia hljs">conversation = aigenerate(&quot;Hi! I&#39;m John&quot;; return_all=true)
@info last(conversation) # display the response

# follow-up (notice that we provide past messages as conversation kwarg
conversation = aigenerate(&quot;What&#39;s my name?&quot;; return_all=true, conversation)

## [ Info: Tokens: 50 @ Cost: $0.0 in 1.0 seconds
## 5-element Vector{PromptingTools.AbstractMessage}:
##  PromptingTools.SystemMessage(&quot;Act as a helpful AI assistant&quot;)
##  PromptingTools.UserMessage(&quot;Hi! I&#39;m John&quot;)
##  AIMessage(&quot;Hello John! How can I assist you today?&quot;)
##  PromptingTools.UserMessage(&quot;What&#39;s my name?&quot;)
##  AIMessage(&quot;Your name is John.&quot;)</code></pre><p>Notice that the last message is the response to the second request, but with <code>return_all=true</code> we can see the whole conversation from the beginning.</p><h2 id="Explain-What-Happens-Under-the-Hood"><a class="docs-heading-anchor" href="#Explain-What-Happens-Under-the-Hood">Explain What Happens Under the Hood</a><a id="Explain-What-Happens-Under-the-Hood-1"></a><a class="docs-heading-anchor-permalink" href="#Explain-What-Happens-Under-the-Hood" title="Permalink"></a></h2><p>4 Key Concepts/Objects:</p><ul><li>Schemas -&gt; object of type <code>AbstractPromptSchema</code> that determines which methods are called and, hence, what providers/APIs are used</li><li>Prompts -&gt; the information you want to convey to the AI model</li><li>Messages -&gt; the basic unit of communication between the user and the AI model (eg, <code>UserMessage</code> vs <code>AIMessage</code>)</li><li>Prompt Templates -&gt; re-usable &quot;prompts&quot; with placeholders that you can replace with your inputs at the time of making the request</li></ul><p>When you call <code>aigenerate</code>, roughly the following happens: <code>render</code> -&gt; <code>UserMessage</code>(s) -&gt; <code>render</code> -&gt; <code>OpenAI.create_chat</code> -&gt; ... -&gt; <code>AIMessage</code>.</p><p>We&#39;ll deep dive into an example in the end.</p><h3 id="Schemas"><a class="docs-heading-anchor" href="#Schemas">Schemas</a><a id="Schemas-1"></a><a class="docs-heading-anchor-permalink" href="#Schemas" title="Permalink"></a></h3><p>For your &quot;message&quot; to reach an AI model, it needs to be formatted and sent to the right place.</p><p>We leverage the multiple dispatch around the &quot;schemas&quot; to pick the right logic. All schemas are subtypes of <code>AbstractPromptSchema</code> and there are many subtypes, eg, <code>OpenAISchema &lt;: AbstractOpenAISchema &lt;:AbstractPromptSchema</code>.</p><p>For example, if you provide <code>schema = OpenAISchema()</code>, the system knows that:</p><ul><li>it will have to format any user inputs to OpenAI&#39;s &quot;message specification&quot; (a vector of dictionaries, see their API documentation). Function <code>render(OpenAISchema(),...)</code> will take care of the rendering.</li><li>it will have to send the message to OpenAI&#39;s API. We will use the amazing <code>OpenAI.jl</code> package to handle the communication.</li></ul><h3 id="Prompts"><a class="docs-heading-anchor" href="#Prompts">Prompts</a><a id="Prompts-1"></a><a class="docs-heading-anchor-permalink" href="#Prompts" title="Permalink"></a></h3><p>Prompt is loosely the information you want to convey to the AI model. It can be a question, a statement, or a command. It can have instructions or some context, eg, previous conversation.</p><p>You need to remember that Large Language Models (LLMs) are <strong>stateless</strong>. They don&#39;t remember the previous conversation/request, so you need to provide the whole history/context every time (similar to how REST APIs work).</p><p>Prompts that we send to the LLMs are effectively a sequence of messages (<code>&lt;:AbstractMessage</code>).</p><h3 id="Messages"><a class="docs-heading-anchor" href="#Messages">Messages</a><a id="Messages-1"></a><a class="docs-heading-anchor-permalink" href="#Messages" title="Permalink"></a></h3><p>Messages are the basic unit of communication between the user and the AI model. </p><p>There are 5 main types of messages (<code>&lt;:AbstractMessage</code>):</p><ul><li><code>SystemMessage</code> - this contains information about the &quot;system&quot;, eg, how it should behave, format its output, etc. (eg, `You&#39;re a world-class Julia programmer. You write brief and concise code.)</li><li><code>UserMessage</code> - the information &quot;from the user&quot;, ie, your question/statement/task</li><li><code>UserMessageWithImages</code> - the same as <code>UserMessage</code>, but with images (URLs or Base64-encoded images)</li><li><code>AIMessage</code> - the response from the AI model, when the &quot;output&quot; is text</li><li><code>DataMessage</code> - the response from the AI model, when the &quot;output&quot; is data, eg, embeddings with <code>aiembed</code> or user-defined structs with <code>aiextract</code></li></ul><h3 id="Prompt-Templates"><a class="docs-heading-anchor" href="#Prompt-Templates">Prompt Templates</a><a id="Prompt-Templates-1"></a><a class="docs-heading-anchor-permalink" href="#Prompt-Templates" title="Permalink"></a></h3><p>We want to have re-usable &quot;prompts&quot;, so we provide you with a system to retrieve pre-defined prompts with placeholders (eg, <code>{{name}}</code>) that you can replace with your inputs at the time of making the request.</p><p>&quot;AI Templates&quot; as we call them (<code>AITemplate</code>) are usually a vector of <code>SystemMessage</code> and a <code>UserMessage</code> with specific purpose/task.</p><p>For example, the template <code>:AssistantAsk</code> is defined loosely as:</p><pre><code class="language-julia hljs"> template = [SystemMessage(&quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;),
             UserMessage(&quot;# Question\n\n{{ask}}&quot;)]</code></pre><p>Notice that we have a placeholder <code>ask</code> (<code>{{ask}}</code>) that you can replace with your question without having to re-write the generic system instructions.</p><p>When you provide a Symbol (eg, <code>:AssistantAsk</code>) to ai* functions, thanks to the multiple dispatch, it recognizes that it&#39;s an <code>AITemplate(:AssistantAsk)</code> and looks it up.</p><p>You can discover all available templates with <code>aitemplates(&quot;some keyword&quot;)</code> or just see the details of some template <code>aitemplates(:AssistantAsk)</code>.</p><h3 id="Walkthrough-Example"><a class="docs-heading-anchor" href="#Walkthrough-Example">Walkthrough Example</a><a id="Walkthrough-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Walkthrough-Example" title="Permalink"></a></h3><pre><code class="language-julia hljs">using PromptingTools
const PT = PromptingTools

# Let&#39;s say this is our ask
msg = aigenerate(:AssistantAsk; ask=&quot;What is the capital of France?&quot;)

# it is effectively the same as:
msg = aigenerate(PT.OpenAISchema(), PT.AITemplate(:AssistantAsk); ask=&quot;What is the capital of France?&quot;, model=&quot;gpt3t&quot;)</code></pre><p>There is no <code>model</code> provided, so we use the default <code>PT.MODEL_CHAT</code> (effectively GPT3.5-Turbo). Then we look it up in <code>PT.MDOEL_REGISTRY</code> and use the associated schema for it (<code>OpenAISchema</code> in this case).</p><p>The next step is to render the template, replace the placeholders and render it for the OpenAI model.</p><pre><code class="language-julia hljs"># Let&#39;s remember out schema
schema = PT.OpenAISchema()
ask = &quot;What is the capital of France?&quot;</code></pre><p>First, we obtain the template (no placeholder replacement yet) and &quot;expand it&quot;</p><pre><code class="language-julia hljs">template_rendered = PT.render(schema, AITemplate(:AssistantAsk); ask)</code></pre><pre><code class="language-plaintext hljs">2-element Vector{PromptingTools.AbstractChatMessage}:
  PromptingTools.SystemMessage(&quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;)
  PromptingTools.UserMessage{String}(&quot;# Question\n\n{{ask}}&quot;, [:ask], :usermessage)</code></pre><p>Second, we replace the placeholders</p><pre><code class="language-julia hljs">rendered_for_api = PT.render(schema, template_rendered;  ask)</code></pre><pre><code class="language-plaintext hljs">2-element Vector{Dict{String, Any}}:
  Dict(&quot;role&quot; =&gt; &quot;system&quot;, &quot;content&quot; =&gt; &quot;You are a world-class AI assistant. Your communication is brief and concise. You&#39;re precise and answer only when you&#39;re confident in the high quality of your answer.&quot;)
  Dict(&quot;role&quot; =&gt; &quot;user&quot;, &quot;content&quot; =&gt; &quot;# Question\n\nWhat is the capital of France?&quot;)</code></pre><p>Notice that the placeholders are only replaced in the second step. The final output here is a vector of messages with &quot;role&quot; and &quot;content&quot; keys, which is the format required by the OpenAI API.</p><p>As a side note, under the hood, the second step is done in two steps:</p><ul><li>replace the placeholders <code>messages_rendered = PT.render(PT.NoSchema(), template_rendered; ask)</code> -&gt; returns a vector of Messages!</li><li>then, we convert the messages to the format required by the provider/schema <code>PT.render(schema, messages_rendered)</code> -&gt; returns the OpenAI formatted messages</li></ul><p>Next, we send the above <code>rendered_for_api</code> to the OpenAI API and get the response back.</p><pre><code class="language-julia hljs">using OpenAI
OpenAI.create_chat(api_key, model, rendered_for_api)</code></pre><p>The last step is to take the JSON response from the API and convert it to the <code>AIMessage</code> object.</p><pre><code class="language-julia hljs"># simplification for educational purposes
msg = AIMessage(; content = r.response[:choices][1][:message][:content])</code></pre><p>In practice, there are more fields we extract, so we define a utility for it: <code>PT.response_to_message</code>. Especially, since with parameter <code>n</code>, you can request multiple AI responses at once, so we want to re-use our response processing logic.</p><p>That&#39;s it! I hope you&#39;ve learned something new about how PromptingTools.jl works under the hood.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/building_RAG/">« Building RAG Application</a><a class="docs-footer-nextpage" href="../reference/">PromptingTools.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Monday 26 February 2024 20:53">Monday 26 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
