<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Various examples · PromptingTools.jl</title><meta name="title" content="Various examples · PromptingTools.jl"/><meta property="og:title" content="Various examples · PromptingTools.jl"/><meta property="twitter:title" content="Various examples · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/examples/readme_examples/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/examples/readme_examples/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/examples/readme_examples/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Various examples</a><ul class="internal"><li><a class="tocitem" href="#Seamless-Integration-Into-Your-Workflow"><span>Seamless Integration Into Your Workflow</span></a></li><li><a class="tocitem" href="#Advanced-Prompts-/-Conversations"><span>Advanced Prompts / Conversations</span></a></li><li><a class="tocitem" href="#Templated-Prompts"><span>Templated Prompts</span></a></li><li><a class="tocitem" href="#Asynchronous-Execution"><span>Asynchronous Execution</span></a></li><li><a class="tocitem" href="#Model-Aliases"><span>Model Aliases</span></a></li><li><a class="tocitem" href="#Embeddings"><span>Embeddings</span></a></li><li><a class="tocitem" href="#Classification"><span>Classification</span></a></li><li><a class="tocitem" href="#Data-Extraction"><span>Data Extraction</span></a></li><li><a class="tocitem" href="#OCR-and-Image-Comprehension"><span>OCR and Image Comprehension</span></a></li><li><a class="tocitem" href="#Using-Ollama-models"><span>Using Ollama models</span></a></li><li><a class="tocitem" href="#Using-MistralAI-API-and-other-OpenAI-compatible-APIs"><span>Using MistralAI API and other OpenAI-compatible APIs</span></a></li></ul></li><li><a class="tocitem" href="../working_with_aitemplates/">Using AITemplates</a></li><li><a class="tocitem" href="../working_with_ollama/">Local models with Ollama.ai</a></li><li><a class="tocitem" href="../working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../building_RAG/">Building RAG Application</a></li></ul></li><li><a class="tocitem" href="../../frequently_asked_questions/">F.A.Q.</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/">PromptingTools.jl</a></li><li><a class="tocitem" href="../../reference_experimental/">Experimental Modules</a></li><li><a class="tocitem" href="../../reference_ragtools/">RAGTools</a></li><li><a class="tocitem" href="../../reference_agenttools/">AgentTools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Various examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Various examples</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/docs/src/examples/readme_examples.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Various-Examples"><a class="docs-heading-anchor" href="#Various-Examples">Various Examples</a><a id="Various-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Various-Examples" title="Permalink"></a></h1><p>Noteworthy functions: <code>aigenerate</code>, <code>aiembed</code>, <code>aiclassify</code>, <code>aiextract</code>, <code>aitemplates</code></p><h2 id="Seamless-Integration-Into-Your-Workflow"><a class="docs-heading-anchor" href="#Seamless-Integration-Into-Your-Workflow">Seamless Integration Into Your Workflow</a><a id="Seamless-Integration-Into-Your-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#Seamless-Integration-Into-Your-Workflow" title="Permalink"></a></h2><p>Google search is great, but it&#39;s a context switch. You often have to open a few pages and read through the discussion to find the answer you need. Same with the ChatGPT website.</p><p>Imagine you are in VSCode, editing your <code>.gitignore</code> file. How do I ignore a file in all subfolders again?</p><p>All you need to do is to type: <code>aai&quot;What to write in .gitignore to ignore file XYZ in any folder or subfolder?&quot;</code></p><p>With <code>aai&quot;&quot;</code> (as opposed to <code>ai&quot;&quot;</code>), we make a non-blocking call to the LLM to not prevent you from continuing your work. When the answer is ready, we log it from the background:</p><pre><code class="language-plaintext hljs">[ Info: Tokens: 102 @ Cost: $0.0002 in 2.7 seconds
┌ Info: AIMessage&gt; To ignore a file called &quot;XYZ&quot; in any folder or subfolder, you can add the following line to your .gitignore file:
│ 
│ ```
│ **/XYZ
│ ```
│ 
└ This pattern uses the double asterisk (`**`) to match any folder or subfolder, and then specifies the name of the file you want to ignore.</code></pre><p>You probably saved 3-5 minutes on this task and probably another 5-10 minutes, because of the context switch/distraction you avoided. It&#39;s a small win, but it adds up quickly.</p><h2 id="Advanced-Prompts-/-Conversations"><a class="docs-heading-anchor" href="#Advanced-Prompts-/-Conversations">Advanced Prompts / Conversations</a><a id="Advanced-Prompts-/-Conversations-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Prompts-/-Conversations" title="Permalink"></a></h2><p>You can use the <code>aigenerate</code> function to replace handlebar variables (eg, <code>{{name}}</code>) via keyword arguments.</p><pre><code class="language-julia hljs">msg = aigenerate(&quot;Say hello to {{name}}!&quot;, name=&quot;World&quot;)</code></pre><p>The more complex prompts are effectively a conversation (a set of messages), where you can have messages from three entities: System, User, AI Assistant. We provide the corresponding types for each of them: <code>SystemMessage</code>, <code>UserMessage</code>, <code>AIMessage</code>. </p><pre><code class="language-julia hljs">using PromptingTools: SystemMessage, UserMessage

conversation = [
    SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Jedi.&quot;),
    UserMessage(&quot;I have feelings for my {{object}}. What should I do?&quot;)]
msg = aigenerate(conversation; object = &quot;old iPhone&quot;)</code></pre><pre><code class="language-plaintext hljs">AIMessage(&quot;Ah, a dilemma, you have. Emotional attachment can cloud your path to becoming a Jedi. To be attached to material possessions, you must not. The iPhone is but a tool, nothing more. Let go, you must.

Seek detachment, young padawan. Reflect upon the impermanence of all things. Appreciate the memories it gave you, and gratefully part ways. In its absence, find new experiences to grow and become one with the Force. Only then, a true Jedi, you shall become.&quot;)</code></pre><p>You can also use it to build conversations, eg, </p><pre><code class="language-julia hljs">new_conversation = vcat(conversation...,msg, UserMessage(&quot;Thank you, master Yoda! Do you have {{object}} to know what it feels like?&quot;))
aigenerate(new_conversation; object = &quot;old iPhone&quot;)</code></pre><pre><code class="language-plaintext hljs">&gt; AIMessage(&quot;Hmm, possess an old iPhone, I do not. But experience with attachments, I have. Detachment, I learned. True power and freedom, it brings...&quot;)</code></pre><h2 id="Templated-Prompts"><a class="docs-heading-anchor" href="#Templated-Prompts">Templated Prompts</a><a id="Templated-Prompts-1"></a><a class="docs-heading-anchor-permalink" href="#Templated-Prompts" title="Permalink"></a></h2><p>With LLMs, the quality / robustness of your results depends on the quality of your prompts. But writing prompts is hard! That&#39;s why we offer a templating system to save you time and effort.</p><p>To use a specific template (eg, `` to ask a Julia language):</p><pre><code class="language-julia hljs">msg = aigenerate(:JuliaExpertAsk; ask = &quot;How do I add packages?&quot;)</code></pre><p>The above is equivalent to a more verbose version that explicitly uses the dispatch on <code>AITemplate</code>:</p><pre><code class="language-julia hljs">msg = aigenerate(AITemplate(:JuliaExpertAsk); ask = &quot;How do I add packages?&quot;)</code></pre><p>Find available templates with <code>aitemplates</code>:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;JuliaExpertAsk&quot;)
# Will surface one specific template
# 1-element Vector{AITemplateMetadata}:
# PromptingTools.AITemplateMetadata
#   name: Symbol JuliaExpertAsk
#   description: String &quot;For asking questions about Julia language. Placeholders: `ask`&quot;
#   version: String &quot;1&quot;
#   wordcount: Int64 237
#   variables: Array{Symbol}((1,))
#   system_preview: String &quot;You are a world-class Julia language programmer with the knowledge of the latest syntax. Your commun&quot;
#   user_preview: String &quot;# Question\n\n{{ask}}&quot;
#   source: String &quot;&quot;</code></pre><p>The above gives you a good idea of what the template is about, what placeholders are available, and how much it would cost to use it (=wordcount).</p><p>Search for all Julia-related templates:</p><pre><code class="language-julia hljs">tmps = aitemplates(&quot;Julia&quot;)
# 2-element Vector{AITemplateMetadata}... -&gt; more to come later!</code></pre><p>If you are on VSCode, you can leverage a nice tabular display with <code>vscodedisplay</code>:</p><pre><code class="language-julia hljs">using DataFrames
tmps = aitemplates(&quot;Julia&quot;) |&gt; DataFrame |&gt; vscodedisplay</code></pre><p>I have my selected template, how do I use it? Just use the &quot;name&quot; in <code>aigenerate</code> or <code>aiclassify</code>   like you see in the first example!</p><p>You can inspect any template by &quot;rendering&quot; it (this is what the LLM will see):</p><pre><code class="language-julia hljs">julia&gt; AITemplate(:JudgeIsItTrue) |&gt; PromptingTools.render</code></pre><p>See more examples in the <a href="https://github.com/svilupp/PromptingTools.jl/tree/main/examples">Examples</a> folder.</p><h2 id="Asynchronous-Execution"><a class="docs-heading-anchor" href="#Asynchronous-Execution">Asynchronous Execution</a><a id="Asynchronous-Execution-1"></a><a class="docs-heading-anchor-permalink" href="#Asynchronous-Execution" title="Permalink"></a></h2><p>You can leverage <code>asyncmap</code> to run multiple AI-powered tasks concurrently, improving performance for batch operations. </p><pre><code class="language-julia hljs">prompts = [aigenerate(&quot;Translate &#39;Hello, World!&#39; to {{language}}&quot;; language) for language in [&quot;Spanish&quot;, &quot;French&quot;, &quot;Mandarin&quot;]]
responses = asyncmap(aigenerate, prompts)</code></pre><p>Pro tip: You can limit the number of concurrent tasks with the keyword <code>asyncmap(...; ntasks=10)</code>.</p><h2 id="Model-Aliases"><a class="docs-heading-anchor" href="#Model-Aliases">Model Aliases</a><a id="Model-Aliases-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Aliases" title="Permalink"></a></h2><p>Certain tasks require more powerful models. All user-facing functions have a keyword argument <code>model</code> that can be used to specify the model to be used. For example, you can use <code>model = &quot;gpt-4-1106-preview&quot;</code> to use the latest GPT-4 Turbo model. However, no one wants to type that!</p><p>We offer a set of model aliases (eg, &quot;gpt3&quot;, &quot;gpt4&quot;, &quot;gpt4t&quot; -&gt; the above GPT-4 Turbo, etc.) that can be used instead. </p><p>Each <code>ai...</code> call first looks up the provided model name in the dictionary <code>PromptingTools.MODEL_ALIASES</code>, so you can easily extend with your own aliases! </p><pre><code class="language-julia hljs">const PT = PromptingTools
PT.MODEL_ALIASES[&quot;gpt4t&quot;] = &quot;gpt-4-1106-preview&quot;</code></pre><p>These aliases also can be used as flags in the <code>@ai_str</code> macro, eg, <code>ai&quot;What is the capital of France?&quot;gpt4t</code> (GPT-4 Turbo has a knowledge cut-off in April 2023, so it&#39;s useful for more contemporary questions).</p><h2 id="Embeddings"><a class="docs-heading-anchor" href="#Embeddings">Embeddings</a><a id="Embeddings-1"></a><a class="docs-heading-anchor-permalink" href="#Embeddings" title="Permalink"></a></h2><p>Use the <code>aiembed</code> function to create embeddings via the default OpenAI model that can be used for semantic search, clustering, and more complex AI workflows.</p><pre><code class="language-julia hljs">text_to_embed = &quot;The concept of artificial intelligence.&quot;
msg = aiembed(text_to_embed)
embedding = msg.content # 1536-element Vector{Float64}</code></pre><p>If you plan to calculate the cosine distance between embeddings, you can normalize them first:</p><pre><code class="language-julia hljs">using LinearAlgebra
msg = aiembed([&quot;embed me&quot;, &quot;and me too&quot;], LinearAlgebra.normalize)

# calculate cosine distance between the two normalized embeddings as a simple dot product
msg.content&#39; * msg.content[:, 1] # [1.0, 0.787]</code></pre><h2 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h2><p>You can use the <code>aiclassify</code> function to classify any provided statement as true/false/unknown. This is useful for fact-checking, hallucination or NLI checks, moderation, filtering, sentiment analysis, feature engineering and more.</p><pre><code class="language-julia hljs">aiclassify(&quot;Is two plus two four?&quot;) 
# true</code></pre><p>System prompts and higher-quality models can be used for more complex tasks, including knowing when to defer to a human:</p><pre><code class="language-julia hljs">aiclassify(:JudgeIsItTrue; it = &quot;Is two plus three a vegetable on Mars?&quot;, model = &quot;gpt4t&quot;) 
# unknown</code></pre><p>In the above example, we used a prompt template <code>:JudgeIsItTrue</code>, which automatically expands into the following system prompt (and a separate user prompt): </p><blockquote><p>&quot;You are an impartial AI judge evaluating whether the provided statement is \&quot;true\&quot; or \&quot;false\&quot;. Answer \&quot;unknown\&quot; if you cannot decide.&quot;</p></blockquote><p>For more information on templates, see the <a href="#templated-prompts">Templated Prompts</a> section.</p><h2 id="Data-Extraction"><a class="docs-heading-anchor" href="#Data-Extraction">Data Extraction</a><a id="Data-Extraction-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Extraction" title="Permalink"></a></h2><p>Are you tired of extracting data with regex? You can use LLMs to extract structured data from text!</p><p>All you have to do is to define the structure of the data you want to extract and the LLM will do the rest.</p><p>Define a <code>return_type</code> with struct. Provide docstrings if needed (improves results and helps with documentation).</p><p>Let&#39;s start with a hard task - extracting the current weather in a given location:</p><pre><code class="language-julia hljs">@enum TemperatureUnits celsius fahrenheit
&quot;&quot;&quot;Extract the current weather in a given location

# Arguments
- `location`: The city and state, e.g. &quot;San Francisco, CA&quot;
- `unit`: The unit of temperature to return, either `celsius` or `fahrenheit`
&quot;&quot;&quot;
struct CurrentWeather
    location::String
    unit::Union{Nothing,TemperatureUnits}
end

# Note that we provide the TYPE itself, not an instance of it!
msg = aiextract(&quot;What&#39;s the weather in Salt Lake City in C?&quot;; return_type=CurrentWeather)
msg.content
# CurrentWeather(&quot;Salt Lake City, UT&quot;, celsius)</code></pre><p>But you can use it even for more complex tasks, like extracting many entities from a text:</p><pre><code class="language-julia hljs">&quot;Person&#39;s age, height, and weight.&quot;
struct MyMeasurement
    age::Int
    height::Union{Int,Nothing}
    weight::Union{Nothing,Float64}
end
struct ManyMeasurements
    measurements::Vector{MyMeasurement}
end
msg = aiextract(&quot;James is 30, weighs 80kg. He&#39;s 180cm tall. Then Jack is 19 but really tall - over 190!&quot;; return_type=ManyMeasurements)
msg.content.measurements
# 2-element Vector{MyMeasurement}:
#  MyMeasurement(30, 180, 80.0)
#  MyMeasurement(19, 190, nothing)</code></pre><p>There is even a wrapper to help you catch errors together with helpful explanations on why parsing failed. See <code>?PromptingTools.MaybeExtract</code> for more information.</p><h2 id="OCR-and-Image-Comprehension"><a class="docs-heading-anchor" href="#OCR-and-Image-Comprehension">OCR and Image Comprehension</a><a id="OCR-and-Image-Comprehension-1"></a><a class="docs-heading-anchor-permalink" href="#OCR-and-Image-Comprehension" title="Permalink"></a></h2><p>With the <code>aiscan</code> function, you can interact with images as if they were text.</p><p>You can simply describe a provided image:</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=&quot;julia.png&quot;, model=&quot;gpt4v&quot;)
# [ Info: Tokens: 1141 @ Cost: \$0.0117 in 2.2 seconds
# AIMessage(&quot;The image shows a logo consisting of the word &quot;julia&quot; written in lowercase&quot;)</code></pre><p>Or you can do an OCR of a screenshot.  Let&#39;s transcribe some SQL code from a screenshot (no more re-typing!), we use a template <code>:OCRTask</code>:</p><pre><code class="language-julia hljs"># Screenshot of some SQL code
image_url = &quot;https://www.sqlservercentral.com/wp-content/uploads/legacy/8755f69180b7ac7ee76a69ae68ec36872a116ad4/24622.png&quot;
msg = aiscan(:OCRTask; image_url, model=&quot;gpt4v&quot;, task=&quot;Transcribe the SQL code in the image.&quot;, api_kwargs=(; max_tokens=2500))

# [ Info: Tokens: 362 @ Cost: \$0.0045 in 2.5 seconds
# AIMessage(&quot;```sql
# update Orders &lt;continue&gt;</code></pre><p>You can add syntax highlighting of the outputs via Markdown</p><pre><code class="language-julia hljs">using Markdown
msg.content |&gt; Markdown.parse</code></pre><h2 id="Using-Ollama-models"><a class="docs-heading-anchor" href="#Using-Ollama-models">Using Ollama models</a><a id="Using-Ollama-models-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Ollama-models" title="Permalink"></a></h2><p><a href="https://ollama.ai/">Ollama.ai</a> is an amazingly simple tool that allows you to run several Large Language Models (LLM) on your computer. It&#39;s especially suitable when you&#39;re working with some sensitive data that should not be sent anywhere.</p><p>Let&#39;s assume you have installed Ollama, downloaded a model, and it&#39;s running in the background.</p><p>We can use it with the <code>aigenerate</code> function:</p><pre><code class="language-julia hljs">const PT = PromptingTools
schema = PT.OllamaManagedSchema() # notice the different schema!

msg = aigenerate(schema, &quot;Say hi!&quot;; model=&quot;openhermes2.5-mistral&quot;)
# [ Info: Tokens: 69 in 0.9 seconds
# AIMessage(&quot;Hello! How can I assist you today?&quot;)</code></pre><p>And we can also use the <code>aiembed</code> function:</p><pre><code class="language-julia hljs">msg = aiembed(schema, &quot;Embed me&quot;, copy; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096-element JSON3.Array{Float64...

msg = aiembed(schema, [&quot;Embed me&quot;, &quot;Embed me&quot;]; model=&quot;openhermes2.5-mistral&quot;)
msg.content # 4096×2 Matrix{Float64}:</code></pre><p>If you&#39;re getting errors, check that Ollama is running - see the <a href="#setup-guide-for-ollama">Setup Guide for Ollama</a> section below.</p><h2 id="Using-MistralAI-API-and-other-OpenAI-compatible-APIs"><a class="docs-heading-anchor" href="#Using-MistralAI-API-and-other-OpenAI-compatible-APIs">Using MistralAI API and other OpenAI-compatible APIs</a><a id="Using-MistralAI-API-and-other-OpenAI-compatible-APIs-1"></a><a class="docs-heading-anchor-permalink" href="#Using-MistralAI-API-and-other-OpenAI-compatible-APIs" title="Permalink"></a></h2><p>Mistral models have long been dominating the open-source space. They are now available via their API, so you can use them with PromptingTools.jl!</p><pre><code class="language-julia hljs">msg = aigenerate(&quot;Say hi!&quot;; model=&quot;mistral-tiny&quot;)
# [ Info: Tokens: 114 @ Cost: $0.0 in 0.9 seconds
# AIMessage(&quot;Hello there! I&#39;m here to help answer any questions you might have, or assist you with tasks to the best of my abilities. How can I be of service to you today? If you have a specific question, feel free to ask and I&#39;ll do my best to provide accurate and helpful information. If you&#39;re looking for general assistance, I can help you find resources or information on a variety of topics. Let me know how I can help.&quot;)</code></pre><p>It all just works, because we have registered the models in the <code>PromptingTools.MODEL_REGISTRY</code>! There are currently 4 models available: <code>mistral-tiny</code>, <code>mistral-small</code>, <code>mistral-medium</code>, <code>mistral-embed</code>.</p><p>Under the hood, we use a dedicated schema <code>MistralOpenAISchema</code> that leverages most of the OpenAI-specific code base, so you can always provide that explicitly as the first argument:</p><pre><code class="language-julia hljs">const PT = PromptingTools
msg = aigenerate(PT.MistralOpenAISchema(), &quot;Say Hi!&quot;; model=&quot;mistral-tiny&quot;, api_key=ENV[&quot;MISTRALAI_API_KEY&quot;])</code></pre><p>As you can see, we can load your API key either from the ENV or via the Preferences.jl mechanism (see <code>?PREFERENCES</code> for more information).</p><p>But MistralAI are not the only ones! There are many other exciting providers, eg, <a href="https://docs.perplexity.ai/">Perplexity.ai</a>, <a href="https://app.fireworks.ai/">Fireworks.ai</a>. As long as they are compatible with the OpenAI API (eg, sending <code>messages</code> with <code>role</code> and <code>content</code> keys), you can use them with PromptingTools.jl by using <code>schema = CustomOpenAISchema()</code>:</p><pre><code class="language-julia hljs"># Set your API key and the necessary base URL for the API
api_key = &quot;...&quot;
prompt = &quot;Say hi!&quot;
msg = aigenerate(PT.CustomOpenAISchema(), prompt; model=&quot;my_model&quot;, api_key, api_kwargs=(; url=&quot;http://localhost:8081&quot;))</code></pre><p>As you can see, it also works for any local models that you might have running on your computer!</p><p>Note: At the moment, we only support <code>aigenerate</code> and <code>aiembed</code> functions for MistralAI and other OpenAI-compatible APIs. We plan to extend the support in the future.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../working_with_aitemplates/">Using AITemplates »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Wednesday 10 January 2024 22:18">Wednesday 10 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
