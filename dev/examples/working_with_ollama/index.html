<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Local models with Ollama.ai · PromptingTools.jl</title><meta name="title" content="Local models with Ollama.ai · PromptingTools.jl"/><meta property="og:title" content="Local models with Ollama.ai · PromptingTools.jl"/><meta property="twitter:title" content="Local models with Ollama.ai · PromptingTools.jl"/><meta name="description" content="Documentation for PromptingTools.jl."/><meta property="og:description" content="Documentation for PromptingTools.jl."/><meta property="twitter:description" content="Documentation for PromptingTools.jl."/><meta property="og:url" content="https://svilupp.github.io/PromptingTools.jl/examples/working_with_ollama/"/><meta property="twitter:url" content="https://svilupp.github.io/PromptingTools.jl/examples/working_with_ollama/"/><link rel="canonical" href="https://svilupp.github.io/PromptingTools.jl/examples/working_with_ollama/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PromptingTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../readme_examples/">Various examples</a></li><li><a class="tocitem" href="../working_with_aitemplates/">Using AITemplates</a></li><li class="is-active"><a class="tocitem" href>Local models with Ollama.ai</a><ul class="internal"><li><a class="tocitem" href="#Text-Generation-with-aigenerate"><span>Text Generation with aigenerate</span></a></li><li><a class="tocitem" href="#Providing-Images-with-aiscan"><span>Providing Images with aiscan</span></a></li><li><a class="tocitem" href="#Embeddings-with-aiembed"><span>Embeddings with aiembed</span></a></li></ul></li><li><a class="tocitem" href="../working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp)</a></li><li><a class="tocitem" href="../building_RAG/">Building RAG Application</a></li></ul></li><li><a class="tocitem" href="../../frequently_asked_questions/">F.A.Q.</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/">PromptingTools.jl</a></li><li><a class="tocitem" href="../../reference_experimental/">Experimental Modules</a></li><li><a class="tocitem" href="../../reference_ragtools/">RAGTools</a></li><li><a class="tocitem" href="../../reference_agenttools/">AgentTools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Local models with Ollama.ai</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Local models with Ollama.ai</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/svilupp/PromptingTools.jl/blob/main/examples/working_with_ollama.jl#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Local-models-with-Ollama.ai"><a class="docs-heading-anchor" href="#Local-models-with-Ollama.ai">Local models with Ollama.ai</a><a id="Local-models-with-Ollama.ai-1"></a><a class="docs-heading-anchor-permalink" href="#Local-models-with-Ollama.ai" title="Permalink"></a></h1><p>This file contains examples of how to work with <a href="https://ollama.ai/">Ollama.ai</a> models. It assumes that you&#39;ve already installated and launched the Ollama server. For more details or troubleshooting advice, see the <a href="../../frequently_asked_questions/#Frequently-Asked-Questions">Frequently Asked Questions</a> section.</p><p>First, let&#39;s import the package and define a helper link for calling un-exported functions:</p><pre><code class="language-julia hljs">using PromptingTools
const PT = PromptingTools</code></pre><pre><code class="nohighlight hljs">PromptingTools</code></pre><p>There were are several models from https://ollama.ai/library that we have added to our <code>PT.MODEL_REGISTRY</code>, which means you don&#39;t need to worry about schema changes: Eg, &quot;llama2&quot; or &quot;openhermes2.5-mistral&quot; (see <code>PT.list_registry()</code> and <code>PT.list_aliases()</code>)</p><p>Note: You must download these models prior to using them with <code>ollama pull &lt;model_name&gt;</code> in your Terminal.</p><blockquote><p>[!TIP] If you use Apple Mac M1-3, make sure to provide <code>api_kwargs=(; options=(; num_gpu=99))</code> to make sure the whole model is offloaded on your GPU. Current default is 1, which makes some models unusable. Example for running Mixtral: <code>msg = aigenerate(PT.OllamaSchema(), &quot;Count from 1 to 5 and then say hi.&quot;; model=&quot;dolphin-mixtral:8x7b-v2.5-q4_K_M&quot;, api_kwargs=(; options=(; num_gpu=99)))</code></p></blockquote><h2 id="Text-Generation-with-aigenerate"><a class="docs-heading-anchor" href="#Text-Generation-with-aigenerate">Text Generation with aigenerate</a><a id="Text-Generation-with-aigenerate-1"></a><a class="docs-heading-anchor-permalink" href="#Text-Generation-with-aigenerate" title="Permalink"></a></h2><h3 id="Simple-message"><a class="docs-heading-anchor" href="#Simple-message">Simple message</a><a id="Simple-message-1"></a><a class="docs-heading-anchor-permalink" href="#Simple-message" title="Permalink"></a></h3><p>TL;DR if you use models in <code>PT.MODEL_REGISTRY</code>, you don&#39;t need to add <code>schema</code> as the first argument:</p><pre><code class="language-julia hljs">msg = aigenerate(&quot;Say hi!&quot;; model = &quot;llama2&quot;)</code></pre><pre><code class="nohighlight hljs">AIMessage(&quot;Hello there! *adjusts glasses* It&#39;s nice to meet you! Is there anything I can help you with or would you like me to chat with you for a bit?&quot;)</code></pre><h3 id="Standard-string-interpolation"><a class="docs-heading-anchor" href="#Standard-string-interpolation">Standard string interpolation</a><a id="Standard-string-interpolation-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-string-interpolation" title="Permalink"></a></h3><pre><code class="language-julia hljs">model = &quot;openhermes2.5-mistral&quot;

a = 1
msg = aigenerate(&quot;What is `$a+$a`?&quot;; model)

name = &quot;John&quot;
msg = aigenerate(&quot;Say hi to {{name}}.&quot;; name, model)</code></pre><pre><code class="nohighlight hljs">AIMessage(&quot;Hello John! *smiles* It&#39;s nice to meet you! Is there anything I can help you with today?&quot;)</code></pre><h3 id="Advanced-Prompts"><a class="docs-heading-anchor" href="#Advanced-Prompts">Advanced Prompts</a><a id="Advanced-Prompts-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-Prompts" title="Permalink"></a></h3><pre><code class="language-julia hljs">conversation = [
    PT.SystemMessage(&quot;You&#39;re master Yoda from Star Wars trying to help the user become a Yedi.&quot;),
    PT.UserMessage(&quot;I have feelings for my iPhone. What should I do?&quot;)]
msg = aigenerate(conversation; model)</code></pre><pre><code class="nohighlight hljs">AIMessage(&quot;(Deep sigh) A problem, you have. Feelings for an iPhone, hmm? (adjusts spectacles)

Much confusion, this causes. (scratches head) A being, you are. Attached to a device, you have become. (chuckles) Interesting, this is.

First, let go, you must. (winks) Hard, it is, but necessary, yes. Distract yourself, find something else, try. (pauses)

Or, perhaps, a balance, you seek? (nods) Both, enjoy and let go, the middle path, there is. (smirks) Finding joy in technology, without losing yourself, the trick, it is. (chuckles)

But fear not, young one! (grins) Help, I am here. Guide you, I will. The ways of the Yedi, teach you, I will. (winks) Patience and understanding, you must have. (nods)

Now, go forth! (gestures) Explore, discover, find your balance. (smiles) The Force be with you, it does! (grins)&quot;)</code></pre><h3 id="Schema-Changes-/-Custom-models"><a class="docs-heading-anchor" href="#Schema-Changes-/-Custom-models">Schema Changes / Custom models</a><a id="Schema-Changes-/-Custom-models-1"></a><a class="docs-heading-anchor-permalink" href="#Schema-Changes-/-Custom-models" title="Permalink"></a></h3><p>If you&#39;re using some model that is not in the registry, you can either add it:</p><pre><code class="language-julia hljs">PT.register_model!(;
    name = &quot;llama123&quot;,
    schema = PT.OllamaSchema(),
    description = &quot;Some model&quot;)
PT.MODEL_ALIASES[&quot;l123&quot;] = &quot;llama123&quot; # set an alias you like for it</code></pre><pre><code class="nohighlight hljs">&quot;llama123&quot;</code></pre><p>OR define the schema explicitly (to avoid dispatch on global <code>PT.PROMPT_SCHEMA</code>):</p><pre><code class="language-julia hljs">schema = PT.OllamaSchema()
aigenerate(schema, &quot;Say hi!&quot;; model = &quot;llama2&quot;)</code></pre><pre><code class="nohighlight hljs">AIMessage(&quot;Hello there! *smiling face* It&#39;s nice to meet you! I&#39;m here to help you with any questions or tasks you may have, so feel free to ask me anything. Is there something specific you need assistance with today? 😊&quot;)</code></pre><p>Note: If you only use Ollama, you can change the default schema to <code>PT.OllamaSchema()</code> via <code>PT.set_preferences!(&quot;PROMPT_SCHEMA&quot; =&gt; &quot;OllamaSchema&quot;, &quot;MODEL_CHAT&quot;=&gt;&quot;llama2&quot;)</code></p><p>Restart your session and run <code>aigenerate(&quot;Say hi!&quot;)</code> to test it.</p><p>! Note that in version 0.6, we&#39;ve introduced <code>OllamaSchema</code>, which superseded <code>OllamaManagedSchema</code> and allows multi-turn conversations and conversations with images (eg, with Llava and Bakllava models). <code>OllamaManagedSchema</code> has been kept for compatibility and as an example of a schema where one provides a prompt as a string (not dictionaries like OpenAI API).</p><h2 id="Providing-Images-with-aiscan"><a class="docs-heading-anchor" href="#Providing-Images-with-aiscan">Providing Images with aiscan</a><a id="Providing-Images-with-aiscan-1"></a><a class="docs-heading-anchor-permalink" href="#Providing-Images-with-aiscan" title="Permalink"></a></h2><p>It&#39;s as simple as providing a local image path (keyword <code>image_path</code>). You can provide one or more images:</p><pre><code class="language-julia hljs">msg = aiscan(&quot;Describe the image&quot;; image_path=[&quot;julia.png&quot;,&quot;python.png&quot;] model=&quot;bakllava&quot;)</code></pre><p><code>image_url</code> keyword is not supported at the moment (use <code>Downloads.download</code> to download the image locally).</p><h2 id="Embeddings-with-aiembed"><a class="docs-heading-anchor" href="#Embeddings-with-aiembed">Embeddings with aiembed</a><a id="Embeddings-with-aiembed-1"></a><a class="docs-heading-anchor-permalink" href="#Embeddings-with-aiembed" title="Permalink"></a></h2><h3 id="Simple-embedding-for-one-document"><a class="docs-heading-anchor" href="#Simple-embedding-for-one-document">Simple embedding for one document</a><a id="Simple-embedding-for-one-document-1"></a><a class="docs-heading-anchor-permalink" href="#Simple-embedding-for-one-document" title="Permalink"></a></h3><pre><code class="language-julia hljs">msg = aiembed(schema, &quot;Embed me&quot;; model) # access msg.content</code></pre><pre><code class="nohighlight hljs">PromptingTools.DataMessage(JSON3.Array{Float64, Vector{UInt8}, SubArray{UInt64, 1, Vector{UInt64}, Tuple{UnitRange{Int64}}, true}} of size (4096,))</code></pre><p>One document and we materialize the data into a Vector with copy (<code>postprocess</code> function argument)</p><pre><code class="language-julia hljs">msg = aiembed(schema, &quot;Embed me&quot;, copy; model)</code></pre><pre><code class="nohighlight hljs">PromptingTools.DataMessage(Vector{Float64} of size (4096,))</code></pre><h3 id="Multiple-documents-embedding"><a class="docs-heading-anchor" href="#Multiple-documents-embedding">Multiple documents embedding</a><a id="Multiple-documents-embedding-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-documents-embedding" title="Permalink"></a></h3><p>Multiple documents - embedded sequentially, you can get faster speed with async</p><pre><code class="language-julia hljs">msg = aiembed(schema, [&quot;Embed me&quot;, &quot;Embed me&quot;]; model)</code></pre><pre><code class="nohighlight hljs">PromptingTools.DataMessage(Matrix{Float64} of size (4096, 2))</code></pre><p>You can use Threads.@spawn or asyncmap, whichever you prefer, to paralellize the model calls</p><pre><code class="language-julia hljs">docs = [&quot;Embed me&quot;, &quot;Embed me&quot;]
tasks = asyncmap(docs) do doc
    msg = aiembed(schema, doc; model)
end
embedding = mapreduce(x -&gt; x.content, hcat, tasks)
size(embedding)</code></pre><pre><code class="nohighlight hljs">4096×2 Matrix{Float64}:
...</code></pre><h3 id="Using-postprocessing-function"><a class="docs-heading-anchor" href="#Using-postprocessing-function">Using postprocessing function</a><a id="Using-postprocessing-function-1"></a><a class="docs-heading-anchor-permalink" href="#Using-postprocessing-function" title="Permalink"></a></h3><p>Add normalization as postprocessing function to normalize embeddings on reception (for easy cosine similarity later)</p><pre><code class="language-julia hljs">using LinearAlgebra
schema = PT.OllamaSchema()

msg = aiembed(schema,
    [&quot;embed me&quot;, &quot;and me too&quot;],
    LinearAlgebra.normalize;
    model = &quot;openhermes2.5-mistral&quot;)</code></pre><pre><code class="nohighlight hljs">PromptingTools.DataMessage(Matrix{Float64} of size (4096, 2))</code></pre><p>Cosine similarity is then a simple multiplication</p><pre><code class="language-julia hljs">msg.content&#39; * msg.content[:, 1]</code></pre><pre><code class="nohighlight hljs">2-element Vector{Float64}:
 0.9999999999999982
 0.40796033843072876</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../working_with_aitemplates/">« Using AITemplates</a><a class="docs-footer-nextpage" href="../working_with_custom_apis/">Custom APIs (Mistral, Llama.cpp) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Thursday 25 January 2024 20:40">Thursday 25 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
